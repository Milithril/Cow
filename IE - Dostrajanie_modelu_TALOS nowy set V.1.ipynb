{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hairy-iraqi",
   "metadata": {},
   "source": [
    "## 1. Biblioteki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "compressed-russell",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import talos as ta\n",
    "from talos.model.early_stopper import early_stopper\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "%load_ext tensorboard\n",
    "\n",
    "\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "presidential-upset",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Dane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "humanitarian-desperate",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1881, 27)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df=pd.read_csv('D:/STUDIA/ROK_II/Projekt/Dane_jalowka_i_krowa_train.csv')\n",
    "test_df=pd.read_csv('D:/STUDIA/ROK_II/Projekt/Dane_jalowka_i_krowa_test.csv')\n",
    "val_df=pd.read_csv('D:/STUDIA/ROK_II/Projekt/Dane_jalowka_i_krowa_val.csv')\n",
    "\n",
    "train_df.columns = ['id','Województwo', 'Data urodzenia', 'IE', 'Dokł. IE', 'PF', 'PI-PROD', 'PI-POKR', 'Prc', 'Psm', 'Pnr', 'Pw','PI-PLOD', 'CRj', 'CRk', 'PP', 'OMC', 'WH-KSOM', 'WH-DLUG', 'IP', 'kg ml', 'kg tł', '% tł', 'kg bi', '% bi', 'rc', 'sm', 'nr', 'w', 'og','kategoria']\n",
    "test_df.columns = ['id','Województwo', 'Data urodzenia', 'IE', 'Dokł. IE', 'PF', 'PI-PROD', 'PI-POKR', 'Prc', 'Psm', 'Pnr', 'Pw','PI-PLOD', 'CRj', 'CRk', 'PP', 'OMC', 'WH-KSOM', 'WH-DLUG', 'IP', 'kg ml', 'kg tł', '% tł', 'kg bi', '% bi', 'rc', 'sm', 'nr', 'w', 'og','kategoria']\n",
    "val_df.columns = ['id','Województwo', 'Data urodzenia', 'IE', 'Dokł. IE', 'PF', 'PI-PROD', 'PI-POKR', 'Prc', 'Psm', 'Pnr', 'Pw','PI-PLOD', 'CRj', 'CRk', 'PP', 'OMC', 'WH-KSOM', 'WH-DLUG', 'IP', 'kg ml', 'kg tł', '% tł', 'kg bi', '% bi', 'rc', 'sm', 'nr', 'w', 'og','kategoria']\n",
    "\n",
    "caly_df = pd.concat([train_df, test_df, val_df])\n",
    "caly_df['Województwo']=caly_df['Województwo'].astype('category')\n",
    "caly_df['Data urodzenia']=caly_df['Data urodzenia'].astype('category')\n",
    "\n",
    "del caly_df[\"id\"]\n",
    "del caly_df[\"PF\"]\n",
    "del caly_df[\"Dokł. IE\"]\n",
    "del caly_df['kategoria']\n",
    "\n",
    "caly_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "22b3fcd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Województwo</th>\n",
       "      <th>Data urodzenia</th>\n",
       "      <th>IE</th>\n",
       "      <th>PI-PROD</th>\n",
       "      <th>PI-POKR</th>\n",
       "      <th>Prc</th>\n",
       "      <th>Psm</th>\n",
       "      <th>Pnr</th>\n",
       "      <th>Pw</th>\n",
       "      <th>PI-PLOD</th>\n",
       "      <th>...</th>\n",
       "      <th>kg ml</th>\n",
       "      <th>kg tł</th>\n",
       "      <th>% tł</th>\n",
       "      <th>kg bi</th>\n",
       "      <th>% bi</th>\n",
       "      <th>rc</th>\n",
       "      <th>sm</th>\n",
       "      <th>nr</th>\n",
       "      <th>w</th>\n",
       "      <th>og</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WIELKOPOLSKIE</td>\n",
       "      <td>2020</td>\n",
       "      <td>2120</td>\n",
       "      <td>137</td>\n",
       "      <td>127</td>\n",
       "      <td>121</td>\n",
       "      <td>113</td>\n",
       "      <td>106</td>\n",
       "      <td>128</td>\n",
       "      <td>109</td>\n",
       "      <td>...</td>\n",
       "      <td>1370</td>\n",
       "      <td>577</td>\n",
       "      <td>2</td>\n",
       "      <td>452</td>\n",
       "      <td>1</td>\n",
       "      <td>115.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>123.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KUJAWSKO-POMORSKIE</td>\n",
       "      <td>2018</td>\n",
       "      <td>1912</td>\n",
       "      <td>126</td>\n",
       "      <td>123</td>\n",
       "      <td>109</td>\n",
       "      <td>113</td>\n",
       "      <td>103</td>\n",
       "      <td>128</td>\n",
       "      <td>114</td>\n",
       "      <td>...</td>\n",
       "      <td>1209</td>\n",
       "      <td>358</td>\n",
       "      <td>-16</td>\n",
       "      <td>354</td>\n",
       "      <td>-4</td>\n",
       "      <td>115.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>123.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>POMORSKIE</td>\n",
       "      <td>2020</td>\n",
       "      <td>2126</td>\n",
       "      <td>126</td>\n",
       "      <td>120</td>\n",
       "      <td>110</td>\n",
       "      <td>107</td>\n",
       "      <td>102</td>\n",
       "      <td>124</td>\n",
       "      <td>113</td>\n",
       "      <td>...</td>\n",
       "      <td>982</td>\n",
       "      <td>368</td>\n",
       "      <td>-4</td>\n",
       "      <td>350</td>\n",
       "      <td>4</td>\n",
       "      <td>104.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>117.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KUJAWSKO-POMORSKIE</td>\n",
       "      <td>2020</td>\n",
       "      <td>2089</td>\n",
       "      <td>133</td>\n",
       "      <td>111</td>\n",
       "      <td>104</td>\n",
       "      <td>110</td>\n",
       "      <td>99</td>\n",
       "      <td>114</td>\n",
       "      <td>114</td>\n",
       "      <td>...</td>\n",
       "      <td>1080</td>\n",
       "      <td>546</td>\n",
       "      <td>12</td>\n",
       "      <td>410</td>\n",
       "      <td>7</td>\n",
       "      <td>106.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>112.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ZACHODNIOPOMORSKIE</td>\n",
       "      <td>2020</td>\n",
       "      <td>2853</td>\n",
       "      <td>125</td>\n",
       "      <td>115</td>\n",
       "      <td>91</td>\n",
       "      <td>95</td>\n",
       "      <td>101</td>\n",
       "      <td>124</td>\n",
       "      <td>134</td>\n",
       "      <td>...</td>\n",
       "      <td>516</td>\n",
       "      <td>501</td>\n",
       "      <td>36</td>\n",
       "      <td>289</td>\n",
       "      <td>15</td>\n",
       "      <td>102.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>109.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Województwo Data urodzenia    IE  PI-PROD  PI-POKR  Prc  Psm  Pnr  \\\n",
       "0       WIELKOPOLSKIE           2020  2120      137      127  121  113  106   \n",
       "1  KUJAWSKO-POMORSKIE           2018  1912      126      123  109  113  103   \n",
       "2           POMORSKIE           2020  2126      126      120  110  107  102   \n",
       "3  KUJAWSKO-POMORSKIE           2020  2089      133      111  104  110   99   \n",
       "4  ZACHODNIOPOMORSKIE           2020  2853      125      115   91   95  101   \n",
       "\n",
       "    Pw  PI-PLOD  ...  kg ml  kg tł  % tł  kg bi  % bi     rc     sm     nr  \\\n",
       "0  128      109  ...   1370    577     2    452     1  115.0  114.0  109.0   \n",
       "1  128      114  ...   1209    358   -16    354    -4  115.0  113.0  104.0   \n",
       "2  124      113  ...    982    368    -4    350     4  104.0  103.0  102.0   \n",
       "3  114      114  ...   1080    546    12    410     7  106.0  106.0  101.0   \n",
       "4  124      134  ...    516    501    36    289    15  102.0  102.0  118.0   \n",
       "\n",
       "       w     og  \n",
       "0  127.0  123.0  \n",
       "1  123.0  123.0  \n",
       "2  122.0  117.0  \n",
       "3  112.0  112.0  \n",
       "4  123.0  109.0  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caly_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1db4b939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1881 entries, 0 to 188\n",
      "Data columns (total 27 columns):\n",
      " #   Column          Non-Null Count  Dtype   \n",
      "---  ------          --------------  -----   \n",
      " 0   Województwo     1881 non-null   category\n",
      " 1   Data urodzenia  1881 non-null   category\n",
      " 2   IE              1881 non-null   int64   \n",
      " 3   PI-PROD         1881 non-null   int64   \n",
      " 4   PI-POKR         1881 non-null   int64   \n",
      " 5   Prc             1881 non-null   int64   \n",
      " 6   Psm             1881 non-null   int64   \n",
      " 7   Pnr             1881 non-null   int64   \n",
      " 8   Pw              1881 non-null   int64   \n",
      " 9   PI-PLOD         1881 non-null   int64   \n",
      " 10  CRj             1881 non-null   int64   \n",
      " 11  CRk             1881 non-null   int64   \n",
      " 12  PP              1881 non-null   int64   \n",
      " 13  OMC             1881 non-null   int64   \n",
      " 14  WH-KSOM         1881 non-null   int64   \n",
      " 15  WH-DLUG         1881 non-null   int64   \n",
      " 16  IP              1881 non-null   int64   \n",
      " 17  kg ml           1881 non-null   int64   \n",
      " 18  kg tł           1881 non-null   int64   \n",
      " 19  % tł            1881 non-null   int64   \n",
      " 20  kg bi           1881 non-null   int64   \n",
      " 21  % bi            1881 non-null   int64   \n",
      " 22  rc              1881 non-null   float64 \n",
      " 23  sm              1881 non-null   float64 \n",
      " 24  nr              1881 non-null   float64 \n",
      " 25  w               1881 non-null   float64 \n",
      " 26  og              1881 non-null   float64 \n",
      "dtypes: category(2), float64(5), int64(20)\n",
      "memory usage: 386.8 KB\n"
     ]
    }
   ],
   "source": [
    "caly_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26512094",
   "metadata": {},
   "source": [
    "## 3. Usuwam mniejszości zmiennych kategorycznych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e9e25cdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WIELKOPOLSKIE          579\n",
       "KUJAWSKO-POMORSKIE     298\n",
       "OPOLSKIE               227\n",
       "ZACHODNIOPOMORSKIE     212\n",
       "PODLASKIE              175\n",
       "MAZOWIECKIE            106\n",
       "POMORSKIE               94\n",
       "LUBUSKIE                47\n",
       "ŁÓDZKIE                 41\n",
       "DOLNOŚLĄSKIE            32\n",
       "WARMIŃSKO-MAZURSKIE     23\n",
       "ŚLĄSKIE                 17\n",
       "ŚWIĘTOKRZYSKIE          13\n",
       "MAŁOPOLSKIE             12\n",
       "LUBELSKIE                4\n",
       "PODKARPACKIE             1\n",
       "Name: Województwo, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caly_df['Województwo'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bab61a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "caly_df=caly_df[caly_df['Województwo']!='PODKARPACKIE']\n",
    "caly_df=caly_df[caly_df['Województwo']!='LUBELSKIE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8facbffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WIELKOPOLSKIE          579\n",
       "KUJAWSKO-POMORSKIE     298\n",
       "OPOLSKIE               227\n",
       "ZACHODNIOPOMORSKIE     212\n",
       "PODLASKIE              175\n",
       "MAZOWIECKIE            106\n",
       "POMORSKIE               94\n",
       "LUBUSKIE                47\n",
       "ŁÓDZKIE                 41\n",
       "DOLNOŚLĄSKIE            32\n",
       "WARMIŃSKO-MAZURSKIE     23\n",
       "ŚLĄSKIE                 17\n",
       "ŚWIĘTOKRZYSKIE          13\n",
       "MAŁOPOLSKIE             12\n",
       "LUBELSKIE                0\n",
       "PODKARPACKIE             0\n",
       "Name: Województwo, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caly_df['Województwo'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "db8d9702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2020    1066\n",
       "2019     396\n",
       "2018     275\n",
       "2017      68\n",
       "2021      47\n",
       "2016      23\n",
       "2015       1\n",
       "Name: Data urodzenia, dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caly_df['Data urodzenia'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e3248c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "caly_df=caly_df[caly_df['Data urodzenia']!=2015]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3340ad27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2020    1066\n",
       "2019     396\n",
       "2018     275\n",
       "2017      68\n",
       "2021      47\n",
       "2016      23\n",
       "2015       0\n",
       "Name: Data urodzenia, dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caly_df['Data urodzenia'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fcb3aef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1875, 27)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caly_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755c159c",
   "metadata": {},
   "source": [
    "## 4. Zakodowuje zmienne kategoryczne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c1643a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=pd.get_dummies(caly_df['Województwo'])\n",
    "b=pd.get_dummies(caly_df['Data urodzenia'])\n",
    "caly_df= pd.concat([caly_df,a,b], axis=1)\n",
    "del caly_df['Województwo']\n",
    "del caly_df['Data urodzenia']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f756e767",
   "metadata": {},
   "source": [
    "## 5. Klasyfikuje wartosci IE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "513abdeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([103.,  85.,  60.,  56., 283., 350., 264., 195.,  96.,  78.,  67.,\n",
       "         47.,  44.,  37.,  24.,  23.,  19.,   6.,  10.,  10.,   2.,   7.,\n",
       "          3.,   1.,   2.,   1.,   1.,   0.,   0.,   1.]),\n",
       " array([1903.        , 1945.06666667, 1987.13333333, 2029.2       ,\n",
       "        2071.26666667, 2113.33333333, 2155.4       , 2197.46666667,\n",
       "        2239.53333333, 2281.6       , 2323.66666667, 2365.73333333,\n",
       "        2407.8       , 2449.86666667, 2491.93333333, 2534.        ,\n",
       "        2576.06666667, 2618.13333333, 2660.2       , 2702.26666667,\n",
       "        2744.33333333, 2786.4       , 2828.46666667, 2870.53333333,\n",
       "        2912.6       , 2954.66666667, 2996.73333333, 3038.8       ,\n",
       "        3080.86666667, 3122.93333333, 3165.        ]),\n",
       " <BarContainer object of 30 artists>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(caly_df[\"IE\"],bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f618a743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1875.000000\n",
       "mean     2186.410133\n",
       "std       166.883876\n",
       "min      1903.000000\n",
       "25%      2096.000000\n",
       "50%      2156.000000\n",
       "75%      2243.000000\n",
       "max      3165.000000\n",
       "Name: IE, dtype: float64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caly_df[\"IE\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0761d130",
   "metadata": {},
   "outputs": [],
   "source": [
    "caly_df.loc[(caly_df[\"IE\"]<=2096), \"Klasa\"] = \"Małe\"\n",
    "caly_df.loc[(2096<caly_df[\"IE\"]) & (caly_df[\"IE\"]<=2156), \"Klasa\"] = \"Średnie\"\n",
    "caly_df.loc[(2156<caly_df[\"IE\"]) & (caly_df[\"IE\"]<=2243), \"Klasa\"] = \"Wysokie\"\n",
    "caly_df.loc[caly_df[\"IE\"]>2243, \"Klasa\"] = \"Ekstrimum\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "910d75b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([475.,   0.,   0., 470.,   0.,   0., 466.,   0.,   0., 464.]),\n",
       " array([0. , 0.3, 0.6, 0.9, 1.2, 1.5, 1.8, 2.1, 2.4, 2.7, 3. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(caly_df[\"Klasa\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "958a64f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Średnie      475\n",
       "Małe         470\n",
       "Ekstrimum    466\n",
       "Wysokie      464\n",
       "Name: Klasa, dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caly_df[\"Klasa\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "268437e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IE',\n",
       " 'PI-PROD',\n",
       " 'PI-POKR',\n",
       " 'Prc',\n",
       " 'Psm',\n",
       " 'Pnr',\n",
       " 'Pw',\n",
       " 'PI-PLOD',\n",
       " 'CRj',\n",
       " 'CRk',\n",
       " 'PP',\n",
       " 'OMC',\n",
       " 'WH-KSOM',\n",
       " 'WH-DLUG',\n",
       " 'IP',\n",
       " 'kg ml',\n",
       " 'kg tł',\n",
       " '% tł',\n",
       " 'kg bi',\n",
       " '% bi',\n",
       " 'rc',\n",
       " 'sm',\n",
       " 'nr',\n",
       " 'w',\n",
       " 'og',\n",
       " 'DOLNOŚLĄSKIE',\n",
       " 'KUJAWSKO-POMORSKIE',\n",
       " 'LUBELSKIE',\n",
       " 'LUBUSKIE',\n",
       " 'MAZOWIECKIE',\n",
       " 'MAŁOPOLSKIE',\n",
       " 'OPOLSKIE',\n",
       " 'PODKARPACKIE',\n",
       " 'PODLASKIE',\n",
       " 'POMORSKIE',\n",
       " 'WARMIŃSKO-MAZURSKIE',\n",
       " 'WIELKOPOLSKIE',\n",
       " 'ZACHODNIOPOMORSKIE',\n",
       " 'ŁÓDZKIE',\n",
       " 'ŚLĄSKIE',\n",
       " 'ŚWIĘTOKRZYSKIE',\n",
       " 2015,\n",
       " 2016,\n",
       " 2017,\n",
       " 2018,\n",
       " 2019,\n",
       " 2020,\n",
       " 2021,\n",
       " 'Klasa']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(caly_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "33410b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(caly_df, test_size=0.2, random_state=42,stratify=caly_df['Klasa'])\n",
    "#test_df, val_df = train_test_split(test_df, test_size=0.5, random_state=42,stratify=test_df['Klasa'])\n",
    "\n",
    "del train_df['Klasa']\n",
    "del test_df['Klasa']\n",
    "#del val_df['Klasa']\n",
    "\n",
    "\n",
    "train_label=train_df['IE']\n",
    "test_label=test_df['IE']\n",
    "#val_label=val_df['IE']\n",
    "\n",
    "del train_df['IE']\n",
    "del test_df['IE']\n",
    "#del val_df['IE']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9cb06bd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PI-PROD',\n",
       " 'PI-POKR',\n",
       " 'Prc',\n",
       " 'Psm',\n",
       " 'Pnr',\n",
       " 'Pw',\n",
       " 'PI-PLOD',\n",
       " 'CRj',\n",
       " 'CRk',\n",
       " 'PP',\n",
       " 'OMC',\n",
       " 'WH-KSOM',\n",
       " 'WH-DLUG',\n",
       " 'IP',\n",
       " 'kg ml',\n",
       " 'kg tł',\n",
       " '% tł',\n",
       " 'kg bi',\n",
       " '% bi',\n",
       " 'rc',\n",
       " 'sm',\n",
       " 'nr',\n",
       " 'w',\n",
       " 'og',\n",
       " 'DOLNOŚLĄSKIE',\n",
       " 'KUJAWSKO-POMORSKIE',\n",
       " 'LUBELSKIE',\n",
       " 'LUBUSKIE',\n",
       " 'MAZOWIECKIE',\n",
       " 'MAŁOPOLSKIE',\n",
       " 'OPOLSKIE',\n",
       " 'PODKARPACKIE',\n",
       " 'PODLASKIE',\n",
       " 'POMORSKIE',\n",
       " 'WARMIŃSKO-MAZURSKIE',\n",
       " 'WIELKOPOLSKIE',\n",
       " 'ZACHODNIOPOMORSKIE',\n",
       " 'ŁÓDZKIE',\n",
       " 'ŚLĄSKIE',\n",
       " 'ŚWIĘTOKRZYSKIE',\n",
       " 2015,\n",
       " 2016,\n",
       " 2017,\n",
       " 2018,\n",
       " 2019,\n",
       " 2020,\n",
       " 2021]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8ef6a0b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(train_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-precipitation",
   "metadata": {},
   "source": [
    "## 1.2 Standaryzacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4bd3a3fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PI-PROD</th>\n",
       "      <th>PI-POKR</th>\n",
       "      <th>Prc</th>\n",
       "      <th>Psm</th>\n",
       "      <th>Pnr</th>\n",
       "      <th>Pw</th>\n",
       "      <th>PI-PLOD</th>\n",
       "      <th>CRj</th>\n",
       "      <th>CRk</th>\n",
       "      <th>PP</th>\n",
       "      <th>...</th>\n",
       "      <th>kg ml</th>\n",
       "      <th>kg tł</th>\n",
       "      <th>% tł</th>\n",
       "      <th>kg bi</th>\n",
       "      <th>% bi</th>\n",
       "      <th>rc</th>\n",
       "      <th>sm</th>\n",
       "      <th>nr</th>\n",
       "      <th>w</th>\n",
       "      <th>og</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>130</td>\n",
       "      <td>111</td>\n",
       "      <td>126</td>\n",
       "      <td>117</td>\n",
       "      <td>100</td>\n",
       "      <td>108</td>\n",
       "      <td>118</td>\n",
       "      <td>116</td>\n",
       "      <td>120</td>\n",
       "      <td>98</td>\n",
       "      <td>...</td>\n",
       "      <td>980</td>\n",
       "      <td>380</td>\n",
       "      <td>-2</td>\n",
       "      <td>417</td>\n",
       "      <td>12</td>\n",
       "      <td>113.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>117.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>135</td>\n",
       "      <td>109</td>\n",
       "      <td>116</td>\n",
       "      <td>108</td>\n",
       "      <td>104</td>\n",
       "      <td>106</td>\n",
       "      <td>120</td>\n",
       "      <td>119</td>\n",
       "      <td>117</td>\n",
       "      <td>96</td>\n",
       "      <td>...</td>\n",
       "      <td>1263</td>\n",
       "      <td>521</td>\n",
       "      <td>0</td>\n",
       "      <td>446</td>\n",
       "      <td>4</td>\n",
       "      <td>108.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>108.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>137</td>\n",
       "      <td>121</td>\n",
       "      <td>121</td>\n",
       "      <td>116</td>\n",
       "      <td>120</td>\n",
       "      <td>110</td>\n",
       "      <td>101</td>\n",
       "      <td>101</td>\n",
       "      <td>100</td>\n",
       "      <td>101</td>\n",
       "      <td>...</td>\n",
       "      <td>1237</td>\n",
       "      <td>666</td>\n",
       "      <td>18</td>\n",
       "      <td>419</td>\n",
       "      <td>2</td>\n",
       "      <td>118.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>117</td>\n",
       "      <td>125</td>\n",
       "      <td>101</td>\n",
       "      <td>103</td>\n",
       "      <td>114</td>\n",
       "      <td>127</td>\n",
       "      <td>123</td>\n",
       "      <td>115</td>\n",
       "      <td>130</td>\n",
       "      <td>114</td>\n",
       "      <td>...</td>\n",
       "      <td>723</td>\n",
       "      <td>305</td>\n",
       "      <td>1</td>\n",
       "      <td>227</td>\n",
       "      <td>-1</td>\n",
       "      <td>102.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>117.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>133</td>\n",
       "      <td>109</td>\n",
       "      <td>104</td>\n",
       "      <td>113</td>\n",
       "      <td>102</td>\n",
       "      <td>108</td>\n",
       "      <td>112</td>\n",
       "      <td>111</td>\n",
       "      <td>116</td>\n",
       "      <td>98</td>\n",
       "      <td>...</td>\n",
       "      <td>1254</td>\n",
       "      <td>485</td>\n",
       "      <td>-3</td>\n",
       "      <td>425</td>\n",
       "      <td>2</td>\n",
       "      <td>108.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>112.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>125</td>\n",
       "      <td>118</td>\n",
       "      <td>117</td>\n",
       "      <td>112</td>\n",
       "      <td>100</td>\n",
       "      <td>119</td>\n",
       "      <td>110</td>\n",
       "      <td>108</td>\n",
       "      <td>113</td>\n",
       "      <td>102</td>\n",
       "      <td>...</td>\n",
       "      <td>845</td>\n",
       "      <td>467</td>\n",
       "      <td>14</td>\n",
       "      <td>307</td>\n",
       "      <td>4</td>\n",
       "      <td>110.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>118.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>143</td>\n",
       "      <td>129</td>\n",
       "      <td>116</td>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "      <td>126</td>\n",
       "      <td>117</td>\n",
       "      <td>115</td>\n",
       "      <td>119</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>1448</td>\n",
       "      <td>694</td>\n",
       "      <td>11</td>\n",
       "      <td>509</td>\n",
       "      <td>4</td>\n",
       "      <td>113.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>125.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>135</td>\n",
       "      <td>121</td>\n",
       "      <td>107</td>\n",
       "      <td>115</td>\n",
       "      <td>105</td>\n",
       "      <td>123</td>\n",
       "      <td>105</td>\n",
       "      <td>104</td>\n",
       "      <td>104</td>\n",
       "      <td>104</td>\n",
       "      <td>...</td>\n",
       "      <td>1113</td>\n",
       "      <td>561</td>\n",
       "      <td>12</td>\n",
       "      <td>437</td>\n",
       "      <td>9</td>\n",
       "      <td>107.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>123.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226</th>\n",
       "      <td>128</td>\n",
       "      <td>124</td>\n",
       "      <td>104</td>\n",
       "      <td>108</td>\n",
       "      <td>110</td>\n",
       "      <td>127</td>\n",
       "      <td>106</td>\n",
       "      <td>103</td>\n",
       "      <td>109</td>\n",
       "      <td>109</td>\n",
       "      <td>...</td>\n",
       "      <td>1111</td>\n",
       "      <td>407</td>\n",
       "      <td>-5</td>\n",
       "      <td>380</td>\n",
       "      <td>2</td>\n",
       "      <td>105.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>120.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1270</th>\n",
       "      <td>121</td>\n",
       "      <td>115</td>\n",
       "      <td>117</td>\n",
       "      <td>108</td>\n",
       "      <td>103</td>\n",
       "      <td>115</td>\n",
       "      <td>103</td>\n",
       "      <td>101</td>\n",
       "      <td>111</td>\n",
       "      <td>103</td>\n",
       "      <td>...</td>\n",
       "      <td>735</td>\n",
       "      <td>395</td>\n",
       "      <td>11</td>\n",
       "      <td>264</td>\n",
       "      <td>3</td>\n",
       "      <td>104.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>114.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      PI-PROD  PI-POKR  Prc  Psm  Pnr   Pw  PI-PLOD  CRj  CRk   PP  ...  \\\n",
       "974       130      111  126  117  100  108      118  116  120   98  ...   \n",
       "84        135      109  116  108  104  106      120  119  117   96  ...   \n",
       "753       137      121  121  116  120  110      101  101  100  101  ...   \n",
       "643       117      125  101  103  114  127      123  115  130  114  ...   \n",
       "1438      133      109  104  113  102  108      112  111  116   98  ...   \n",
       "...       ...      ...  ...  ...  ...  ...      ...  ...  ...  ...  ...   \n",
       "143       125      118  117  112  100  119      110  108  113  102  ...   \n",
       "650       143      129  116  115  115  126      117  115  119  100  ...   \n",
       "732       135      121  107  115  105  123      105  104  104  104  ...   \n",
       "1226      128      124  104  108  110  127      106  103  109  109  ...   \n",
       "1270      121      115  117  108  103  115      103  101  111  103  ...   \n",
       "\n",
       "      kg ml  kg tł  % tł  kg bi  % bi     rc     sm     nr      w     og  \n",
       "974     980    380    -2    417    12  113.0  110.0  103.0  108.0  117.0  \n",
       "84     1263    521     0    446     4  108.0  105.0  110.0  108.0  108.0  \n",
       "753    1237    666    18    419     2  118.0  115.0  126.0  108.0  121.0  \n",
       "643     723    305     1    227    -1  102.0  101.0  124.0  127.0  117.0  \n",
       "1438   1254    485    -3    425     2  108.0  108.0  116.0  111.0  112.0  \n",
       "...     ...    ...   ...    ...   ...    ...    ...    ...    ...    ...  \n",
       "143     845    467    14    307     4  110.0  108.0  101.0  118.0  118.0  \n",
       "650    1448    694    11    509     4  113.0  109.0  122.0  126.0  125.0  \n",
       "732    1113    561    12    437     9  107.0  109.0  109.0  119.0  123.0  \n",
       "1226   1111    407    -5    380     2  105.0  103.0  118.0  128.0  120.0  \n",
       "1270    735    395    11    264     3  104.0  104.0  108.0  116.0  114.0  \n",
       "\n",
       "[1500 rows x 24 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.iloc[:,:24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "indonesian-majority",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train_df.iloc[:,:24]=scaler.fit_transform(train_df.iloc[:,:24])\n",
    "test_df.iloc[:,:24]=scaler.fit_transform(test_df.iloc[:,:24])\n",
    "#val_df.iloc[:,14:]=scaler.fit_transform(val_df.iloc[:,14:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "dcc66d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sm</th>\n",
       "      <th>nr</th>\n",
       "      <th>w</th>\n",
       "      <th>og</th>\n",
       "      <th>DOLNOŚLĄSKIE</th>\n",
       "      <th>KUJAWSKO-POMORSKIE</th>\n",
       "      <th>LUBELSKIE</th>\n",
       "      <th>LUBUSKIE</th>\n",
       "      <th>MAZOWIECKIE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>0.891719</td>\n",
       "      <td>-1.253508</td>\n",
       "      <td>-1.582152</td>\n",
       "      <td>0.297493</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>-0.188542</td>\n",
       "      <td>-0.294750</td>\n",
       "      <td>-1.582152</td>\n",
       "      <td>-1.048403</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>1.971980</td>\n",
       "      <td>1.896698</td>\n",
       "      <td>-1.582152</td>\n",
       "      <td>0.895669</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>-1.052750</td>\n",
       "      <td>1.622767</td>\n",
       "      <td>1.088977</td>\n",
       "      <td>0.297493</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438</th>\n",
       "      <td>0.459615</td>\n",
       "      <td>0.527043</td>\n",
       "      <td>-1.160395</td>\n",
       "      <td>-0.450227</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0.459615</td>\n",
       "      <td>-1.527439</td>\n",
       "      <td>-0.176295</td>\n",
       "      <td>0.447037</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>0.675667</td>\n",
       "      <td>1.348836</td>\n",
       "      <td>0.948391</td>\n",
       "      <td>1.493845</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>0.675667</td>\n",
       "      <td>-0.431715</td>\n",
       "      <td>-0.035709</td>\n",
       "      <td>1.194757</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226</th>\n",
       "      <td>-0.620646</td>\n",
       "      <td>0.800974</td>\n",
       "      <td>1.229563</td>\n",
       "      <td>0.746125</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1270</th>\n",
       "      <td>-0.404594</td>\n",
       "      <td>-0.568681</td>\n",
       "      <td>-0.457466</td>\n",
       "      <td>-0.151139</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            sm        nr         w        og  DOLNOŚLĄSKIE  \\\n",
       "974   0.891719 -1.253508 -1.582152  0.297493             0   \n",
       "84   -0.188542 -0.294750 -1.582152 -1.048403             0   \n",
       "753   1.971980  1.896698 -1.582152  0.895669             0   \n",
       "643  -1.052750  1.622767  1.088977  0.297493             0   \n",
       "1438  0.459615  0.527043 -1.160395 -0.450227             0   \n",
       "...        ...       ...       ...       ...           ...   \n",
       "143   0.459615 -1.527439 -0.176295  0.447037             0   \n",
       "650   0.675667  1.348836  0.948391  1.493845             0   \n",
       "732   0.675667 -0.431715 -0.035709  1.194757             0   \n",
       "1226 -0.620646  0.800974  1.229563  0.746125             0   \n",
       "1270 -0.404594 -0.568681 -0.457466 -0.151139             0   \n",
       "\n",
       "      KUJAWSKO-POMORSKIE  LUBELSKIE  LUBUSKIE  MAZOWIECKIE  \n",
       "974                    0          0         0            0  \n",
       "84                     0          0         0            0  \n",
       "753                    0          0         0            0  \n",
       "643                    0          0         0            0  \n",
       "1438                   0          0         0            0  \n",
       "...                  ...        ...       ...          ...  \n",
       "143                    0          0         0            0  \n",
       "650                    0          0         0            0  \n",
       "732                    0          0         0            0  \n",
       "1226                   0          0         0            0  \n",
       "1270                   0          0         0            0  \n",
       "\n",
       "[1500 rows x 9 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.iloc[:,20:29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "humanitarian-sperm",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 47)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "83dd9a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(375, 47)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d360ef87",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = np.array(train_df)\n",
    "test_df = np.array(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e8ff5748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.45476225, -0.93083361,  2.37267516, ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [ 1.23222221, -1.21598635,  1.10580699, ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [ 1.5432062 ,  0.49493011,  1.73924108, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ...,\n",
       "       [ 1.23222221,  0.49493011, -0.03437436, ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [ 0.14377826,  0.92265923, -0.41443481, ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [-0.94466569, -0.36052812,  1.23249381, ...,  1.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b312464b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = np.array(train_label)\n",
    "test_label = np.array(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b23fc815",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2085, 2142, 2037, ..., 2128, 2096, 2058], dtype=int64)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "perfect-atlas",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS=['mae']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twelve-proportion",
   "metadata": {},
   "source": [
    "# 2 Moduł TALOS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fbc6dd",
   "metadata": {},
   "source": [
    "Zamiast dobierać wszystkie hiperparametry na nowo, sprawdzę czy siec preferuje inne w waskim zakresie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-stick",
   "metadata": {},
   "source": [
    "## 2.1 Słownik parametrów do wypróbowania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "experimental-ladder",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = {'first_neuron':[160,320], #\n",
    "     'hidden_neuron':[50,100],#\n",
    "     'hidden_layers':[1,2],  #\n",
    "     'batch_size': [32,64], #\n",
    "     'optimizer': ['adam'],# do zrobienia potem\n",
    "     'kernel_initializer': ['normal'], # do zrobienia potem\n",
    "     'epochs': [2000], # never touch it\n",
    "     'dropout': [0],  # po dopasowaniu znowu nie bedzie potrzebne\n",
    "     'activation_layer':['relu'], # do zrobienia potem\n",
    "     'batc_normalization':[False], # do zrobienia potem\n",
    "     'last_activation': ['linear']} #never touch it\n",
    "#     'optimizer': ['rmsprop','adam','adadelta','adamax','nadam','adagrad'],\n",
    "#     'kernel_initializer': ['orthogonal','identity','zeros','ones','uniform'],\n",
    "#    'activation_layer':['sigmoid','tanh','selu','elu','relu'],"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-thanksgiving",
   "metadata": {},
   "source": [
    "## 2.2 Tworzę funkcję do tworzenia instancji modelu keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "rapid-humanity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerai_model(x_train, y_train, x_val, y_val, params):\n",
    "    print(params)\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    ## initial layer\n",
    "    model.add(Dense(params['first_neuron'], input_dim=x_train.shape[1],\n",
    "                    activation='relu',\n",
    "                    kernel_initializer = params['kernel_initializer'] ))\n",
    "    if params['batc_normalization']==True:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Dropout(params['dropout']))\n",
    "    \n",
    "    ## hidden layers\n",
    "    for i in range(params['hidden_layers']):\n",
    "        print (f\"adding layer {i+1}\")\n",
    "        model.add(Dense(params['hidden_neuron'], activation='relu',\n",
    "                    kernel_initializer=params['kernel_initializer']))\n",
    "        if params['batc_normalization']==True:\n",
    "            model.add(BatchNormalization())\n",
    "        model.add(Dropout(params['dropout']))\n",
    "    \n",
    "    \n",
    "    ## final layer\n",
    "    model.add(Dense(1, activation=params['last_activation'],\n",
    "                    kernel_initializer=params['kernel_initializer']))\n",
    "    \n",
    "    model.compile(loss='mean_absolute_error', \n",
    "                  optimizer=params['optimizer'])\n",
    "    \n",
    "    history = model.fit(x_train, y_train, \n",
    "                        validation_data=[x_val, y_val],\n",
    "                        batch_size=params['batch_size'],\n",
    "                        epochs=params['epochs'],\n",
    "                        callbacks = [early_stopper(params['epochs'], patience=10,monitor='val_loss')] #,ta.live(),\n",
    "                        )\n",
    "\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-married",
   "metadata": {},
   "source": [
    "## 2.3 Przeprowadzam skan, używając parametrów i funkcji wyżej\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "muslim-picnic",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'batc_normalization': False, 'batch_size': 32, 'dropout': 0, 'epochs': 2000, 'first_neuron': 160, 'hidden_layers': 1, 'hidden_neuron': 50, 'kernel_initializer': 'normal', 'last_activation': 'linear', 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "Train on 1500 samples, validate on 375 samples\n",
      "Epoch 1/2000\n",
      "1500/1500 [==============================] - 2s 1ms/sample - loss: 2184.5754 - val_loss: 2181.2981\n",
      "Epoch 2/2000\n",
      "1500/1500 [==============================] - 0s 62us/sample - loss: 2162.0041 - val_loss: 2130.1087\n",
      "Epoch 3/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 2053.1653 - val_loss: 1944.8225\n",
      "Epoch 4/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 1757.5706 - val_loss: 1521.2986\n",
      "Epoch 5/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 1191.9752 - val_loss: 869.0474\n",
      "Epoch 6/2000\n",
      "1500/1500 [==============================] - 0s 34us/sample - loss: 702.4650 - val_loss: 607.1395\n",
      "Epoch 7/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 578.0247 - val_loss: 521.7681\n",
      "Epoch 8/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 508.5426 - val_loss: 458.5617\n",
      "Epoch 9/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 445.8688 - val_loss: 400.9100\n",
      "Epoch 10/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 384.2834 - val_loss: 341.5319\n",
      "Epoch 11/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 323.5080 - val_loss: 283.0203\n",
      "Epoch 12/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 260.6604 - val_loss: 222.6249\n",
      "Epoch 13/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 202.5763 - val_loss: 175.8228\n",
      "Epoch 14/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 156.8145 - val_loss: 137.4625\n",
      "Epoch 15/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 123.2989 - val_loss: 110.8882\n",
      "Epoch 16/2000\n",
      "1500/1500 [==============================] - 0s 34us/sample - loss: 99.5365 - val_loss: 88.3905\n",
      "Epoch 17/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 80.6079 - val_loss: 72.2317\n",
      "Epoch 18/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 65.6189 - val_loss: 62.2437\n",
      "Epoch 19/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 55.1446 - val_loss: 53.5763\n",
      "Epoch 20/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 47.2290 - val_loss: 45.4185\n",
      "Epoch 21/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 41.1862 - val_loss: 40.7775\n",
      "Epoch 22/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 37.3857 - val_loss: 39.7750\n",
      "Epoch 23/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 34.3129 - val_loss: 36.7262\n",
      "Epoch 24/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 32.8627 - val_loss: 34.7405\n",
      "Epoch 25/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 30.7445 - val_loss: 33.6697\n",
      "Epoch 26/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 29.6867 - val_loss: 31.5712\n",
      "Epoch 27/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 28.6292 - val_loss: 34.3855\n",
      "Epoch 28/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 27.7751 - val_loss: 31.0267\n",
      "Epoch 29/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 27.0532 - val_loss: 28.4862\n",
      "Epoch 30/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 25.9981 - val_loss: 28.0031\n",
      "Epoch 31/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 25.3086 - val_loss: 30.7279\n",
      "Epoch 32/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 24.9731 - val_loss: 27.4506\n",
      "Epoch 33/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 24.2188 - val_loss: 27.6623\n",
      "Epoch 34/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 25.1898 - val_loss: 26.9348\n",
      "Epoch 35/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 23.7750 - val_loss: 28.3785\n",
      "Epoch 36/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 23.7128 - val_loss: 26.6040\n",
      "Epoch 37/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 23.5868 - val_loss: 25.4722\n",
      "Epoch 38/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 22.8682 - val_loss: 23.5823\n",
      "Epoch 39/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 23.1971 - val_loss: 24.0194\n",
      "Epoch 40/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 23.3252 - val_loss: 25.9504\n",
      "Epoch 41/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 22.8301 - val_loss: 24.2627\n",
      "Epoch 42/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 22.8802 - val_loss: 26.1716\n",
      "Epoch 43/2000\n",
      "1500/1500 [==============================] - 0s 34us/sample - loss: 23.0675 - val_loss: 24.4439\n",
      "Epoch 44/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 22.8835 - val_loss: 25.1278\n",
      "Epoch 45/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 22.0994 - val_loss: 23.7803\n",
      "Epoch 46/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 22.8957 - val_loss: 22.9330\n",
      "Epoch 47/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 23.3061 - val_loss: 23.3427\n",
      "Epoch 48/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 22.6233 - val_loss: 23.1917\n",
      "Epoch 49/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 22.5891 - val_loss: 25.3634\n",
      "Epoch 50/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 22.5316 - val_loss: 24.3891\n",
      "Epoch 51/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 22.6171 - val_loss: 24.8839\n",
      "Epoch 52/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 22.4864 - val_loss: 22.8424\n",
      "Epoch 53/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.8440 - val_loss: 22.9781\n",
      "Epoch 54/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 22.3777 - val_loss: 24.6632\n",
      "Epoch 55/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 21.8056 - val_loss: 23.8693\n",
      "Epoch 56/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 22.2679 - val_loss: 23.9553\n",
      "Epoch 57/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 21.8820 - val_loss: 23.7722\n",
      "Epoch 58/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 22.4441 - val_loss: 22.7820\n",
      "Epoch 59/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 22.6398 - val_loss: 25.3848\n",
      "Epoch 60/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 22.1928 - val_loss: 25.5297\n",
      "Epoch 61/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 21.5121 - val_loss: 23.7319\n",
      "Epoch 62/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 22.9654 - val_loss: 23.9528\n",
      "Epoch 63/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 22.3910 - val_loss: 24.3458\n",
      "Epoch 64/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 22.1628 - val_loss: 23.4264\n",
      "Epoch 65/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.9046 - val_loss: 24.4915\n",
      "Epoch 66/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 21.5921 - val_loss: 23.3881\n",
      "Epoch 67/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.7092 - val_loss: 23.8825\n",
      "Epoch 68/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 21.8263 - val_loss: 25.8214\n",
      "Epoch 69/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.6799 - val_loss: 23.6604\n",
      "Epoch 70/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 21.6368 - val_loss: 24.3867\n",
      "Epoch 71/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.6709 - val_loss: 22.9289\n",
      "Epoch 72/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 21.2952 - val_loss: 24.4653\n",
      "Epoch 73/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 21.8907 - val_loss: 23.3314\n",
      "Epoch 74/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 22.3621 - val_loss: 25.5916\n",
      "Epoch 75/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 22.0414 - val_loss: 22.7756\n",
      "Epoch 76/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 21.6815 - val_loss: 24.3464\n",
      "Epoch 77/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 22.1151 - val_loss: 24.2292\n",
      "Epoch 78/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 22.0925 - val_loss: 24.2254\n",
      "Epoch 79/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.7211 - val_loss: 23.8980\n",
      "Epoch 80/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 21.7118 - val_loss: 22.5069\n",
      "Epoch 81/2000\n",
      "1500/1500 [==============================] - 0s 34us/sample - loss: 22.1152 - val_loss: 23.5698\n",
      "Epoch 82/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 22.2873 - val_loss: 23.7537\n",
      "Epoch 83/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 22.4475 - val_loss: 23.1185\n",
      "Epoch 84/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.5986 - val_loss: 23.5815\n",
      "Epoch 85/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.5047 - val_loss: 22.9177\n",
      "Epoch 86/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 21.8275 - val_loss: 24.4783\n",
      "Epoch 87/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 21.2536 - val_loss: 23.9861\n",
      "Epoch 88/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 21.8333 - val_loss: 24.2653\n",
      "Epoch 89/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 21.7182 - val_loss: 23.2030\n",
      "Epoch 90/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 21.0003 - val_loss: 24.1077\n",
      "Epoch 91/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 22.1986 - val_loss: 24.3304\n",
      "Epoch 92/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.5504 - val_loss: 23.2067\n",
      "Epoch 93/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 21.4296 - val_loss: 25.8529\n",
      "Epoch 94/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 21.4634 - val_loss: 24.0993\n",
      "Epoch 95/2000\n",
      "1500/1500 [==============================] - 0s 34us/sample - loss: 21.2549 - val_loss: 25.5157\n",
      "Epoch 96/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.9642 - val_loss: 23.5728\n",
      "Epoch 97/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 21.4234 - val_loss: 25.7200\n",
      "Epoch 98/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.4436 - val_loss: 23.6927\n",
      "Epoch 99/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.0133 - val_loss: 23.4965\n",
      "Epoch 100/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.0509 - val_loss: 22.5902\n",
      "Epoch 101/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.1163 - val_loss: 23.5087\n",
      "Epoch 102/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.2600 - val_loss: 23.1251\n",
      "Epoch 103/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 21.0636 - val_loss: 24.2855\n",
      "Epoch 104/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.3150 - val_loss: 23.3472\n",
      "Epoch 105/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.6435 - val_loss: 24.8956\n",
      "Epoch 106/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.7358 - val_loss: 24.5200\n",
      "Epoch 107/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.4177 - val_loss: 24.0800\n",
      "Epoch 108/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 21.1157 - val_loss: 24.9487\n",
      "Epoch 109/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 21.1381 - val_loss: 24.4133\n",
      "Epoch 110/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 21.2701 - val_loss: 24.5787\n",
      "Epoch 111/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.2515 - val_loss: 23.4748\n",
      "Epoch 112/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.4423 - val_loss: 23.7993\n",
      "Epoch 113/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.7294 - val_loss: 25.4666\n",
      "Epoch 114/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 21.4331 - val_loss: 24.8253\n",
      "Epoch 115/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 21.1759 - val_loss: 23.2989\n",
      "Epoch 116/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.8751 - val_loss: 23.1204\n",
      "Epoch 117/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.8916 - val_loss: 24.0243\n",
      "Epoch 118/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.6813 - val_loss: 23.6294\n",
      "Epoch 119/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.3585 - val_loss: 23.7240\n",
      "Epoch 120/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 21.6897 - val_loss: 24.2679\n",
      "Epoch 121/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.9696 - val_loss: 22.4883\n",
      "Epoch 122/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.9353 - val_loss: 27.3196\n",
      "Epoch 123/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.4143 - val_loss: 23.6624\n",
      "Epoch 124/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 20.7431 - val_loss: 22.8232\n",
      "Epoch 125/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.1595 - val_loss: 22.8810\n",
      "Epoch 126/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.4544 - val_loss: 22.9670\n",
      "Epoch 127/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.8337 - val_loss: 22.9969\n",
      "Epoch 128/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 21.1605 - val_loss: 23.2461\n",
      "Epoch 129/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 21.0201 - val_loss: 24.0705\n",
      "Epoch 130/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.8783 - val_loss: 23.2852\n",
      "Epoch 131/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.9038 - val_loss: 23.4885\n",
      "Epoch 132/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.5915 - val_loss: 26.6349\n",
      "Epoch 133/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.9891 - val_loss: 23.3519\n",
      "Epoch 134/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 21.1557 - val_loss: 24.1617\n",
      "Epoch 135/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.7792 - val_loss: 22.6765\n",
      "Epoch 136/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 20.3366 - val_loss: 22.7449\n",
      "Epoch 137/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 21.0020 - val_loss: 22.7530\n",
      "Epoch 138/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 21.1711 - val_loss: 24.4271\n",
      "Epoch 139/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.9590 - val_loss: 23.8303\n",
      "Epoch 140/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 21.2144 - val_loss: 22.9255\n",
      "Epoch 141/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 21.1619 - val_loss: 24.7168\n",
      "Epoch 142/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 21.4893 - val_loss: 23.0666\n",
      "Epoch 143/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.5418 - val_loss: 23.8719\n",
      "Epoch 144/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 21.2057 - val_loss: 22.7973\n",
      "Epoch 145/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 21.0670 - val_loss: 23.4182\n",
      "Epoch 146/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.5501 - val_loss: 23.6119\n",
      "Epoch 147/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.8181 - val_loss: 23.5835\n",
      "Epoch 148/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.0566 - val_loss: 23.5770\n",
      "Epoch 149/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 20.6141 - val_loss: 23.5444\n",
      "Epoch 150/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 20.5431 - val_loss: 25.4720\n",
      "Epoch 151/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.9935 - val_loss: 25.0805\n",
      "Epoch 152/2000\n",
      "1500/1500 [==============================] - 0s 34us/sample - loss: 20.5030 - val_loss: 23.6766\n",
      "Epoch 153/2000\n",
      "1500/1500 [==============================] - 0s 34us/sample - loss: 20.8794 - val_loss: 23.4154\n",
      "Epoch 154/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.8365 - val_loss: 24.2369\n",
      "Epoch 155/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.8502 - val_loss: 25.5392\n",
      "Epoch 156/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.4266 - val_loss: 23.9411\n",
      "Epoch 157/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 21.8894 - val_loss: 22.8987\n",
      "Epoch 158/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.3966 - val_loss: 23.8636\n",
      "Epoch 159/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.3576 - val_loss: 22.8485\n",
      "Epoch 160/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.8282 - val_loss: 24.1670\n",
      "Epoch 161/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.6977 - val_loss: 22.8713\n",
      "Epoch 162/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.3987 - val_loss: 23.6549\n",
      "Epoch 163/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 21.8436 - val_loss: 23.9045\n",
      "Epoch 164/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.0253 - val_loss: 22.1088\n",
      "Epoch 165/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.8883 - val_loss: 25.0405\n",
      "Epoch 166/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 21.3816 - val_loss: 22.2985\n",
      "Epoch 167/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.4847 - val_loss: 24.1313\n",
      "Epoch 168/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.4585 - val_loss: 24.2628\n",
      "Epoch 169/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.7411 - val_loss: 22.8539\n",
      "Epoch 170/2000\n",
      "1500/1500 [==============================] - 0s 34us/sample - loss: 20.4128 - val_loss: 24.5879\n",
      "Epoch 171/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.9582 - val_loss: 23.1028\n",
      "Epoch 172/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.8775 - val_loss: 25.1577\n",
      "Epoch 173/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.6663 - val_loss: 22.7883\n",
      "Epoch 174/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 21.8397 - val_loss: 25.2862\n",
      "Epoch 175/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.8204 - val_loss: 23.3359\n",
      "Epoch 176/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.3055 - val_loss: 24.1562\n",
      "Epoch 177/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.0333 - val_loss: 23.4917\n",
      "Epoch 178/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.3299 - val_loss: 23.4992\n",
      "Epoch 179/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.7373 - val_loss: 24.9824\n",
      "Epoch 180/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.2588 - val_loss: 22.8313\n",
      "Epoch 181/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.3096 - val_loss: 24.8331\n",
      "Epoch 182/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.7660 - val_loss: 23.4076\n",
      "Epoch 183/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.8300 - val_loss: 22.7926\n",
      "Epoch 184/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.9157 - val_loss: 23.9725\n",
      "Epoch 185/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.6866 - val_loss: 23.9976\n",
      "Epoch 186/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.1761 - val_loss: 24.0748\n",
      "Epoch 187/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.3950 - val_loss: 24.3303\n",
      "Epoch 188/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.9864 - val_loss: 23.0279\n",
      "Epoch 189/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 21.3311 - val_loss: 23.2944\n",
      "Epoch 190/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 21.0237 - val_loss: 24.0977\n",
      "Epoch 191/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.6023 - val_loss: 23.2259\n",
      "Epoch 192/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.2625 - val_loss: 22.8059\n",
      "Epoch 193/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.4278 - val_loss: 22.4556\n",
      "Epoch 194/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.2503 - val_loss: 22.9457\n",
      "Epoch 195/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.2999 - val_loss: 24.0330\n",
      "Epoch 196/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.7256 - val_loss: 22.6688\n",
      "Epoch 197/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.4233 - val_loss: 23.8377\n",
      "Epoch 198/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.5641 - val_loss: 22.1917\n",
      "Epoch 199/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.1754 - val_loss: 23.3572\n",
      "Epoch 200/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.1368 - val_loss: 23.8565\n",
      "Epoch 201/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.3253 - val_loss: 26.3783\n",
      "Epoch 202/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.8492 - val_loss: 22.5569\n",
      "Epoch 203/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.3641 - val_loss: 23.2796\n",
      "Epoch 204/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.2699 - val_loss: 25.5133\n",
      "Epoch 205/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.8573 - val_loss: 24.1416\n",
      "Epoch 206/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.6049 - val_loss: 23.5733\n",
      "Epoch 207/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 21.0798 - val_loss: 22.7060\n",
      "Epoch 208/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.1504 - val_loss: 22.8125\n",
      "Epoch 209/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 19.9672 - val_loss: 22.3235\n",
      "Epoch 210/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.5604 - val_loss: 23.1905\n",
      "Epoch 211/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.2416 - val_loss: 24.3310\n",
      "Epoch 212/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.4013 - val_loss: 23.1672\n",
      "Epoch 213/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.3462 - val_loss: 22.7677\n",
      "Epoch 214/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.0813 - val_loss: 23.2843\n",
      "Epoch 215/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.8216 - val_loss: 23.8354\n",
      "Epoch 216/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 19.8943 - val_loss: 23.0927\n",
      "Epoch 217/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.3740 - val_loss: 24.7787\n",
      "Epoch 218/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.4744 - val_loss: 23.8697\n",
      "Epoch 219/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.0937 - val_loss: 23.5509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 220/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.2118 - val_loss: 22.4062\n",
      "Epoch 221/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.2470 - val_loss: 23.6366\n",
      "Epoch 222/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.1349 - val_loss: 24.1653\n",
      "Epoch 223/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.4451 - val_loss: 22.9222\n",
      "Epoch 224/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.5721 - val_loss: 22.3789\n",
      "Epoch 225/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.3954 - val_loss: 24.5249\n",
      "Epoch 226/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.3478 - val_loss: 23.2150\n",
      "Epoch 227/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.9841 - val_loss: 23.5165\n",
      "Epoch 228/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.5139 - val_loss: 23.5570\n",
      "Epoch 229/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.2503 - val_loss: 23.9328\n",
      "Epoch 230/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.8655 - val_loss: 22.2996\n",
      "Epoch 231/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.6873 - val_loss: 22.6631\n",
      "Epoch 232/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.0973 - val_loss: 22.3866\n",
      "Epoch 233/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 19.8249 - val_loss: 23.9950\n",
      "Epoch 234/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.7787 - val_loss: 22.8429\n",
      "Epoch 235/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.7373 - val_loss: 22.9770\n",
      "Epoch 236/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.1450 - val_loss: 24.6901\n",
      "Epoch 237/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.1632 - val_loss: 23.6493\n",
      "Epoch 238/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.1703 - val_loss: 25.6767\n",
      "Epoch 239/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.7941 - val_loss: 22.0121\n",
      "Epoch 240/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.1933 - val_loss: 23.9599\n",
      "Epoch 241/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.8148 - val_loss: 23.5757\n",
      "Epoch 242/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.9595 - val_loss: 23.6662\n",
      "Epoch 243/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.7047 - val_loss: 21.7878\n",
      "Epoch 244/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.0172 - val_loss: 24.8649\n",
      "Epoch 245/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.7491 - val_loss: 22.3795\n",
      "Epoch 246/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.2489 - val_loss: 22.9165\n",
      "Epoch 247/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.3923 - val_loss: 22.8483\n",
      "Epoch 248/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.9057 - val_loss: 25.6886\n",
      "Epoch 249/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.7148 - val_loss: 23.1964\n",
      "Epoch 250/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 19.9109 - val_loss: 24.0243\n",
      "Epoch 251/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.0542 - val_loss: 23.2229\n",
      "Epoch 252/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.7254 - val_loss: 22.4111\n",
      "Epoch 253/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.8574 - val_loss: 23.9631\n",
      "Epoch 254/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.0563 - val_loss: 22.8427\n",
      "Epoch 255/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.2812 - val_loss: 23.1180\n",
      "Epoch 256/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 19.9202 - val_loss: 23.0727\n",
      "Epoch 257/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.0387 - val_loss: 22.9203\n",
      "Epoch 258/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.9032 - val_loss: 23.7227\n",
      "Epoch 259/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.6705 - val_loss: 22.3518\n",
      "Epoch 260/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 19.9125 - val_loss: 21.9807\n",
      "Epoch 261/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 19.5976 - val_loss: 22.0595\n",
      "Epoch 262/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 19.7736 - val_loss: 23.0138\n",
      "Epoch 263/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 19.9748 - val_loss: 23.6166\n",
      "Epoch 264/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.1867 - val_loss: 24.1937\n",
      "Epoch 265/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.8176 - val_loss: 23.5694\n",
      "Epoch 266/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.8812 - val_loss: 22.8887\n",
      "Epoch 267/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 21.1920 - val_loss: 23.3363\n",
      "Epoch 268/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.4071 - val_loss: 22.5095\n",
      "Epoch 269/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 19.8767 - val_loss: 24.4254\n",
      "Epoch 270/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.9955 - val_loss: 23.8537\n",
      "Epoch 271/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.5502 - val_loss: 23.3557\n",
      "Epoch 272/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 19.7037 - val_loss: 22.5575\n",
      "Epoch 273/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.7289 - val_loss: 23.0606\n",
      "Epoch 274/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.9344 - val_loss: 23.7597\n",
      "Epoch 275/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 19.6076 - val_loss: 22.5210\n",
      "Epoch 276/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.9861 - val_loss: 23.7403\n",
      "Epoch 277/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.2574 - val_loss: 23.9395\n",
      "Epoch 278/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.6987 - val_loss: 22.4412\n",
      "Epoch 279/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.5688 - val_loss: 23.0098\n",
      "Epoch 280/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.0383 - val_loss: 22.9660\n",
      "Epoch 281/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.7517 - val_loss: 23.2373\n",
      "Epoch 282/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 19.9773 - val_loss: 22.4803\n",
      "Epoch 283/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.3205 - val_loss: 23.6757\n",
      "Epoch 284/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.7845 - val_loss: 22.9584\n",
      "Epoch 285/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.5539 - val_loss: 22.5110\n",
      "Epoch 286/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.2627 - val_loss: 23.3771\n",
      "Epoch 287/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.9695 - val_loss: 23.1319\n",
      "Epoch 288/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.3761 - val_loss: 23.4826\n",
      "Epoch 289/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.8343 - val_loss: 23.9841\n",
      "Epoch 290/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.0217 - val_loss: 23.3092\n",
      "Epoch 291/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 19.6254 - val_loss: 23.1217\n",
      "Epoch 292/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.7430 - val_loss: 24.8444\n",
      "Epoch 293/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.5890 - val_loss: 22.6701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 294/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.7913 - val_loss: 23.3050\n",
      "Epoch 295/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.1578 - val_loss: 23.5849\n",
      "Epoch 296/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 19.7997 - val_loss: 23.2061\n",
      "Epoch 297/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.3948 - val_loss: 24.2981\n",
      "Epoch 298/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 19.6333 - val_loss: 23.4277\n",
      "Epoch 299/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.7454 - val_loss: 23.5680\n",
      "Epoch 300/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.1592 - val_loss: 23.8928\n",
      "Epoch 301/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.2049 - val_loss: 23.5050\n",
      "Epoch 302/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.2294 - val_loss: 23.0486\n",
      "Epoch 303/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.5291 - val_loss: 22.4514\n",
      "Epoch 304/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.4153 - val_loss: 24.0789\n",
      "Epoch 305/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.7225 - val_loss: 23.0140\n",
      "Epoch 306/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.5872 - val_loss: 25.2182\n",
      "Epoch 307/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.8689 - val_loss: 22.6747\n",
      "Epoch 308/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 19.6814 - val_loss: 23.2012\n",
      "Epoch 309/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.6354 - val_loss: 23.3053\n",
      "Epoch 310/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.1010 - val_loss: 24.1505\n",
      "Epoch 311/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.0661 - val_loss: 22.3655\n",
      "Epoch 312/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.3889 - val_loss: 22.8812\n",
      "Epoch 313/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.4143 - val_loss: 23.9947\n",
      "Epoch 314/2000\n",
      "1500/1500 [==============================] - 0s 34us/sample - loss: 19.4636 - val_loss: 23.2557\n",
      "Epoch 315/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.4783 - val_loss: 22.1566\n",
      "Epoch 316/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.6435 - val_loss: 23.7564\n",
      "Epoch 317/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.9946 - val_loss: 25.6124\n",
      "Epoch 318/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.4228 - val_loss: 22.4469\n",
      "Epoch 319/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.6297 - val_loss: 22.0209\n",
      "Epoch 320/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 19.2743 - val_loss: 23.8348\n",
      "Epoch 321/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.7880 - val_loss: 24.3502\n",
      "Epoch 322/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 21.4490 - val_loss: 24.1832\n",
      "Epoch 323/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.3656 - val_loss: 22.7469\n",
      "Epoch 324/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.4271 - val_loss: 24.1289\n",
      "Epoch 325/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.5601 - val_loss: 22.9967\n",
      "Epoch 326/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.3298 - val_loss: 22.2132\n",
      "Epoch 327/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.6181 - val_loss: 23.0822\n",
      "Epoch 328/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 19.9576 - val_loss: 22.9772\n",
      "Epoch 329/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.5500 - val_loss: 24.1028\n",
      "Epoch 330/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.7930 - val_loss: 24.0541\n",
      "Epoch 331/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 18.8934 - val_loss: 23.3792\n",
      "Epoch 332/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.6408 - val_loss: 23.8684\n",
      "Epoch 333/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.6018 - val_loss: 24.8065\n",
      "Epoch 334/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.5176 - val_loss: 22.6022\n",
      "Epoch 335/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 19.7010 - val_loss: 22.9679\n",
      "Epoch 336/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.3252 - val_loss: 23.4020\n",
      "Epoch 337/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.2707 - val_loss: 22.1984\n",
      "Epoch 338/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.1560 - val_loss: 25.4483\n",
      "Epoch 339/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 19.7843 - val_loss: 22.6213\n",
      "Epoch 340/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.9234 - val_loss: 22.5777\n",
      "Epoch 341/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.5642 - val_loss: 22.9445\n",
      "Epoch 342/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.6231 - val_loss: 22.6368\n",
      "Epoch 343/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.7498 - val_loss: 24.5086\n",
      "Epoch 344/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.6036 - val_loss: 22.6521\n",
      "Epoch 345/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.6019 - val_loss: 23.8676\n",
      "Epoch 346/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 19.3304 - val_loss: 23.5686\n",
      "Epoch 347/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.3824 - val_loss: 23.8412\n",
      "Epoch 348/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.4151 - val_loss: 22.7835\n",
      "Epoch 349/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.1598 - val_loss: 22.3466\n",
      "Epoch 350/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.1761 - val_loss: 22.5092\n",
      "Epoch 351/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.4240 - val_loss: 24.5102\n",
      "Epoch 352/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.3661 - val_loss: 22.1731\n",
      "Epoch 353/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 19.5496 - val_loss: 22.2756\n",
      "Epoch 354/2000\n",
      "1500/1500 [==============================] - 0s 34us/sample - loss: 19.5674 - val_loss: 23.3762\n",
      "Epoch 355/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 18.9987 - val_loss: 23.0573\n",
      "Epoch 356/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.1234 - val_loss: 22.3059\n",
      "Epoch 357/2000\n",
      "1500/1500 [==============================] - 0s 34us/sample - loss: 19.1354 - val_loss: 23.1165\n",
      "Epoch 358/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.0951 - val_loss: 23.5423\n",
      "Epoch 359/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.6468 - val_loss: 24.6448\n",
      "Epoch 360/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.5360 - val_loss: 24.3871\n",
      "Epoch 361/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.6824 - val_loss: 22.7803\n",
      "Epoch 362/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.6877 - val_loss: 22.7484\n",
      "Epoch 363/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.6612 - val_loss: 23.8513\n",
      "Epoch 364/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 19.9658 - val_loss: 22.9228\n",
      "Epoch 365/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.5425 - val_loss: 23.1703\n",
      "Epoch 366/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.6314 - val_loss: 24.0750\n",
      "Epoch 367/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 20.0531 - val_loss: 23.2382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 368/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.3521 - val_loss: 24.3127\n",
      "Epoch 369/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 19.1135 - val_loss: 23.7089\n",
      "Epoch 370/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 19.1400 - val_loss: 24.0147\n",
      "Epoch 371/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 19.3873 - val_loss: 22.2134\n",
      "Epoch 372/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.0695 - val_loss: 22.4890\n",
      "Epoch 373/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 19.6919 - val_loss: 24.5013\n",
      "Epoch 374/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.7804 - val_loss: 22.8015\n",
      "Epoch 375/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.3034 - val_loss: 21.8482\n",
      "Epoch 376/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.0849 - val_loss: 24.8147\n",
      "Epoch 377/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 19.3633 - val_loss: 23.7237\n",
      "Epoch 378/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.0625 - val_loss: 22.3131\n",
      "Epoch 379/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.3897 - val_loss: 23.6103\n",
      "Epoch 380/2000\n",
      "1500/1500 [==============================] - 0s 34us/sample - loss: 19.4726 - val_loss: 22.3529\n",
      "Epoch 381/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.7917 - val_loss: 23.8944\n",
      "Epoch 382/2000\n",
      "1500/1500 [==============================] - 0s 34us/sample - loss: 18.9735 - val_loss: 22.9388\n",
      "Epoch 383/2000\n",
      "1500/1500 [==============================] - 0s 34us/sample - loss: 19.3852 - val_loss: 22.3934\n",
      "Epoch 384/2000\n",
      "1500/1500 [==============================] - 0s 34us/sample - loss: 19.5147 - val_loss: 23.2381\n",
      "Epoch 385/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.0404 - val_loss: 23.1836\n",
      "Epoch 386/2000\n",
      "1500/1500 [==============================] - 0s 34us/sample - loss: 19.1292 - val_loss: 22.2813\n",
      "Epoch 387/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 19.4990 - val_loss: 22.8597\n",
      "Epoch 388/2000\n",
      "1500/1500 [==============================] - 0s 34us/sample - loss: 19.1696 - val_loss: 22.8932\n",
      "Epoch 389/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.7220 - val_loss: 23.6613\n",
      "Epoch 390/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.0147 - val_loss: 22.8787\n",
      "Epoch 391/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.4767 - val_loss: 22.2371\n",
      "Epoch 392/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.3928 - val_loss: 23.8912\n",
      "Epoch 393/2000\n",
      "1500/1500 [==============================] - 0s 34us/sample - loss: 19.0468 - val_loss: 22.5928\n",
      "Epoch 394/2000\n",
      "1500/1500 [==============================] - 0s 34us/sample - loss: 18.8250 - val_loss: 23.6511\n",
      "Epoch 395/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 18.6960 - val_loss: 22.7897\n",
      "Epoch 396/2000\n",
      "1500/1500 [==============================] - 0s 33us/sample - loss: 19.0211 - val_loss: 24.4479\n",
      "Epoch 397/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.0178 - val_loss: 23.0180\n",
      "Epoch 398/2000\n",
      "1500/1500 [==============================] - 0s 33us/sample - loss: 18.6014 - val_loss: 23.9379\n",
      "Epoch 399/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.3475 - val_loss: 22.7766\n",
      "Epoch 400/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.8373 - val_loss: 24.0663\n",
      "Epoch 401/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.1266 - val_loss: 22.9423\n",
      "Epoch 402/2000\n",
      "1500/1500 [==============================] - 0s 33us/sample - loss: 19.4178 - val_loss: 25.4611\n",
      "Epoch 403/2000\n",
      "1500/1500 [==============================] - 0s 33us/sample - loss: 19.0400 - val_loss: 24.3995\n",
      "Epoch 404/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.0441 - val_loss: 25.5000\n",
      "Epoch 405/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.3156 - val_loss: 24.3882\n",
      "Epoch 406/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.0103 - val_loss: 22.8751\n",
      "Epoch 407/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.2322 - val_loss: 21.9603\n",
      "Epoch 408/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.3663 - val_loss: 22.4290\n",
      "Epoch 409/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 19.1266 - val_loss: 23.5844\n",
      "Epoch 410/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 19.0171 - val_loss: 23.1711\n",
      "Epoch 411/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.3986 - val_loss: 23.2561\n",
      "Epoch 412/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 18.5593 - val_loss: 23.3891\n",
      "Epoch 413/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.5216 - val_loss: 22.4560\n",
      "Epoch 414/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.0106 - val_loss: 22.4653\n",
      "Epoch 415/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.8405 - val_loss: 22.8073\n",
      "Epoch 416/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 18.8454 - val_loss: 24.4652\n",
      "Epoch 417/2000\n",
      "1500/1500 [==============================] - 0s 34us/sample - loss: 19.2189 - val_loss: 23.1637\n",
      "Epoch 418/2000\n",
      "1500/1500 [==============================] - 0s 34us/sample - loss: 19.0942 - val_loss: 22.1081\n",
      "Epoch 419/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.4821 - val_loss: 23.9819\n",
      "Epoch 420/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.2698 - val_loss: 22.8805\n",
      "Epoch 421/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.1509 - val_loss: 23.3589\n",
      "Epoch 422/2000\n",
      "1500/1500 [==============================] - 0s 34us/sample - loss: 19.3706 - val_loss: 23.1995\n",
      "Epoch 423/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 18.8473 - val_loss: 22.7895\n",
      "Epoch 424/2000\n",
      "1500/1500 [==============================] - 0s 34us/sample - loss: 18.8721 - val_loss: 24.1698\n",
      "Epoch 425/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.0393 - val_loss: 23.7790\n",
      "Epoch 426/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.4429 - val_loss: 23.6098\n",
      "Epoch 427/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.0347 - val_loss: 22.4278\n",
      "Epoch 428/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.0152 - val_loss: 23.8817\n",
      "Epoch 429/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 18.9412 - val_loss: 23.9722\n",
      "Epoch 430/2000\n",
      "1500/1500 [==============================] - 0s 33us/sample - loss: 19.1321 - val_loss: 23.0468\n",
      "Epoch 431/2000\n",
      "1500/1500 [==============================] - 0s 33us/sample - loss: 19.4149 - val_loss: 23.1478\n",
      "Epoch 432/2000\n",
      "1500/1500 [==============================] - 0s 33us/sample - loss: 19.5387 - val_loss: 23.5481\n",
      "Epoch 433/2000\n",
      "1500/1500 [==============================] - 0s 33us/sample - loss: 19.4518 - val_loss: 23.6645\n",
      "Epoch 434/2000\n",
      "1500/1500 [==============================] - 0s 34us/sample - loss: 19.2465 - val_loss: 22.4670\n",
      "Epoch 435/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 18.7759 - val_loss: 23.0865\n",
      "Epoch 436/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 19.2081 - val_loss: 23.5435\n",
      "Epoch 437/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 18.8948 - val_loss: 22.5016\n",
      "Epoch 438/2000\n",
      "1500/1500 [==============================] - 0s 34us/sample - loss: 19.4608 - val_loss: 26.1370\n",
      "Epoch 439/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 18.9594 - val_loss: 23.3139\n",
      "Epoch 440/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.0151 - val_loss: 22.9849\n",
      "Epoch 441/2000\n",
      "1500/1500 [==============================] - 0s 33us/sample - loss: 18.7889 - val_loss: 23.7785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 442/2000\n",
      "1500/1500 [==============================] - 0s 33us/sample - loss: 18.8611 - val_loss: 23.9027\n",
      "Epoch 443/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 18.9277 - val_loss: 23.4122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|█████▏                                                                             | 1/16 [00:27<06:58, 27.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'batc_normalization': False, 'batch_size': 32, 'dropout': 0, 'epochs': 2000, 'first_neuron': 160, 'hidden_layers': 1, 'hidden_neuron': 100, 'kernel_initializer': 'normal', 'last_activation': 'linear', 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "Train on 1500 samples, validate on 375 samples\n",
      "Epoch 1/2000\n",
      "1500/1500 [==============================] - 0s 246us/sample - loss: 2182.4721 - val_loss: 2173.1203\n",
      "Epoch 2/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 2126.0195 - val_loss: 2043.2089\n",
      "Epoch 3/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 1849.9483 - val_loss: 1571.8320\n",
      "Epoch 4/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 1145.4059 - val_loss: 738.4808\n",
      "Epoch 5/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 620.5928 - val_loss: 535.4878\n",
      "Epoch 6/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 513.4521 - val_loss: 454.5717\n",
      "Epoch 7/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 429.9445 - val_loss: 378.2265\n",
      "Epoch 8/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 348.9007 - val_loss: 295.9571\n",
      "Epoch 9/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 268.2566 - val_loss: 218.3400\n",
      "Epoch 10/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 194.3421 - val_loss: 161.3073\n",
      "Epoch 11/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 141.5366 - val_loss: 120.7525\n",
      "Epoch 12/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 107.4472 - val_loss: 93.3282\n",
      "Epoch 13/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 82.4015 - val_loss: 74.0806\n",
      "Epoch 14/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 65.1676 - val_loss: 57.8580\n",
      "Epoch 15/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 52.9574 - val_loss: 48.8261\n",
      "Epoch 16/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 43.8563 - val_loss: 41.9507\n",
      "Epoch 17/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 38.7304 - val_loss: 38.8961\n",
      "Epoch 18/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 35.2858 - val_loss: 36.6157\n",
      "Epoch 19/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 33.0510 - val_loss: 33.9555\n",
      "Epoch 20/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 30.5335 - val_loss: 30.9742\n",
      "Epoch 21/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 29.1003 - val_loss: 30.3564\n",
      "Epoch 22/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 28.1434 - val_loss: 29.5502\n",
      "Epoch 23/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 27.5819 - val_loss: 29.5950\n",
      "Epoch 24/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 27.1798 - val_loss: 26.4804\n",
      "Epoch 25/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 26.3268 - val_loss: 27.5172\n",
      "Epoch 26/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 25.3803 - val_loss: 26.8936\n",
      "Epoch 27/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 24.8024 - val_loss: 25.8930\n",
      "Epoch 28/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 23.7926 - val_loss: 23.5781\n",
      "Epoch 29/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 24.5278 - val_loss: 25.2671\n",
      "Epoch 30/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 23.4155 - val_loss: 24.7023\n",
      "Epoch 31/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 24.2226 - val_loss: 24.2616\n",
      "Epoch 32/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 23.1183 - val_loss: 24.5273\n",
      "Epoch 33/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 23.4975 - val_loss: 23.8400\n",
      "Epoch 34/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 23.0052 - val_loss: 23.6065\n",
      "Epoch 35/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 23.2874 - val_loss: 25.5437\n",
      "Epoch 36/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 23.5132 - val_loss: 24.6895\n",
      "Epoch 37/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 23.3142 - val_loss: 24.2154\n",
      "Epoch 38/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 22.9525 - val_loss: 23.7884\n",
      "Epoch 39/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 24.1821 - val_loss: 26.2681\n",
      "Epoch 40/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 24.0149 - val_loss: 24.8241\n",
      "Epoch 41/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 22.7155 - val_loss: 23.5506\n",
      "Epoch 42/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 22.2885 - val_loss: 23.6156\n",
      "Epoch 43/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 22.8297 - val_loss: 25.3819\n",
      "Epoch 44/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 23.0387 - val_loss: 26.2029\n",
      "Epoch 45/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 22.9784 - val_loss: 24.5041\n",
      "Epoch 46/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 22.2559 - val_loss: 25.1732\n",
      "Epoch 47/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 22.5716 - val_loss: 25.7577\n",
      "Epoch 48/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 23.2445 - val_loss: 25.4837\n",
      "Epoch 49/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 22.1688 - val_loss: 23.9347\n",
      "Epoch 50/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 21.9107 - val_loss: 23.4712\n",
      "Epoch 51/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 22.2675 - val_loss: 25.0288\n",
      "Epoch 52/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.5866 - val_loss: 23.5330\n",
      "Epoch 53/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 22.2269 - val_loss: 25.2586\n",
      "Epoch 54/2000\n",
      "1500/1500 [==============================] - 0s 61us/sample - loss: 22.3379 - val_loss: 25.4373\n",
      "Epoch 55/2000\n",
      "1500/1500 [==============================] - 0s 51us/sample - loss: 22.5905 - val_loss: 23.6805\n",
      "Epoch 56/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 22.4814 - val_loss: 26.5308\n",
      "Epoch 57/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.9081 - val_loss: 23.5566\n",
      "Epoch 58/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 22.2857 - val_loss: 25.4222\n",
      "Epoch 59/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 22.1582 - val_loss: 23.3745\n",
      "Epoch 60/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 22.5396 - val_loss: 24.7819\n",
      "Epoch 61/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.6921 - val_loss: 23.3834\n",
      "Epoch 62/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 22.0413 - val_loss: 24.7833\n",
      "Epoch 63/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 22.7320 - val_loss: 25.0355\n",
      "Epoch 64/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.5485 - val_loss: 23.8324\n",
      "Epoch 65/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 22.2895 - val_loss: 22.4146\n",
      "Epoch 66/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 22.1667 - val_loss: 24.5908\n",
      "Epoch 67/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.2000 - val_loss: 23.3056\n",
      "Epoch 68/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 21.8554 - val_loss: 23.9916\n",
      "Epoch 69/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 23.7631 - val_loss: 26.7280\n",
      "Epoch 70/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.7308 - val_loss: 23.4402\n",
      "Epoch 71/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 22.5777 - val_loss: 22.7874\n",
      "Epoch 72/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 22.6265 - val_loss: 23.7004\n",
      "Epoch 73/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 22.0034 - val_loss: 25.9184\n",
      "Epoch 74/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.9786 - val_loss: 24.7860\n",
      "Epoch 75/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 21.3959 - val_loss: 25.3198\n",
      "Epoch 76/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.6878 - val_loss: 25.4763\n",
      "Epoch 77/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.5559 - val_loss: 25.9241\n",
      "Epoch 78/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 21.9874 - val_loss: 24.9894\n",
      "Epoch 79/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 22.0164 - val_loss: 26.2326\n",
      "Epoch 80/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 22.3593 - val_loss: 23.8872\n",
      "Epoch 81/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 22.2644 - val_loss: 25.2273\n",
      "Epoch 82/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.6044 - val_loss: 24.5794\n",
      "Epoch 83/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.2069 - val_loss: 25.0896\n",
      "Epoch 84/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.7465 - val_loss: 24.1425\n",
      "Epoch 85/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 22.3783 - val_loss: 24.3658\n",
      "Epoch 86/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 22.0100 - val_loss: 25.1124\n",
      "Epoch 87/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 22.0726 - val_loss: 24.6106\n",
      "Epoch 88/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.0831 - val_loss: 23.2443\n",
      "Epoch 89/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.1798 - val_loss: 23.0785\n",
      "Epoch 90/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 21.4222 - val_loss: 24.8671\n",
      "Epoch 91/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 22.0372 - val_loss: 24.8524\n",
      "Epoch 92/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 22.0465 - val_loss: 23.1876\n",
      "Epoch 93/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 23.0093 - val_loss: 23.7767\n",
      "Epoch 94/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.0501 - val_loss: 23.9125\n",
      "Epoch 95/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.1378 - val_loss: 22.8769\n",
      "Epoch 96/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.5628 - val_loss: 23.1388\n",
      "Epoch 97/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.3653 - val_loss: 23.9858\n",
      "Epoch 98/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.1127 - val_loss: 23.7198\n",
      "Epoch 99/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.3270 - val_loss: 22.5556\n",
      "Epoch 100/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.8261 - val_loss: 24.4107\n",
      "Epoch 101/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.2475 - val_loss: 25.0476\n",
      "Epoch 102/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 21.5549 - val_loss: 22.6767\n",
      "Epoch 103/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.0558 - val_loss: 22.8055\n",
      "Epoch 104/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.3880 - val_loss: 22.5606\n",
      "Epoch 105/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 21.2556 - val_loss: 25.0268\n",
      "Epoch 106/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 21.2242 - val_loss: 24.6374\n",
      "Epoch 107/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.9296 - val_loss: 23.7232\n",
      "Epoch 108/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 20.8533 - val_loss: 24.8271\n",
      "Epoch 109/2000\n",
      "1500/1500 [==============================] - 0s 68us/sample - loss: 20.9212 - val_loss: 25.9851\n",
      "Epoch 110/2000\n",
      "1500/1500 [==============================] - 0s 66us/sample - loss: 22.4645 - val_loss: 23.6912\n",
      "Epoch 111/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.5975 - val_loss: 23.9860\n",
      "Epoch 112/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.4186 - val_loss: 28.1709\n",
      "Epoch 113/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 21.7566 - val_loss: 24.5143\n",
      "Epoch 114/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 20.6317 - val_loss: 24.2898\n",
      "Epoch 115/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.2761 - val_loss: 22.5625\n",
      "Epoch 116/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 21.8562 - val_loss: 23.8597\n",
      "Epoch 117/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 21.0267 - val_loss: 24.3083\n",
      "Epoch 118/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 22.0412 - val_loss: 24.8533\n",
      "Epoch 119/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.5023 - val_loss: 23.4320\n",
      "Epoch 120/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 21.0447 - val_loss: 25.6435\n",
      "Epoch 121/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 20.5212 - val_loss: 24.7909\n",
      "Epoch 122/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 21.0604 - val_loss: 23.8102\n",
      "Epoch 123/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 21.7115 - val_loss: 23.1670\n",
      "Epoch 124/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 21.4505 - val_loss: 27.7115\n",
      "Epoch 125/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.8779 - val_loss: 23.0816\n",
      "Epoch 126/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 20.8762 - val_loss: 23.1950\n",
      "Epoch 127/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 21.6712 - val_loss: 22.1503\n",
      "Epoch 128/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.5930 - val_loss: 23.4868\n",
      "Epoch 129/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.3054 - val_loss: 23.4505\n",
      "Epoch 130/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 21.8392 - val_loss: 22.6769\n",
      "Epoch 131/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 21.8260 - val_loss: 27.3889\n",
      "Epoch 132/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 21.1093 - val_loss: 23.1610\n",
      "Epoch 133/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 21.4612 - val_loss: 25.0259\n",
      "Epoch 134/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 20.6365 - val_loss: 24.7967\n",
      "Epoch 135/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 20.7926 - val_loss: 23.3650\n",
      "Epoch 136/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.5683 - val_loss: 23.0452\n",
      "Epoch 137/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 20.5899 - val_loss: 23.8924\n",
      "Epoch 138/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 20.6987 - val_loss: 22.8744\n",
      "Epoch 139/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 21.2991 - val_loss: 27.4986\n",
      "Epoch 140/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 21.1667 - val_loss: 23.1922\n",
      "Epoch 141/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 21.1100 - val_loss: 24.1006\n",
      "Epoch 142/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 21.5533 - val_loss: 23.1538\n",
      "Epoch 143/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.9743 - val_loss: 23.8265\n",
      "Epoch 144/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.1150 - val_loss: 23.2482\n",
      "Epoch 145/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.4702 - val_loss: 23.1410\n",
      "Epoch 146/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.4795 - val_loss: 23.0477\n",
      "Epoch 147/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.3537 - val_loss: 22.3688\n",
      "Epoch 148/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.2122 - val_loss: 23.2224\n",
      "Epoch 149/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 21.1437 - val_loss: 22.7086\n",
      "Epoch 150/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 21.1090 - val_loss: 23.4026\n",
      "Epoch 151/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 20.7343 - val_loss: 25.3248\n",
      "Epoch 152/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 20.9546 - val_loss: 24.7157\n",
      "Epoch 153/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.3797 - val_loss: 23.6211\n",
      "Epoch 154/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.7021 - val_loss: 22.2430\n",
      "Epoch 155/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.4238 - val_loss: 23.2347\n",
      "Epoch 156/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.8329 - val_loss: 23.3373\n",
      "Epoch 157/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.8647 - val_loss: 23.2523\n",
      "Epoch 158/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.4868 - val_loss: 23.8134\n",
      "Epoch 159/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.6550 - val_loss: 23.5544\n",
      "Epoch 160/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.6437 - val_loss: 26.4290\n",
      "Epoch 161/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.6083 - val_loss: 24.7914\n",
      "Epoch 162/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.7411 - val_loss: 23.3168\n",
      "Epoch 163/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.1439 - val_loss: 23.0762\n",
      "Epoch 164/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.7401 - val_loss: 24.3090\n",
      "Epoch 165/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.3707 - val_loss: 24.4828\n",
      "Epoch 166/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.3430 - val_loss: 24.2148\n",
      "Epoch 167/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.0209 - val_loss: 22.6675\n",
      "Epoch 168/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.6487 - val_loss: 24.9532\n",
      "Epoch 169/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.4787 - val_loss: 23.3284\n",
      "Epoch 170/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 20.1957 - val_loss: 22.1922\n",
      "Epoch 171/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 20.8234 - val_loss: 24.8518\n",
      "Epoch 172/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 20.8626 - val_loss: 22.8638\n",
      "Epoch 173/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.4253 - val_loss: 24.4494\n",
      "Epoch 174/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 20.5530 - val_loss: 22.6361\n",
      "Epoch 175/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.6435 - val_loss: 26.2308\n",
      "Epoch 176/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.6831 - val_loss: 21.9538\n",
      "Epoch 177/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.7709 - val_loss: 22.1054\n",
      "Epoch 178/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.9683 - val_loss: 23.2071\n",
      "Epoch 179/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 20.3548 - val_loss: 23.8070\n",
      "Epoch 180/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 19.7047 - val_loss: 22.6153\n",
      "Epoch 181/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.1403 - val_loss: 23.2887\n",
      "Epoch 182/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.1870 - val_loss: 22.3246\n",
      "Epoch 183/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.3096 - val_loss: 22.8816\n",
      "Epoch 184/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.6313 - val_loss: 24.4299\n",
      "Epoch 185/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 19.8927 - val_loss: 22.8914\n",
      "Epoch 186/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 19.9201 - val_loss: 23.9399\n",
      "Epoch 187/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 21.2710 - val_loss: 23.2808\n",
      "Epoch 188/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.1788 - val_loss: 25.0773\n",
      "Epoch 189/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 20.7102 - val_loss: 23.8075\n",
      "Epoch 190/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 20.1095 - val_loss: 22.8340\n",
      "Epoch 191/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 19.7488 - val_loss: 24.0470\n",
      "Epoch 192/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.5352 - val_loss: 24.4631\n",
      "Epoch 193/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.5533 - val_loss: 22.4993\n",
      "Epoch 194/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.3239 - val_loss: 23.2278\n",
      "Epoch 195/2000\n",
      "1500/1500 [==============================] - 0s 51us/sample - loss: 20.3113 - val_loss: 23.1069\n",
      "Epoch 196/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.6387 - val_loss: 23.6407\n",
      "Epoch 197/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 19.8898 - val_loss: 22.9785\n",
      "Epoch 198/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.0359 - val_loss: 22.3999\n",
      "Epoch 199/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.5550 - val_loss: 22.5576\n",
      "Epoch 200/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 20.9946 - val_loss: 22.3399\n",
      "Epoch 201/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 20.4690 - val_loss: 25.2712\n",
      "Epoch 202/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 20.2203 - val_loss: 23.6571\n",
      "Epoch 203/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.8588 - val_loss: 22.9405\n",
      "Epoch 204/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 20.6730 - val_loss: 25.4696\n",
      "Epoch 205/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 21.2443 - val_loss: 23.4850\n",
      "Epoch 206/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 19.7750 - val_loss: 25.0310\n",
      "Epoch 207/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.4715 - val_loss: 27.4913\n",
      "Epoch 208/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.1653 - val_loss: 24.5820\n",
      "Epoch 209/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.4037 - val_loss: 23.3126\n",
      "Epoch 210/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.6343 - val_loss: 24.2616\n",
      "Epoch 211/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.9067 - val_loss: 23.1024\n",
      "Epoch 212/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.7939 - val_loss: 23.1265\n",
      "Epoch 213/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.0633 - val_loss: 24.9684\n",
      "Epoch 214/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.2359 - val_loss: 23.1964\n",
      "Epoch 215/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.7791 - val_loss: 25.1629\n",
      "Epoch 216/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.1898 - val_loss: 23.3121\n",
      "Epoch 217/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 20.6520 - val_loss: 23.9853\n",
      "Epoch 218/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 20.3860 - val_loss: 27.5565\n",
      "Epoch 219/2000\n",
      "1500/1500 [==============================] - 0s 52us/sample - loss: 20.0467 - val_loss: 23.0679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 220/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 20.3230 - val_loss: 23.9217\n",
      "Epoch 221/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.7975 - val_loss: 22.5351\n",
      "Epoch 222/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 20.8600 - val_loss: 25.6370\n",
      "Epoch 223/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 20.3684 - val_loss: 24.6174\n",
      "Epoch 224/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 20.5250 - val_loss: 23.0807\n",
      "Epoch 225/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 19.6772 - val_loss: 21.9825\n",
      "Epoch 226/2000\n",
      "1500/1500 [==============================] - 0s 50us/sample - loss: 20.4384 - val_loss: 23.2590\n",
      "Epoch 227/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 19.8776 - val_loss: 22.7623\n",
      "Epoch 228/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 19.3731 - val_loss: 23.5882\n",
      "Epoch 229/2000\n",
      "1500/1500 [==============================] - 0s 53us/sample - loss: 19.8636 - val_loss: 23.7952\n",
      "Epoch 230/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 20.3315 - val_loss: 24.7466\n",
      "Epoch 231/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 20.3626 - val_loss: 23.3062\n",
      "Epoch 232/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 19.7402 - val_loss: 23.0952\n",
      "Epoch 233/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 19.9001 - val_loss: 22.7987\n",
      "Epoch 234/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 20.3112 - val_loss: 23.4130\n",
      "Epoch 235/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 19.4754 - val_loss: 22.9373\n",
      "Epoch 236/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 22.3456 - val_loss: 22.8408\n",
      "Epoch 237/2000\n",
      "1500/1500 [==============================] - 0s 51us/sample - loss: 19.9852 - val_loss: 22.8396\n",
      "Epoch 238/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 19.2583 - val_loss: 23.0297\n",
      "Epoch 239/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 19.6725 - val_loss: 23.1864\n",
      "Epoch 240/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 20.1542 - val_loss: 22.7742\n",
      "Epoch 241/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 20.0622 - val_loss: 22.3729\n",
      "Epoch 242/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 20.3232 - val_loss: 23.7031\n",
      "Epoch 243/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 19.6990 - val_loss: 23.5310\n",
      "Epoch 244/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 19.3456 - val_loss: 24.4205\n",
      "Epoch 245/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 19.9145 - val_loss: 22.2936\n",
      "Epoch 246/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 19.8193 - val_loss: 23.9626\n",
      "Epoch 247/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.2245 - val_loss: 23.6298\n",
      "Epoch 248/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 19.5766 - val_loss: 24.0350\n",
      "Epoch 249/2000\n",
      "1500/1500 [==============================] - 0s 55us/sample - loss: 20.2254 - val_loss: 22.0930\n",
      "Epoch 250/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 20.3350 - val_loss: 24.2498\n",
      "Epoch 251/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.5870 - val_loss: 23.1791\n",
      "Epoch 252/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.7868 - val_loss: 22.4737\n",
      "Epoch 253/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.3128 - val_loss: 24.3828\n",
      "Epoch 254/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.1940 - val_loss: 22.6154\n",
      "Epoch 255/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.1635 - val_loss: 24.4581\n",
      "Epoch 256/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 19.5780 - val_loss: 24.2990\n",
      "Epoch 257/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 19.5865 - val_loss: 23.7672\n",
      "Epoch 258/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.5405 - val_loss: 22.8723\n",
      "Epoch 259/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.5672 - val_loss: 23.9605\n",
      "Epoch 260/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.4065 - val_loss: 22.7363\n",
      "Epoch 261/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.7322 - val_loss: 23.7050\n",
      "Epoch 262/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.3975 - val_loss: 24.1053\n",
      "Epoch 263/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.6613 - val_loss: 25.0967\n",
      "Epoch 264/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.9657 - val_loss: 25.6686\n",
      "Epoch 265/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 19.8678 - val_loss: 22.9610\n",
      "Epoch 266/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.6220 - val_loss: 23.8959\n",
      "Epoch 267/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.9789 - val_loss: 24.4655\n",
      "Epoch 268/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 20.5667 - val_loss: 24.0184\n",
      "Epoch 269/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 19.8174 - val_loss: 22.7496\n",
      "Epoch 270/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.3781 - val_loss: 22.6799\n",
      "Epoch 271/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 19.5976 - val_loss: 23.3948\n",
      "Epoch 272/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 20.0310 - val_loss: 24.3087\n",
      "Epoch 273/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.9693 - val_loss: 23.5097\n",
      "Epoch 274/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 19.5391 - val_loss: 24.5123\n",
      "Epoch 275/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 19.3315 - val_loss: 22.3225\n",
      "Epoch 276/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 19.9675 - val_loss: 23.3492\n",
      "Epoch 277/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.4482 - val_loss: 23.6313\n",
      "Epoch 278/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.5762 - val_loss: 22.7603\n",
      "Epoch 279/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.3361 - val_loss: 22.6922\n",
      "Epoch 280/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 20.8911 - val_loss: 25.3087\n",
      "Epoch 281/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 19.5948 - val_loss: 22.2231\n",
      "Epoch 282/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 19.2885 - val_loss: 22.6269\n",
      "Epoch 283/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.1201 - val_loss: 22.7319\n",
      "Epoch 284/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.5187 - val_loss: 25.7648\n",
      "Epoch 285/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 19.7295 - val_loss: 23.8914\n",
      "Epoch 286/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.6083 - val_loss: 25.0324\n",
      "Epoch 287/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.1917 - val_loss: 22.6981\n",
      "Epoch 288/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.4133 - val_loss: 23.8427\n",
      "Epoch 289/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.4957 - val_loss: 23.0253\n",
      "Epoch 290/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.2862 - val_loss: 23.0683\n",
      "Epoch 291/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 19.7074 - val_loss: 23.6792\n",
      "Epoch 292/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 20.0717 - val_loss: 25.5927\n",
      "Epoch 293/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 19.6510 - val_loss: 23.5239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 294/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 20.7656 - val_loss: 23.5127\n",
      "Epoch 295/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 19.2476 - val_loss: 22.9888\n",
      "Epoch 296/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.3935 - val_loss: 23.5600\n",
      "Epoch 297/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.5851 - val_loss: 22.7081\n",
      "Epoch 298/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.5743 - val_loss: 23.9764\n",
      "Epoch 299/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.0709 - val_loss: 22.4250\n",
      "Epoch 300/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.3168 - val_loss: 23.5250\n",
      "Epoch 301/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.6168 - val_loss: 24.2230\n",
      "Epoch 302/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.7117 - val_loss: 22.4455\n",
      "Epoch 303/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 20.4878 - val_loss: 26.2539\n",
      "Epoch 304/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.8859 - val_loss: 22.7023\n",
      "Epoch 305/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.6492 - val_loss: 23.6805\n",
      "Epoch 306/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.4725 - val_loss: 23.3799\n",
      "Epoch 307/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.6272 - val_loss: 22.1021\n",
      "Epoch 308/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.6530 - val_loss: 22.0773\n",
      "Epoch 309/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 20.0261 - val_loss: 25.0334\n",
      "Epoch 310/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 19.6501 - val_loss: 22.8126\n",
      "Epoch 311/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.9212 - val_loss: 23.0439\n",
      "Epoch 312/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.3858 - val_loss: 23.7784\n",
      "Epoch 313/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 19.2772 - val_loss: 22.0005\n",
      "Epoch 314/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 19.5023 - val_loss: 23.7589\n",
      "Epoch 315/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.0181 - val_loss: 23.7833\n",
      "Epoch 316/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 19.2044 - val_loss: 22.3512\n",
      "Epoch 317/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.8183 - val_loss: 22.8210\n",
      "Epoch 318/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.5446 - val_loss: 23.6112\n",
      "Epoch 319/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.3595 - val_loss: 22.6521\n",
      "Epoch 320/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.6679 - val_loss: 24.2440\n",
      "Epoch 321/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.5374 - val_loss: 26.9251\n",
      "Epoch 322/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.5312 - val_loss: 24.3477\n",
      "Epoch 323/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.3475 - val_loss: 22.6248\n",
      "Epoch 324/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 19.0971 - val_loss: 23.2903\n",
      "Epoch 325/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 19.2708 - val_loss: 22.5951\n",
      "Epoch 326/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 19.4384 - val_loss: 25.3038\n",
      "Epoch 327/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 19.4328 - val_loss: 22.1773\n",
      "Epoch 328/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.4594 - val_loss: 23.3670\n",
      "Epoch 329/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 19.0603 - val_loss: 26.3056\n",
      "Epoch 330/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 19.5291 - val_loss: 24.2270\n",
      "Epoch 331/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.1427 - val_loss: 22.2562\n",
      "Epoch 332/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.5656 - val_loss: 22.6080\n",
      "Epoch 333/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.5588 - val_loss: 23.8805\n",
      "Epoch 334/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.8020 - val_loss: 23.1166\n",
      "Epoch 335/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 19.8342 - val_loss: 23.6696\n",
      "Epoch 336/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.8674 - val_loss: 23.3872\n",
      "Epoch 337/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.9700 - val_loss: 26.9361\n",
      "Epoch 338/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.3286 - val_loss: 22.3471\n",
      "Epoch 339/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.6437 - val_loss: 22.9013\n",
      "Epoch 340/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.7935 - val_loss: 23.9690\n",
      "Epoch 341/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.2274 - val_loss: 23.3294\n",
      "Epoch 342/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.9394 - val_loss: 25.4993\n",
      "Epoch 343/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.7769 - val_loss: 23.0775\n",
      "Epoch 344/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.8067 - val_loss: 23.2999\n",
      "Epoch 345/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.2215 - val_loss: 23.4339\n",
      "Epoch 346/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.1257 - val_loss: 22.4496\n",
      "Epoch 347/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.6782 - val_loss: 23.9524\n",
      "Epoch 348/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.0936 - val_loss: 22.7929\n",
      "Epoch 349/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.2405 - val_loss: 25.3744\n",
      "Epoch 350/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.4687 - val_loss: 22.9814\n",
      "Epoch 351/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.8430 - val_loss: 24.3896\n",
      "Epoch 352/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 19.3413 - val_loss: 23.7280\n",
      "Epoch 353/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 18.8801 - val_loss: 24.8590\n",
      "Epoch 354/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 19.0860 - val_loss: 22.8552\n",
      "Epoch 355/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.0617 - val_loss: 22.5015\n",
      "Epoch 356/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 18.9265 - val_loss: 22.5973\n",
      "Epoch 357/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 19.3442 - val_loss: 22.0345\n",
      "Epoch 358/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 19.0596 - val_loss: 22.1243\n",
      "Epoch 359/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.0761 - val_loss: 22.4840\n",
      "Epoch 360/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 18.8721 - val_loss: 24.5399\n",
      "Epoch 361/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 18.8206 - val_loss: 23.3542\n",
      "Epoch 362/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 19.7522 - val_loss: 22.1229\n",
      "Epoch 363/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.2088 - val_loss: 22.6397\n",
      "Epoch 364/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.1150 - val_loss: 22.4702\n",
      "Epoch 365/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 18.6757 - val_loss: 23.0449\n",
      "Epoch 366/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.0883 - val_loss: 24.1441\n",
      "Epoch 367/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 18.7271 - val_loss: 22.2504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 368/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.0418 - val_loss: 23.6415\n",
      "Epoch 369/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.2414 - val_loss: 23.0068\n",
      "Epoch 370/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.5229 - val_loss: 22.9525\n",
      "Epoch 371/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.4005 - val_loss: 23.8264\n",
      "Epoch 372/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.5082 - val_loss: 22.7885\n",
      "Epoch 373/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.5683 - val_loss: 24.7405\n",
      "Epoch 374/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 18.9681 - val_loss: 22.4220\n",
      "Epoch 375/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 18.4285 - val_loss: 24.9030\n",
      "Epoch 376/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.9887 - val_loss: 25.1009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|██████████▍                                                                        | 2/16 [00:51<05:53, 25.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'batc_normalization': False, 'batch_size': 32, 'dropout': 0, 'epochs': 2000, 'first_neuron': 160, 'hidden_layers': 2, 'hidden_neuron': 50, 'kernel_initializer': 'normal', 'last_activation': 'linear', 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "Train on 1500 samples, validate on 375 samples\n",
      "Epoch 1/2000\n",
      "1500/1500 [==============================] - 0s 268us/sample - loss: 2183.2588 - val_loss: 2170.9848\n",
      "Epoch 2/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 2038.3265 - val_loss: 1715.7546\n",
      "Epoch 3/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 971.8142 - val_loss: 547.5203\n",
      "Epoch 4/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 511.2818 - val_loss: 402.1773\n",
      "Epoch 5/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 361.5013 - val_loss: 286.8040\n",
      "Epoch 6/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 239.3034 - val_loss: 181.5743\n",
      "Epoch 7/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 156.0840 - val_loss: 125.0650\n",
      "Epoch 8/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 108.5205 - val_loss: 92.7357\n",
      "Epoch 9/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 78.3159 - val_loss: 71.2725\n",
      "Epoch 10/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 59.4216 - val_loss: 58.1461\n",
      "Epoch 11/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 47.6475 - val_loss: 46.6433\n",
      "Epoch 12/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 40.1449 - val_loss: 43.7123\n",
      "Epoch 13/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 36.4512 - val_loss: 36.8173\n",
      "Epoch 14/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 33.7000 - val_loss: 35.8520\n",
      "Epoch 15/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 31.1801 - val_loss: 32.0072\n",
      "Epoch 16/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 28.5175 - val_loss: 31.0239\n",
      "Epoch 17/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 28.4743 - val_loss: 29.9305\n",
      "Epoch 18/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 26.6008 - val_loss: 32.9555\n",
      "Epoch 19/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 27.9554 - val_loss: 29.0023\n",
      "Epoch 20/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 26.3945 - val_loss: 28.1709\n",
      "Epoch 21/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 25.5810 - val_loss: 24.9665\n",
      "Epoch 22/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 25.2200 - val_loss: 26.7103\n",
      "Epoch 23/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 25.1859 - val_loss: 25.3998\n",
      "Epoch 24/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 25.2425 - val_loss: 28.0634\n",
      "Epoch 25/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 24.4828 - val_loss: 27.6479\n",
      "Epoch 26/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 25.0691 - val_loss: 28.4966\n",
      "Epoch 27/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 24.3517 - val_loss: 25.1320\n",
      "Epoch 28/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 23.9783 - val_loss: 25.8038\n",
      "Epoch 29/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 24.5110 - val_loss: 25.6166\n",
      "Epoch 30/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 24.3319 - val_loss: 24.5989\n",
      "Epoch 31/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 23.1200 - val_loss: 26.3349\n",
      "Epoch 32/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 24.3084 - val_loss: 24.4687\n",
      "Epoch 33/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 23.8230 - val_loss: 27.4814\n",
      "Epoch 34/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 23.2293 - val_loss: 24.8762\n",
      "Epoch 35/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 23.3735 - val_loss: 27.1843\n",
      "Epoch 36/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 24.2486 - val_loss: 25.5448\n",
      "Epoch 37/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 22.4554 - val_loss: 24.7204\n",
      "Epoch 38/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 23.2166 - val_loss: 25.3439\n",
      "Epoch 39/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 23.0433 - val_loss: 24.7927\n",
      "Epoch 40/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 23.4714 - val_loss: 27.4976\n",
      "Epoch 41/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 23.4092 - val_loss: 24.5305\n",
      "Epoch 42/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 22.2956 - val_loss: 26.3692\n",
      "Epoch 43/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 23.5576 - val_loss: 26.5013\n",
      "Epoch 44/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 23.2470 - val_loss: 24.7121\n",
      "Epoch 45/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 24.0225 - val_loss: 28.7819\n",
      "Epoch 46/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 22.9962 - val_loss: 25.0592\n",
      "Epoch 47/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 23.9316 - val_loss: 29.2985\n",
      "Epoch 48/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 23.9972 - val_loss: 26.3845\n",
      "Epoch 49/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 22.5401 - val_loss: 25.1786\n",
      "Epoch 50/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 23.3134 - val_loss: 28.1836\n",
      "Epoch 51/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 22.2909 - val_loss: 24.5232\n",
      "Epoch 52/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.2902 - val_loss: 25.8961\n",
      "Epoch 53/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 22.7514 - val_loss: 24.8103\n",
      "Epoch 54/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 23.1446 - val_loss: 23.8756\n",
      "Epoch 55/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 22.5462 - val_loss: 24.8062\n",
      "Epoch 56/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 22.5508 - val_loss: 25.8460\n",
      "Epoch 57/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 23.8406 - val_loss: 23.6314\n",
      "Epoch 58/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 22.5118 - val_loss: 26.8762\n",
      "Epoch 59/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 23.5499 - val_loss: 27.7882\n",
      "Epoch 60/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 24.0313 - val_loss: 25.4355\n",
      "Epoch 61/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 23.7612 - val_loss: 28.0723\n",
      "Epoch 62/2000\n",
      "1500/1500 [==============================] - 0s 51us/sample - loss: 24.3053 - val_loss: 26.0909\n",
      "Epoch 63/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 22.2162 - val_loss: 23.2490\n",
      "Epoch 64/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 22.6808 - val_loss: 23.2966\n",
      "Epoch 65/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 22.5914 - val_loss: 26.5824\n",
      "Epoch 66/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 22.8413 - val_loss: 24.7344\n",
      "Epoch 67/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 21.8983 - val_loss: 24.3271\n",
      "Epoch 68/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 23.3891 - val_loss: 23.2874\n",
      "Epoch 69/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 22.9572 - val_loss: 24.0471\n",
      "Epoch 70/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 22.8206 - val_loss: 26.3996\n",
      "Epoch 71/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 22.9142 - val_loss: 23.8221\n",
      "Epoch 72/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 22.3759 - val_loss: 25.7543\n",
      "Epoch 73/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 22.0733 - val_loss: 25.9847\n",
      "Epoch 74/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 22.5311 - val_loss: 23.6066\n",
      "Epoch 75/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 22.3700 - val_loss: 23.4979\n",
      "Epoch 76/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 23.0381 - val_loss: 26.8426\n",
      "Epoch 77/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 22.1301 - val_loss: 25.1424\n",
      "Epoch 78/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 22.3997 - val_loss: 23.9724\n",
      "Epoch 79/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 22.6507 - val_loss: 31.1371\n",
      "Epoch 80/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 22.3750 - val_loss: 25.3767\n",
      "Epoch 81/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 22.9639 - val_loss: 23.2010\n",
      "Epoch 82/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 22.4412 - val_loss: 23.8431\n",
      "Epoch 83/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 23.2439 - val_loss: 26.0257\n",
      "Epoch 84/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.6304 - val_loss: 24.7110\n",
      "Epoch 85/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 23.6492 - val_loss: 23.6656\n",
      "Epoch 86/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 24.0368 - val_loss: 23.5336\n",
      "Epoch 87/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.4454 - val_loss: 23.3434\n",
      "Epoch 88/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.6779 - val_loss: 24.8360\n",
      "Epoch 89/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 21.9550 - val_loss: 23.7143\n",
      "Epoch 90/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 21.1309 - val_loss: 23.1476\n",
      "Epoch 91/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 22.0243 - val_loss: 24.2108\n",
      "Epoch 92/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 22.2364 - val_loss: 24.9005\n",
      "Epoch 93/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 22.5202 - val_loss: 29.9553\n",
      "Epoch 94/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 22.1926 - val_loss: 22.8841\n",
      "Epoch 95/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.6220 - val_loss: 26.0215\n",
      "Epoch 96/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 22.7980 - val_loss: 23.9064\n",
      "Epoch 97/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 23.0828 - val_loss: 27.7488\n",
      "Epoch 98/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 23.4576 - val_loss: 25.3731\n",
      "Epoch 99/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 22.2919 - val_loss: 24.4015\n",
      "Epoch 100/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 22.3663 - val_loss: 23.2776\n",
      "Epoch 101/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 22.5565 - val_loss: 23.1016\n",
      "Epoch 102/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 23.3146 - val_loss: 26.7805\n",
      "Epoch 103/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 22.4032 - val_loss: 23.7571\n",
      "Epoch 104/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.2760 - val_loss: 23.1984\n",
      "Epoch 105/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.9761 - val_loss: 23.7935\n",
      "Epoch 106/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.1288 - val_loss: 28.8312\n",
      "Epoch 107/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.9657 - val_loss: 23.6914\n",
      "Epoch 108/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 20.8397 - val_loss: 26.1053\n",
      "Epoch 109/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.4167 - val_loss: 24.7741\n",
      "Epoch 110/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 21.9526 - val_loss: 22.6050\n",
      "Epoch 111/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.2792 - val_loss: 22.4843\n",
      "Epoch 112/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 22.1202 - val_loss: 26.6655\n",
      "Epoch 113/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 22.3389 - val_loss: 31.3417\n",
      "Epoch 114/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 22.9314 - val_loss: 26.9576\n",
      "Epoch 115/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 21.2810 - val_loss: 25.1639\n",
      "Epoch 116/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 21.3574 - val_loss: 26.9865\n",
      "Epoch 117/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 21.5889 - val_loss: 26.8537\n",
      "Epoch 118/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.9423 - val_loss: 25.5319\n",
      "Epoch 119/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 23.1808 - val_loss: 29.0223\n",
      "Epoch 120/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 22.8653 - val_loss: 26.9008\n",
      "Epoch 121/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 22.9102 - val_loss: 31.1733\n",
      "Epoch 122/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 21.8200 - val_loss: 24.0392\n",
      "Epoch 123/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.4929 - val_loss: 24.9299\n",
      "Epoch 124/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.5758 - val_loss: 23.9171\n",
      "Epoch 125/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 20.8904 - val_loss: 24.7732\n",
      "Epoch 126/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 22.7209 - val_loss: 31.7826\n",
      "Epoch 127/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 23.8854 - val_loss: 24.3909\n",
      "Epoch 128/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 22.5224 - val_loss: 28.0955\n",
      "Epoch 129/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 22.1973 - val_loss: 26.4898\n",
      "Epoch 130/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 20.6236 - val_loss: 23.6142\n",
      "Epoch 131/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 20.8703 - val_loss: 24.8529\n",
      "Epoch 132/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.3371 - val_loss: 26.9527\n",
      "Epoch 133/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 21.5712 - val_loss: 24.3929\n",
      "Epoch 134/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.9749 - val_loss: 22.9898\n",
      "Epoch 135/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.7105 - val_loss: 27.5774\n",
      "Epoch 136/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.6997 - val_loss: 25.3816\n",
      "Epoch 137/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 21.9620 - val_loss: 28.0804\n",
      "Epoch 138/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 22.9778 - val_loss: 24.7065\n",
      "Epoch 139/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 21.3638 - val_loss: 25.5179\n",
      "Epoch 140/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 22.6134 - val_loss: 27.3131\n",
      "Epoch 141/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 23.3069 - val_loss: 26.7493\n",
      "Epoch 142/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.7552 - val_loss: 23.0589\n",
      "Epoch 143/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 20.6970 - val_loss: 25.4763\n",
      "Epoch 144/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.4221 - val_loss: 23.7372\n",
      "Epoch 145/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 21.2632 - val_loss: 26.8995\n",
      "Epoch 146/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 38us/sample - loss: 21.2454 - val_loss: 24.2133\n",
      "Epoch 147/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.1675 - val_loss: 23.5069\n",
      "Epoch 148/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.9628 - val_loss: 25.6672\n",
      "Epoch 149/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 21.4897 - val_loss: 25.3385\n",
      "Epoch 150/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 20.5120 - val_loss: 23.9164\n",
      "Epoch 151/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.5590 - val_loss: 22.8029\n",
      "Epoch 152/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 20.2734 - val_loss: 25.1006\n",
      "Epoch 153/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.6441 - val_loss: 25.3979\n",
      "Epoch 154/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 20.5012 - val_loss: 23.4523\n",
      "Epoch 155/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.6812 - val_loss: 24.6934\n",
      "Epoch 156/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 22.1230 - val_loss: 27.4281\n",
      "Epoch 157/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.4723 - val_loss: 23.6361\n",
      "Epoch 158/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.3129 - val_loss: 24.7641\n",
      "Epoch 159/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.0367 - val_loss: 23.6505\n",
      "Epoch 160/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 21.0255 - val_loss: 24.5303\n",
      "Epoch 161/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.2358 - val_loss: 24.8260\n",
      "Epoch 162/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 20.5822 - val_loss: 23.7365\n",
      "Epoch 163/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.6940 - val_loss: 24.5623\n",
      "Epoch 164/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.6478 - val_loss: 24.8798\n",
      "Epoch 165/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.6971 - val_loss: 25.4818\n",
      "Epoch 166/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.9284 - val_loss: 24.0191\n",
      "Epoch 167/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.6232 - val_loss: 23.8372\n",
      "Epoch 168/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.4346 - val_loss: 23.1432\n",
      "Epoch 169/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.4649 - val_loss: 23.1491\n",
      "Epoch 170/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 21.7456 - val_loss: 25.2479\n",
      "Epoch 171/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.2409 - val_loss: 23.5916\n",
      "Epoch 172/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 22.1413 - val_loss: 25.5170\n",
      "Epoch 173/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 20.5008 - val_loss: 28.0969\n",
      "Epoch 174/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 24.4652 - val_loss: 24.8124\n",
      "Epoch 175/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 21.9469 - val_loss: 23.6738\n",
      "Epoch 176/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 21.0394 - val_loss: 24.5150\n",
      "Epoch 177/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.4396 - val_loss: 25.3409\n",
      "Epoch 178/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 22.1960 - val_loss: 30.0738\n",
      "Epoch 179/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 21.4957 - val_loss: 23.5905\n",
      "Epoch 180/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.7093 - val_loss: 26.9943\n",
      "Epoch 181/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 22.3145 - val_loss: 26.5729\n",
      "Epoch 182/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.0093 - val_loss: 27.0788\n",
      "Epoch 183/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 21.3155 - val_loss: 27.2500\n",
      "Epoch 184/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.5841 - val_loss: 28.4523\n",
      "Epoch 185/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.2759 - val_loss: 24.6382\n",
      "Epoch 186/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.7711 - val_loss: 25.2698\n",
      "Epoch 187/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.3053 - val_loss: 26.6818\n",
      "Epoch 188/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 21.0210 - val_loss: 25.1931\n",
      "Epoch 189/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.3919 - val_loss: 30.9899\n",
      "Epoch 190/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 23.3416 - val_loss: 26.0503\n",
      "Epoch 191/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.7691 - val_loss: 25.7378\n",
      "Epoch 192/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.5634 - val_loss: 25.1646\n",
      "Epoch 193/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.3069 - val_loss: 24.6392\n",
      "Epoch 194/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 21.1659 - val_loss: 25.6327\n",
      "Epoch 195/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.7564 - val_loss: 23.2673\n",
      "Epoch 196/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 21.2574 - val_loss: 22.5390\n",
      "Epoch 197/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.9081 - val_loss: 24.7715\n",
      "Epoch 198/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.5326 - val_loss: 25.6509\n",
      "Epoch 199/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.0387 - val_loss: 24.0031\n",
      "Epoch 200/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 22.4975 - val_loss: 27.5213\n",
      "Epoch 201/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.3913 - val_loss: 23.4449\n",
      "Epoch 202/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.9197 - val_loss: 27.5093\n",
      "Epoch 203/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 21.6220 - val_loss: 29.1437\n",
      "Epoch 204/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 22.8482 - val_loss: 22.7861\n",
      "Epoch 205/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.1595 - val_loss: 24.2435\n",
      "Epoch 206/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.5967 - val_loss: 23.6665\n",
      "Epoch 207/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.7731 - val_loss: 25.9339\n",
      "Epoch 208/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.0614 - val_loss: 23.0050\n",
      "Epoch 209/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.6452 - val_loss: 23.0375\n",
      "Epoch 210/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.9376 - val_loss: 23.6918\n",
      "Epoch 211/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.8591 - val_loss: 23.1344\n",
      "Epoch 212/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 21.0700 - val_loss: 24.0760\n",
      "Epoch 213/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.8073 - val_loss: 24.4198\n",
      "Epoch 214/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.1112 - val_loss: 24.2385\n",
      "Epoch 215/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 19.6727 - val_loss: 23.9046\n",
      "Epoch 216/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.3820 - val_loss: 25.4840\n",
      "Epoch 217/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.1136 - val_loss: 25.8449\n",
      "Epoch 218/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.2479 - val_loss: 24.3434\n",
      "Epoch 219/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.7177 - val_loss: 22.7671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 220/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 19.1667 - val_loss: 22.4388\n",
      "Epoch 221/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 19.1674 - val_loss: 23.5320\n",
      "Epoch 222/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.9112 - val_loss: 26.1914\n",
      "Epoch 223/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.7621 - val_loss: 24.0534\n",
      "Epoch 224/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.6308 - val_loss: 24.4529\n",
      "Epoch 225/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.5063 - val_loss: 23.0191\n",
      "Epoch 226/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.0797 - val_loss: 25.1664\n",
      "Epoch 227/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 20.8287 - val_loss: 24.5226\n",
      "Epoch 228/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 21.0729 - val_loss: 25.0948\n",
      "Epoch 229/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 20.0909 - val_loss: 23.4846\n",
      "Epoch 230/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 20.0712 - val_loss: 24.0202\n",
      "Epoch 231/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.6916 - val_loss: 23.2826\n",
      "Epoch 232/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 20.3191 - val_loss: 23.4573\n",
      "Epoch 233/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 19.9062 - val_loss: 23.7085\n",
      "Epoch 234/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.7209 - val_loss: 25.6798\n",
      "Epoch 235/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.8213 - val_loss: 26.9504\n",
      "Epoch 236/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 21.1744 - val_loss: 24.1835\n",
      "Epoch 237/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.5992 - val_loss: 26.0796\n",
      "Epoch 238/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 20.9840 - val_loss: 24.1696\n",
      "Epoch 239/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.0826 - val_loss: 24.3124\n",
      "Epoch 240/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.9711 - val_loss: 24.1990\n",
      "Epoch 241/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.5686 - val_loss: 23.0590\n",
      "Epoch 242/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.0773 - val_loss: 22.4097\n",
      "Epoch 243/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 21.1126 - val_loss: 26.7436\n",
      "Epoch 244/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 20.1798 - val_loss: 23.8674\n",
      "Epoch 245/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.8313 - val_loss: 23.8306\n",
      "Epoch 246/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.2776 - val_loss: 26.9820\n",
      "Epoch 247/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 20.5496 - val_loss: 24.8560\n",
      "Epoch 248/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 22.8660 - val_loss: 27.4830\n",
      "Epoch 249/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 21.8011 - val_loss: 23.3195\n",
      "Epoch 250/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.6306 - val_loss: 23.5404\n",
      "Epoch 251/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.2179 - val_loss: 25.3275\n",
      "Epoch 252/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 19.9349 - val_loss: 23.3484\n",
      "Epoch 253/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 19.0297 - val_loss: 22.4381\n",
      "Epoch 254/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.5591 - val_loss: 24.7527\n",
      "Epoch 255/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.6588 - val_loss: 22.8331\n",
      "Epoch 256/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 20.9686 - val_loss: 27.4604\n",
      "Epoch 257/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.4515 - val_loss: 25.6734\n",
      "Epoch 258/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 19.9526 - val_loss: 22.9961\n",
      "Epoch 259/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.5975 - val_loss: 23.5309\n",
      "Epoch 260/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.3394 - val_loss: 22.4293\n",
      "Epoch 261/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.1221 - val_loss: 29.0380\n",
      "Epoch 262/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.7555 - val_loss: 25.0509\n",
      "Epoch 263/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.5659 - val_loss: 27.8736\n",
      "Epoch 264/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.2698 - val_loss: 22.8217\n",
      "Epoch 265/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.4494 - val_loss: 25.1916\n",
      "Epoch 266/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.3810 - val_loss: 25.2993\n",
      "Epoch 267/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.0986 - val_loss: 24.4232\n",
      "Epoch 268/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.0821 - val_loss: 27.9207\n",
      "Epoch 269/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 19.0850 - val_loss: 25.2768\n",
      "Epoch 270/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.8394 - val_loss: 23.5058\n",
      "Epoch 271/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.2954 - val_loss: 23.1778\n",
      "Epoch 272/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.0369 - val_loss: 24.0137\n",
      "Epoch 273/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.7852 - val_loss: 24.5656\n",
      "Epoch 274/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.3588 - val_loss: 23.5040\n",
      "Epoch 275/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.7663 - val_loss: 22.2912\n",
      "Epoch 276/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.6595 - val_loss: 25.6550\n",
      "Epoch 277/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.7994 - val_loss: 30.1279\n",
      "Epoch 278/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 21.2905 - val_loss: 23.2865\n",
      "Epoch 279/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.5634 - val_loss: 23.0932\n",
      "Epoch 280/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.8735 - val_loss: 24.3142\n",
      "Epoch 281/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.3834 - val_loss: 24.2336\n",
      "Epoch 282/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.8611 - val_loss: 23.6519\n",
      "Epoch 283/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.1852 - val_loss: 22.6683\n",
      "Epoch 284/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 20.2405 - val_loss: 25.8283\n",
      "Epoch 285/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.5367 - val_loss: 26.1109\n",
      "Epoch 286/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 22.0283 - val_loss: 26.4362\n",
      "Epoch 287/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.9207 - val_loss: 23.1135\n",
      "Epoch 288/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 20.9030 - val_loss: 23.7461\n",
      "Epoch 289/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 21.3537 - val_loss: 23.6662\n",
      "Epoch 290/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 19.4389 - val_loss: 26.2011\n",
      "Epoch 291/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 20.9516 - val_loss: 25.3399\n",
      "Epoch 292/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.5213 - val_loss: 23.3002\n",
      "Epoch 293/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.3214 - val_loss: 24.5090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 294/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 19.5286 - val_loss: 23.9429\n",
      "Epoch 295/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.0297 - val_loss: 25.0667\n",
      "Epoch 296/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.1777 - val_loss: 26.7824\n",
      "Epoch 297/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 19.5472 - val_loss: 26.8459\n",
      "Epoch 298/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.6342 - val_loss: 24.7593\n",
      "Epoch 299/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.8855 - val_loss: 23.2907\n",
      "Epoch 300/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.8709 - val_loss: 23.8342\n",
      "Epoch 301/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 20.5019 - val_loss: 22.9864\n",
      "Epoch 302/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.5875 - val_loss: 24.1383\n",
      "Epoch 303/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.9424 - val_loss: 24.3835\n",
      "Epoch 304/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.0983 - val_loss: 22.3725\n",
      "Epoch 305/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.2679 - val_loss: 24.7445\n",
      "Epoch 306/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 20.6502 - val_loss: 23.4721\n",
      "Epoch 307/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.8924 - val_loss: 24.6409\n",
      "Epoch 308/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.4184 - val_loss: 25.0746\n",
      "Epoch 309/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.8852 - val_loss: 25.2757\n",
      "Epoch 310/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.7323 - val_loss: 25.1641\n",
      "Epoch 311/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.5739 - val_loss: 24.3351\n",
      "Epoch 312/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.9466 - val_loss: 26.7961\n",
      "Epoch 313/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.6179 - val_loss: 30.8187\n",
      "Epoch 314/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.6046 - val_loss: 22.6674\n",
      "Epoch 315/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.2837 - val_loss: 28.2684\n",
      "Epoch 316/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.4610 - val_loss: 22.9798\n",
      "Epoch 317/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.4814 - val_loss: 24.0383\n",
      "Epoch 318/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.4614 - val_loss: 25.7349\n",
      "Epoch 319/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.7062 - val_loss: 23.3612\n",
      "Epoch 320/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.0251 - val_loss: 25.9985\n",
      "Epoch 321/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 21.1286 - val_loss: 28.6746\n",
      "Epoch 322/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.1433 - val_loss: 25.6512\n",
      "Epoch 323/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.4504 - val_loss: 26.3696\n",
      "Epoch 324/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.5748 - val_loss: 24.1913\n",
      "Epoch 325/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.5129 - val_loss: 23.6548\n",
      "Epoch 326/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.1728 - val_loss: 24.5167\n",
      "Epoch 327/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.1602 - val_loss: 25.5253\n",
      "Epoch 328/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.7542 - val_loss: 28.7796\n",
      "Epoch 329/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 19.6158 - val_loss: 23.7446\n",
      "Epoch 330/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.9286 - val_loss: 23.2115\n",
      "Epoch 331/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.0057 - val_loss: 26.0792\n",
      "Epoch 332/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.2487 - val_loss: 24.2653\n",
      "Epoch 333/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.7906 - val_loss: 24.1569\n",
      "Epoch 334/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 18.6199 - val_loss: 26.8365\n",
      "Epoch 335/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.5734 - val_loss: 25.5313\n",
      "Epoch 336/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.2708 - val_loss: 24.0165\n",
      "Epoch 337/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.1189 - val_loss: 22.9218\n",
      "Epoch 338/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 20.0483 - val_loss: 23.2920\n",
      "Epoch 339/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.6417 - val_loss: 22.2713\n",
      "Epoch 340/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.4700 - val_loss: 23.8604\n",
      "Epoch 341/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.2107 - val_loss: 22.9653\n",
      "Epoch 342/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.3570 - val_loss: 26.4416\n",
      "Epoch 343/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.9694 - val_loss: 22.3328\n",
      "Epoch 344/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.0783 - val_loss: 22.6683\n",
      "Epoch 345/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.0279 - val_loss: 22.3871\n",
      "Epoch 346/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 18.4630 - val_loss: 24.7425\n",
      "Epoch 347/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 20.2281 - val_loss: 23.4021\n",
      "Epoch 348/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.7505 - val_loss: 25.1742\n",
      "Epoch 349/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.3112 - val_loss: 22.5786\n",
      "Epoch 350/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.2041 - val_loss: 22.7824\n",
      "Epoch 351/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.2417 - val_loss: 25.0261\n",
      "Epoch 352/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.2397 - val_loss: 25.4157\n",
      "Epoch 353/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.4553 - val_loss: 22.6419\n",
      "Epoch 354/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.8266 - val_loss: 25.0918\n",
      "Epoch 355/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.6104 - val_loss: 25.2359\n",
      "Epoch 356/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.7877 - val_loss: 24.7362\n",
      "Epoch 357/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.7244 - val_loss: 31.5978\n",
      "Epoch 358/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.4487 - val_loss: 22.6862\n",
      "Epoch 359/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 20.4933 - val_loss: 22.6789\n",
      "Epoch 360/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.6139 - val_loss: 24.9256\n",
      "Epoch 361/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.0765 - val_loss: 26.7232\n",
      "Epoch 362/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.6261 - val_loss: 23.4357\n",
      "Epoch 363/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.4022 - val_loss: 22.3887\n",
      "Epoch 364/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.1187 - val_loss: 25.5339\n",
      "Epoch 365/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.4254 - val_loss: 22.6829\n",
      "Epoch 366/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.5173 - val_loss: 23.5295\n",
      "Epoch 367/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.2396 - val_loss: 22.8226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 368/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.1624 - val_loss: 27.3479\n",
      "Epoch 369/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.9628 - val_loss: 25.6461\n",
      "Epoch 370/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.7747 - val_loss: 25.0279\n",
      "Epoch 371/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.9922 - val_loss: 27.1524\n",
      "Epoch 372/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.5940 - val_loss: 24.4028\n",
      "Epoch 373/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.0882 - val_loss: 22.9938\n",
      "Epoch 374/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.9621 - val_loss: 25.0513\n",
      "Epoch 375/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.3984 - val_loss: 27.8454\n",
      "Epoch 376/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.5489 - val_loss: 22.6754\n",
      "Epoch 377/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.0277 - val_loss: 24.5306\n",
      "Epoch 378/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.5911 - val_loss: 25.4247\n",
      "Epoch 379/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 17.9489 - val_loss: 24.9312\n",
      "Epoch 380/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.5199 - val_loss: 24.7558\n",
      "Epoch 381/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 19.0958 - val_loss: 23.5612\n",
      "Epoch 382/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.8244 - val_loss: 23.0927\n",
      "Epoch 383/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.0625 - val_loss: 24.7498\n",
      "Epoch 384/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.3942 - val_loss: 23.7560\n",
      "Epoch 385/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.3593 - val_loss: 23.4462\n",
      "Epoch 386/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.1063 - val_loss: 23.3208\n",
      "Epoch 387/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.5400 - val_loss: 22.3161\n",
      "Epoch 388/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 19.4299 - val_loss: 24.4258\n",
      "Epoch 389/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 17.4514 - val_loss: 22.2678\n",
      "Epoch 390/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.2006 - val_loss: 24.2908\n",
      "Epoch 391/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 18.9106 - val_loss: 23.4096\n",
      "Epoch 392/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.6314 - val_loss: 24.1240\n",
      "Epoch 393/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.9707 - val_loss: 23.1133\n",
      "Epoch 394/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.1229 - val_loss: 23.6807\n",
      "Epoch 395/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.3589 - val_loss: 23.3184\n",
      "Epoch 396/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.3239 - val_loss: 23.7299\n",
      "Epoch 397/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.4030 - val_loss: 24.0576\n",
      "Epoch 398/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 18.4356 - val_loss: 23.5376\n",
      "Epoch 399/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 19.3725 - val_loss: 27.5231\n",
      "Epoch 400/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.5508 - val_loss: 23.8162\n",
      "Epoch 401/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.1098 - val_loss: 22.7285\n",
      "Epoch 402/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.0878 - val_loss: 23.3889\n",
      "Epoch 403/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.9278 - val_loss: 24.9358\n",
      "Epoch 404/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.0396 - val_loss: 22.5497\n",
      "Epoch 405/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.2363 - val_loss: 24.4781\n",
      "Epoch 406/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.6364 - val_loss: 23.0233\n",
      "Epoch 407/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.1728 - val_loss: 23.2752\n",
      "Epoch 408/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.3291 - val_loss: 24.1434\n",
      "Epoch 409/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.0187 - val_loss: 23.8753\n",
      "Epoch 410/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.2390 - val_loss: 23.1229\n",
      "Epoch 411/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.7087 - val_loss: 24.0425\n",
      "Epoch 412/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.4205 - val_loss: 22.3293\n",
      "Epoch 413/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 17.7616 - val_loss: 23.3828\n",
      "Epoch 414/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 17.6867 - val_loss: 25.0852\n",
      "Epoch 415/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 18.5403 - val_loss: 23.0532\n",
      "Epoch 416/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.4328 - val_loss: 22.5417\n",
      "Epoch 417/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 18.9342 - val_loss: 23.1015\n",
      "Epoch 418/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.7720 - val_loss: 23.7496\n",
      "Epoch 419/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.8528 - val_loss: 24.8969\n",
      "Epoch 420/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.4051 - val_loss: 24.1677\n",
      "Epoch 421/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 17.7902 - val_loss: 24.2432\n",
      "Epoch 422/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.1581 - val_loss: 30.3649\n",
      "Epoch 423/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.3941 - val_loss: 24.9642\n",
      "Epoch 424/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.9766 - val_loss: 23.4259\n",
      "Epoch 425/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.2794 - val_loss: 25.6782\n",
      "Epoch 426/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.5585 - val_loss: 24.4153\n",
      "Epoch 427/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 17.9575 - val_loss: 24.9953\n",
      "Epoch 428/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.2391 - val_loss: 25.4134\n",
      "Epoch 429/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.1465 - val_loss: 24.4389\n",
      "Epoch 430/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.2482 - val_loss: 23.9288\n",
      "Epoch 431/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.1377 - val_loss: 24.8177\n",
      "Epoch 432/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.4671 - val_loss: 25.6555\n",
      "Epoch 433/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.6526 - val_loss: 24.9914\n",
      "Epoch 434/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 17.8995 - val_loss: 24.4046\n",
      "Epoch 435/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.2776 - val_loss: 23.7113\n",
      "Epoch 436/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.7479 - val_loss: 23.3421\n",
      "Epoch 437/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.9449 - val_loss: 25.8628\n",
      "Epoch 438/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.0699 - val_loss: 22.8920\n",
      "Epoch 439/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.2154 - val_loss: 25.3033\n",
      "Epoch 440/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.1393 - val_loss: 23.6268\n",
      "Epoch 441/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.1654 - val_loss: 24.4337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 442/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.0709 - val_loss: 23.7159\n",
      "Epoch 443/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.0960 - val_loss: 23.6868\n",
      "Epoch 444/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 17.4953 - val_loss: 24.2357\n",
      "Epoch 445/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 19.4088 - val_loss: 27.7675\n",
      "Epoch 446/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 19.3309 - val_loss: 24.1016\n",
      "Epoch 447/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.8694 - val_loss: 24.3706\n",
      "Epoch 448/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 18.9820 - val_loss: 23.4908\n",
      "Epoch 449/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 17.6037 - val_loss: 25.9541\n",
      "Epoch 450/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 18.2006 - val_loss: 22.2422\n",
      "Epoch 451/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.6379 - val_loss: 23.5802\n",
      "Epoch 452/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.3906 - val_loss: 24.8560\n",
      "Epoch 453/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.9403 - val_loss: 24.4001\n",
      "Epoch 454/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 18.3606 - val_loss: 23.0625\n",
      "Epoch 455/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 17.9422 - val_loss: 23.6131\n",
      "Epoch 456/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.0084 - val_loss: 26.8398\n",
      "Epoch 457/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.4700 - val_loss: 25.0047\n",
      "Epoch 458/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.7196 - val_loss: 23.5129\n",
      "Epoch 459/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.7112 - val_loss: 24.0039\n",
      "Epoch 460/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.2015 - val_loss: 26.8334\n",
      "Epoch 461/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.5993 - val_loss: 21.8529\n",
      "Epoch 462/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 17.2249 - val_loss: 24.0800\n",
      "Epoch 463/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.8989 - val_loss: 24.4334\n",
      "Epoch 464/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.0940 - val_loss: 22.5011\n",
      "Epoch 465/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.2550 - val_loss: 24.0125\n",
      "Epoch 466/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.2823 - val_loss: 24.1307\n",
      "Epoch 467/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.5185 - val_loss: 25.8558\n",
      "Epoch 468/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.6720 - val_loss: 24.1600\n",
      "Epoch 469/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.2242 - val_loss: 25.4693\n",
      "Epoch 470/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.2338 - val_loss: 22.7120\n",
      "Epoch 471/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.0440 - val_loss: 23.2421\n",
      "Epoch 472/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.0004 - val_loss: 23.0573\n",
      "Epoch 473/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.6197 - val_loss: 24.7940\n",
      "Epoch 474/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.3220 - val_loss: 25.1213\n",
      "Epoch 475/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.7618 - val_loss: 22.9292\n",
      "Epoch 476/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.7525 - val_loss: 23.5457\n",
      "Epoch 477/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.9948 - val_loss: 24.6533\n",
      "Epoch 478/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.3537 - val_loss: 24.7187\n",
      "Epoch 479/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.1071 - val_loss: 27.0240\n",
      "Epoch 480/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.1075 - val_loss: 24.9976\n",
      "Epoch 481/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 17.9812 - val_loss: 30.1169\n",
      "Epoch 482/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.3242 - val_loss: 24.9576\n",
      "Epoch 483/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 19.0016 - val_loss: 22.3177\n",
      "Epoch 484/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 17.7261 - val_loss: 25.4362\n",
      "Epoch 485/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.7024 - val_loss: 23.1154\n",
      "Epoch 486/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.8895 - val_loss: 21.9628\n",
      "Epoch 487/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.5662 - val_loss: 22.7072\n",
      "Epoch 488/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.1705 - val_loss: 23.8081\n",
      "Epoch 489/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.3191 - val_loss: 24.8651\n",
      "Epoch 490/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.3652 - val_loss: 22.6349\n",
      "Epoch 491/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 17.1802 - val_loss: 23.7517\n",
      "Epoch 492/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.7762 - val_loss: 22.3912\n",
      "Epoch 493/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.9894 - val_loss: 23.1981\n",
      "Epoch 494/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.8098 - val_loss: 25.4240\n",
      "Epoch 495/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.3293 - val_loss: 22.7385\n",
      "Epoch 496/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 18.1274 - val_loss: 23.4576\n",
      "Epoch 497/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.6099 - val_loss: 26.5599\n",
      "Epoch 498/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.3488 - val_loss: 25.0247\n",
      "Epoch 499/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.4988 - val_loss: 21.6697\n",
      "Epoch 500/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 17.7622 - val_loss: 23.9833\n",
      "Epoch 501/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 16.7205 - val_loss: 22.6109\n",
      "Epoch 502/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.9568 - val_loss: 24.2103\n",
      "Epoch 503/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.7436 - val_loss: 24.3295\n",
      "Epoch 504/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 17.3154 - val_loss: 23.5722\n",
      "Epoch 505/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 16.9192 - val_loss: 27.3438\n",
      "Epoch 506/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.2901 - val_loss: 23.5806\n",
      "Epoch 507/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.9865 - val_loss: 24.7400\n",
      "Epoch 508/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 17.4341 - val_loss: 22.3701\n",
      "Epoch 509/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.0403 - val_loss: 24.7912\n",
      "Epoch 510/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.6874 - val_loss: 24.6305\n",
      "Epoch 511/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.9660 - val_loss: 22.1664\n",
      "Epoch 512/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 17.3145 - val_loss: 27.3175\n",
      "Epoch 513/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.0844 - val_loss: 23.9502\n",
      "Epoch 514/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.3812 - val_loss: 23.1739\n",
      "Epoch 515/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.2623 - val_loss: 23.6380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 516/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 16.9862 - val_loss: 22.1323\n",
      "Epoch 517/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.9420 - val_loss: 23.2776\n",
      "Epoch 518/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 17.4549 - val_loss: 23.1109\n",
      "Epoch 519/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.1396 - val_loss: 23.4930\n",
      "Epoch 520/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.3533 - val_loss: 23.1382\n",
      "Epoch 521/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.2834 - val_loss: 22.8489\n",
      "Epoch 522/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.2142 - val_loss: 23.1977\n",
      "Epoch 523/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.3646 - val_loss: 23.7159\n",
      "Epoch 524/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.3858 - val_loss: 26.4401\n",
      "Epoch 525/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.2181 - val_loss: 23.8899\n",
      "Epoch 526/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.9487 - val_loss: 23.3615\n",
      "Epoch 527/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.7802 - val_loss: 23.6550\n",
      "Epoch 528/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 17.1161 - val_loss: 22.4671\n",
      "Epoch 529/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 17.2379 - val_loss: 23.8798\n",
      "Epoch 530/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 16.7967 - val_loss: 23.3445\n",
      "Epoch 531/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.2425 - val_loss: 23.4132\n",
      "Epoch 532/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 17.7281 - val_loss: 24.5830\n",
      "Epoch 533/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.7334 - val_loss: 24.4850\n",
      "Epoch 534/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 17.2951 - val_loss: 23.7496\n",
      "Epoch 535/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.3441 - val_loss: 24.4391\n",
      "Epoch 536/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.0691 - val_loss: 24.0502\n",
      "Epoch 537/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.1486 - val_loss: 25.6704\n",
      "Epoch 538/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.2380 - val_loss: 23.6974\n",
      "Epoch 539/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 16.8224 - val_loss: 25.3869\n",
      "Epoch 540/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.2913 - val_loss: 21.7716\n",
      "Epoch 541/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.5149 - val_loss: 24.1790\n",
      "Epoch 542/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.0359 - val_loss: 24.7563\n",
      "Epoch 543/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 17.2318 - val_loss: 23.6164\n",
      "Epoch 544/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.2817 - val_loss: 23.9372\n",
      "Epoch 545/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.4364 - val_loss: 23.6485\n",
      "Epoch 546/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 16.9706 - val_loss: 23.3314\n",
      "Epoch 547/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.0613 - val_loss: 24.3071\n",
      "Epoch 548/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 17.7391 - val_loss: 25.0871\n",
      "Epoch 549/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.4531 - val_loss: 25.4163\n",
      "Epoch 550/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.0450 - val_loss: 23.9594\n",
      "Epoch 551/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 18.3431 - val_loss: 26.0969\n",
      "Epoch 552/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.0280 - val_loss: 23.4928\n",
      "Epoch 553/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.1199 - val_loss: 23.5952\n",
      "Epoch 554/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.1254 - val_loss: 27.7159\n",
      "Epoch 555/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.0135 - val_loss: 23.0650\n",
      "Epoch 556/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.6145 - val_loss: 27.4483\n",
      "Epoch 557/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.1474 - val_loss: 22.5356\n",
      "Epoch 558/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.4768 - val_loss: 24.9914\n",
      "Epoch 559/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.0526 - val_loss: 23.9807\n",
      "Epoch 560/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 17.2856 - val_loss: 22.0292\n",
      "Epoch 561/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.9839 - val_loss: 23.7412\n",
      "Epoch 562/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 16.6052 - val_loss: 22.4621\n",
      "Epoch 563/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.2647 - val_loss: 23.1192\n",
      "Epoch 564/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.7184 - val_loss: 23.8691\n",
      "Epoch 565/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.3031 - val_loss: 23.5645\n",
      "Epoch 566/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.3696 - val_loss: 23.5447\n",
      "Epoch 567/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.8353 - val_loss: 23.6510\n",
      "Epoch 568/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.7797 - val_loss: 23.6971\n",
      "Epoch 569/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.9347 - val_loss: 25.1138\n",
      "Epoch 570/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.0777 - val_loss: 25.0218\n",
      "Epoch 571/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.7702 - val_loss: 24.1486\n",
      "Epoch 572/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.7084 - val_loss: 23.9865\n",
      "Epoch 573/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.0011 - val_loss: 23.4199\n",
      "Epoch 574/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.2158 - val_loss: 24.3738\n",
      "Epoch 575/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.5281 - val_loss: 22.4477\n",
      "Epoch 576/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 16.9513 - val_loss: 22.3144\n",
      "Epoch 577/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 17.8307 - val_loss: 24.2231\n",
      "Epoch 578/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 17.2609 - val_loss: 22.6818\n",
      "Epoch 579/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.7381 - val_loss: 24.7410\n",
      "Epoch 580/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 18.0831 - val_loss: 22.8013\n",
      "Epoch 581/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.7249 - val_loss: 25.1595\n",
      "Epoch 582/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.4499 - val_loss: 28.4302\n",
      "Epoch 583/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.9626 - val_loss: 22.3004\n",
      "Epoch 584/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.3608 - val_loss: 24.8038\n",
      "Epoch 585/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.9256 - val_loss: 23.9209\n",
      "Epoch 586/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.3370 - val_loss: 24.1662\n",
      "Epoch 587/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 18.0460 - val_loss: 22.8636\n",
      "Epoch 588/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 17.8347 - val_loss: 24.1885\n",
      "Epoch 589/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.0647 - val_loss: 22.8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 590/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 16.7709 - val_loss: 23.5562\n",
      "Epoch 591/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 17.6894 - val_loss: 23.1498\n",
      "Epoch 592/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.1061 - val_loss: 23.8977\n",
      "Epoch 593/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.5556 - val_loss: 26.1795\n",
      "Epoch 594/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.8622 - val_loss: 23.5147\n",
      "Epoch 595/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.9268 - val_loss: 23.1842\n",
      "Epoch 596/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.3602 - val_loss: 28.4672\n",
      "Epoch 597/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.5107 - val_loss: 23.2094\n",
      "Epoch 598/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.0596 - val_loss: 23.4665\n",
      "Epoch 599/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 16.5965 - val_loss: 24.2711\n",
      "Epoch 600/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 16.4188 - val_loss: 23.5056\n",
      "Epoch 601/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.6246 - val_loss: 23.4793\n",
      "Epoch 602/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 17.6384 - val_loss: 25.0952\n",
      "Epoch 603/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.2191 - val_loss: 25.1416\n",
      "Epoch 604/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.5359 - val_loss: 21.8473\n",
      "Epoch 605/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 16.5133 - val_loss: 23.2721\n",
      "Epoch 606/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.4964 - val_loss: 23.5667\n",
      "Epoch 607/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 17.9941 - val_loss: 24.9333\n",
      "Epoch 608/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 16.8688 - val_loss: 24.7491\n",
      "Epoch 609/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 16.6002 - val_loss: 23.2115\n",
      "Epoch 610/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.1117 - val_loss: 21.8580\n",
      "Epoch 611/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.2820 - val_loss: 22.7521\n",
      "Epoch 612/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.0899 - val_loss: 25.9660\n",
      "Epoch 613/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 16.0683 - val_loss: 22.7167\n",
      "Epoch 614/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.0750 - val_loss: 24.7077\n",
      "Epoch 615/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.8945 - val_loss: 23.4933\n",
      "Epoch 616/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 17.2731 - val_loss: 24.9188\n",
      "Epoch 617/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 17.4905 - val_loss: 23.8963\n",
      "Epoch 618/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 17.9112 - val_loss: 23.9167\n",
      "Epoch 619/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.6925 - val_loss: 24.6115\n",
      "Epoch 620/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 17.0639 - val_loss: 23.7401\n",
      "Epoch 621/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.6164 - val_loss: 23.5804\n",
      "Epoch 622/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.9748 - val_loss: 22.8784\n",
      "Epoch 623/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.8120 - val_loss: 22.8107\n",
      "Epoch 624/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 16.6224 - val_loss: 22.9842\n",
      "Epoch 625/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 16.0909 - val_loss: 23.7468\n",
      "Epoch 626/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 16.4825 - val_loss: 24.4398\n",
      "Epoch 627/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.3402 - val_loss: 24.8156\n",
      "Epoch 628/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 16.9447 - val_loss: 23.8344\n",
      "Epoch 629/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.6660 - val_loss: 22.5536\n",
      "Epoch 630/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 17.3242 - val_loss: 23.4519\n",
      "Epoch 631/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 16.1095 - val_loss: 21.9469\n",
      "Epoch 632/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.6458 - val_loss: 23.4540\n",
      "Epoch 633/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.7273 - val_loss: 25.7751\n",
      "Epoch 634/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.1580 - val_loss: 23.9167\n",
      "Epoch 635/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.8537 - val_loss: 24.4170\n",
      "Epoch 636/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.7153 - val_loss: 23.9303\n",
      "Epoch 637/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.4127 - val_loss: 23.4078\n",
      "Epoch 638/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.5819 - val_loss: 23.0801\n",
      "Epoch 639/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 16.1080 - val_loss: 25.1188\n",
      "Epoch 640/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 16.2746 - val_loss: 22.1087\n",
      "Epoch 641/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 16.3866 - val_loss: 22.4743\n",
      "Epoch 642/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 16.7999 - val_loss: 22.2906\n",
      "Epoch 643/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 16.5398 - val_loss: 23.7752\n",
      "Epoch 644/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.9888 - val_loss: 23.4584\n",
      "Epoch 645/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 17.7783 - val_loss: 23.1959\n",
      "Epoch 646/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.0604 - val_loss: 23.1897\n",
      "Epoch 647/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.5110 - val_loss: 23.4721\n",
      "Epoch 648/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.3076 - val_loss: 23.4860\n",
      "Epoch 649/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.6623 - val_loss: 27.9189\n",
      "Epoch 650/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.3985 - val_loss: 22.7151\n",
      "Epoch 651/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.9863 - val_loss: 27.8080\n",
      "Epoch 652/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.3558 - val_loss: 22.7636\n",
      "Epoch 653/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.4681 - val_loss: 22.3473\n",
      "Epoch 654/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 17.8873 - val_loss: 22.8640\n",
      "Epoch 655/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 15.5757 - val_loss: 24.2668\n",
      "Epoch 656/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.0245 - val_loss: 25.1689\n",
      "Epoch 657/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 16.9453 - val_loss: 23.2668\n",
      "Epoch 658/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 16.6046 - val_loss: 24.0280\n",
      "Epoch 659/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 17.6903 - val_loss: 23.7594\n",
      "Epoch 660/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 16.4024 - val_loss: 22.6843\n",
      "Epoch 661/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.0850 - val_loss: 24.6235\n",
      "Epoch 662/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 16.8053 - val_loss: 26.8291\n",
      "Epoch 663/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.6723 - val_loss: 21.7618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 664/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.9274 - val_loss: 25.8265\n",
      "Epoch 665/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.1625 - val_loss: 24.1651\n",
      "Epoch 666/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.4946 - val_loss: 23.8907\n",
      "Epoch 667/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.3554 - val_loss: 22.3476\n",
      "Epoch 668/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.7781 - val_loss: 24.3548\n",
      "Epoch 669/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.3431 - val_loss: 23.8904\n",
      "Epoch 670/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 15.5525 - val_loss: 22.3916\n",
      "Epoch 671/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 16.1660 - val_loss: 21.7395\n",
      "Epoch 672/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 16.1571 - val_loss: 21.2286\n",
      "Epoch 673/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.3417 - val_loss: 28.3173\n",
      "Epoch 674/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.7639 - val_loss: 22.4146\n",
      "Epoch 675/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.7817 - val_loss: 22.0507\n",
      "Epoch 676/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 17.0535 - val_loss: 23.1641\n",
      "Epoch 677/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 16.4007 - val_loss: 22.7248\n",
      "Epoch 678/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.4952 - val_loss: 24.1956\n",
      "Epoch 679/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 16.2145 - val_loss: 23.2610\n",
      "Epoch 680/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.0779 - val_loss: 23.2997\n",
      "Epoch 681/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.7050 - val_loss: 26.2524\n",
      "Epoch 682/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 17.6333 - val_loss: 21.5335\n",
      "Epoch 683/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 15.9948 - val_loss: 23.4126\n",
      "Epoch 684/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 15.6573 - val_loss: 22.9749\n",
      "Epoch 685/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 16.8542 - val_loss: 22.8285\n",
      "Epoch 686/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 15.5680 - val_loss: 23.8456\n",
      "Epoch 687/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 16.4585 - val_loss: 23.2091\n",
      "Epoch 688/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.5506 - val_loss: 23.0690\n",
      "Epoch 689/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 16.8689 - val_loss: 22.6870\n",
      "Epoch 690/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 16.0535 - val_loss: 23.1654\n",
      "Epoch 691/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.2315 - val_loss: 23.2946\n",
      "Epoch 692/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 17.0463 - val_loss: 22.8577\n",
      "Epoch 693/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 16.8040 - val_loss: 25.1582\n",
      "Epoch 694/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 15.6310 - val_loss: 23.0657\n",
      "Epoch 695/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 16.2232 - val_loss: 23.2965\n",
      "Epoch 696/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 15.4289 - val_loss: 24.3732\n",
      "Epoch 697/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.3660 - val_loss: 22.4571\n",
      "Epoch 698/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 15.3392 - val_loss: 22.7194\n",
      "Epoch 699/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.5233 - val_loss: 23.5075\n",
      "Epoch 700/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.9035 - val_loss: 23.4475\n",
      "Epoch 701/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.0853 - val_loss: 23.5376\n",
      "Epoch 702/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 16.4190 - val_loss: 23.5750\n",
      "Epoch 703/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.0873 - val_loss: 22.1087\n",
      "Epoch 704/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 16.1840 - val_loss: 23.1217\n",
      "Epoch 705/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 15.7754 - val_loss: 23.0700\n",
      "Epoch 706/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 17.6716 - val_loss: 22.5621\n",
      "Epoch 707/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 16.4527 - val_loss: 23.3663\n",
      "Epoch 708/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 14.9500 - val_loss: 25.1745\n",
      "Epoch 709/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 16.0610 - val_loss: 22.4391\n",
      "Epoch 710/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 16.4985 - val_loss: 24.6709\n",
      "Epoch 711/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 15.7175 - val_loss: 23.4269\n",
      "Epoch 712/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.4992 - val_loss: 24.0767\n",
      "Epoch 713/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.4579 - val_loss: 23.6786\n",
      "Epoch 714/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.8009 - val_loss: 23.7227\n",
      "Epoch 715/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.7894 - val_loss: 24.9822\n",
      "Epoch 716/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.0289 - val_loss: 22.0780\n",
      "Epoch 717/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.7350 - val_loss: 23.5827\n",
      "Epoch 718/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 16.2193 - val_loss: 23.8734\n",
      "Epoch 719/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.5812 - val_loss: 25.7922\n",
      "Epoch 720/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.3448 - val_loss: 23.5113\n",
      "Epoch 721/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.8782 - val_loss: 23.4938\n",
      "Epoch 722/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 16.3988 - val_loss: 24.8050\n",
      "Epoch 723/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 16.9527 - val_loss: 22.6939\n",
      "Epoch 724/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.7546 - val_loss: 22.7036\n",
      "Epoch 725/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 15.9418 - val_loss: 24.3438\n",
      "Epoch 726/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 16.0034 - val_loss: 22.2922\n",
      "Epoch 727/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 16.3194 - val_loss: 23.4302\n",
      "Epoch 728/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 16.0213 - val_loss: 22.5630\n",
      "Epoch 729/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.8612 - val_loss: 23.5023\n",
      "Epoch 730/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.8675 - val_loss: 25.6287\n",
      "Epoch 731/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.1164 - val_loss: 21.9274\n",
      "Epoch 732/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.1538 - val_loss: 22.9363\n",
      "Epoch 733/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 15.7728 - val_loss: 23.7486\n",
      "Epoch 734/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.0079 - val_loss: 22.8098\n",
      "Epoch 735/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.8894 - val_loss: 23.6172\n",
      "Epoch 736/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.8883 - val_loss: 23.7063\n",
      "Epoch 737/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.3476 - val_loss: 22.2256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 738/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 15.3708 - val_loss: 23.0869\n",
      "Epoch 739/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.3025 - val_loss: 25.1271\n",
      "Epoch 740/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.2553 - val_loss: 24.6167\n",
      "Epoch 741/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 15.9501 - val_loss: 23.3031\n",
      "Epoch 742/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.0301 - val_loss: 22.5785\n",
      "Epoch 743/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.2522 - val_loss: 22.3878\n",
      "Epoch 744/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.2739 - val_loss: 22.1327\n",
      "Epoch 745/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.9630 - val_loss: 23.3366\n",
      "Epoch 746/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 16.0845 - val_loss: 24.0748\n",
      "Epoch 747/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.2766 - val_loss: 24.1312\n",
      "Epoch 748/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.4084 - val_loss: 21.9027\n",
      "Epoch 749/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.0154 - val_loss: 24.5630\n",
      "Epoch 750/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.5148 - val_loss: 23.0402\n",
      "Epoch 751/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 16.4884 - val_loss: 22.9415\n",
      "Epoch 752/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.7351 - val_loss: 25.4925\n",
      "Epoch 753/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.4428 - val_loss: 24.8646\n",
      "Epoch 754/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.0292 - val_loss: 24.7292\n",
      "Epoch 755/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 15.9374 - val_loss: 23.8678\n",
      "Epoch 756/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 15.7806 - val_loss: 24.8179\n",
      "Epoch 757/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 15.5028 - val_loss: 25.1340\n",
      "Epoch 758/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 14.7604 - val_loss: 23.1818\n",
      "Epoch 759/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.2553 - val_loss: 23.9393\n",
      "Epoch 760/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.9151 - val_loss: 23.4565\n",
      "Epoch 761/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.8809 - val_loss: 25.9748\n",
      "Epoch 762/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.7011 - val_loss: 23.5228\n",
      "Epoch 763/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 15.4929 - val_loss: 23.4446\n",
      "Epoch 764/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 16.1200 - val_loss: 22.3063\n",
      "Epoch 765/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 17.1334 - val_loss: 24.5597\n",
      "Epoch 766/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.6393 - val_loss: 23.7382\n",
      "Epoch 767/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.7474 - val_loss: 23.8853\n",
      "Epoch 768/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.7846 - val_loss: 22.0006\n",
      "Epoch 769/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 14.9977 - val_loss: 22.4659\n",
      "Epoch 770/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 14.7186 - val_loss: 21.8791\n",
      "Epoch 771/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.9141 - val_loss: 23.4653\n",
      "Epoch 772/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 16.4800 - val_loss: 22.7865\n",
      "Epoch 773/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 15.5909 - val_loss: 24.9332\n",
      "Epoch 774/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.9579 - val_loss: 22.7345\n",
      "Epoch 775/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.5824 - val_loss: 24.1520\n",
      "Epoch 776/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.9931 - val_loss: 22.5268\n",
      "Epoch 777/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.3730 - val_loss: 24.2221\n",
      "Epoch 778/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.4991 - val_loss: 22.9112\n",
      "Epoch 779/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.0376 - val_loss: 23.1152\n",
      "Epoch 780/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.6945 - val_loss: 22.6376\n",
      "Epoch 781/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 16.9968 - val_loss: 22.4283\n",
      "Epoch 782/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.8374 - val_loss: 23.4465\n",
      "Epoch 783/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 15.8563 - val_loss: 23.0600\n",
      "Epoch 784/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 16.3692 - val_loss: 23.2329\n",
      "Epoch 785/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.6754 - val_loss: 24.1919\n",
      "Epoch 786/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.4199 - val_loss: 26.1388\n",
      "Epoch 787/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 16.9388 - val_loss: 24.6518\n",
      "Epoch 788/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 15.1150 - val_loss: 22.6082\n",
      "Epoch 789/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.5351 - val_loss: 22.1209\n",
      "Epoch 790/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 16.0511 - val_loss: 22.7820\n",
      "Epoch 791/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.3185 - val_loss: 25.8284\n",
      "Epoch 792/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.3776 - val_loss: 24.1164\n",
      "Epoch 793/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.7651 - val_loss: 22.9614\n",
      "Epoch 794/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 16.2423 - val_loss: 23.5583\n",
      "Epoch 795/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.7092 - val_loss: 24.5490\n",
      "Epoch 796/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.4017 - val_loss: 24.7348\n",
      "Epoch 797/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.9017 - val_loss: 24.0591\n",
      "Epoch 798/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.4664 - val_loss: 24.2244\n",
      "Epoch 799/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 16.7537 - val_loss: 23.5973\n",
      "Epoch 800/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 14.5718 - val_loss: 24.7278\n",
      "Epoch 801/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.3964 - val_loss: 26.2154\n",
      "Epoch 802/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 16.6243 - val_loss: 24.9759\n",
      "Epoch 803/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.8333 - val_loss: 23.7481\n",
      "Epoch 804/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.0976 - val_loss: 22.1791\n",
      "Epoch 805/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.6215 - val_loss: 24.5722\n",
      "Epoch 806/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.9345 - val_loss: 22.1701\n",
      "Epoch 807/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.2646 - val_loss: 21.8818\n",
      "Epoch 808/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.1359 - val_loss: 21.8097\n",
      "Epoch 809/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 15.5604 - val_loss: 24.3559\n",
      "Epoch 810/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 14.7126 - val_loss: 23.4232\n",
      "Epoch 811/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.2525 - val_loss: 23.5392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 812/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.3727 - val_loss: 23.2287\n",
      "Epoch 813/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 14.9550 - val_loss: 24.0923\n",
      "Epoch 814/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.1685 - val_loss: 22.7937\n",
      "Epoch 815/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 15.6996 - val_loss: 23.0582\n",
      "Epoch 816/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.9881 - val_loss: 22.6922\n",
      "Epoch 817/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 15.5307 - val_loss: 24.1450\n",
      "Epoch 818/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 15.0775 - val_loss: 23.8729\n",
      "Epoch 819/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.5631 - val_loss: 21.7872\n",
      "Epoch 820/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 15.3841 - val_loss: 29.2008\n",
      "Epoch 821/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.9389 - val_loss: 28.6380\n",
      "Epoch 822/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.7602 - val_loss: 23.9920\n",
      "Epoch 823/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.2790 - val_loss: 21.2989\n",
      "Epoch 824/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.8499 - val_loss: 23.2890\n",
      "Epoch 825/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.3503 - val_loss: 24.1710\n",
      "Epoch 826/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.6029 - val_loss: 23.6826\n",
      "Epoch 827/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.6952 - val_loss: 28.7783\n",
      "Epoch 828/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 18.4905 - val_loss: 23.5342\n",
      "Epoch 829/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.1244 - val_loss: 22.8030\n",
      "Epoch 830/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 16.7745 - val_loss: 25.9414\n",
      "Epoch 831/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 16.3008 - val_loss: 23.0959\n",
      "Epoch 832/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.5018 - val_loss: 26.5377\n",
      "Epoch 833/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.5847 - val_loss: 23.0687\n",
      "Epoch 834/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 14.4777 - val_loss: 22.6370\n",
      "Epoch 835/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 15.4766 - val_loss: 22.2893\n",
      "Epoch 836/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.4376 - val_loss: 22.1939\n",
      "Epoch 837/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 15.1700 - val_loss: 24.4514\n",
      "Epoch 838/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 16.6897 - val_loss: 23.4820\n",
      "Epoch 839/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 15.4576 - val_loss: 24.1866\n",
      "Epoch 840/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.4931 - val_loss: 23.6594\n",
      "Epoch 841/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 14.7175 - val_loss: 24.1042\n",
      "Epoch 842/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 15.4479 - val_loss: 24.3328\n",
      "Epoch 843/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 16.6760 - val_loss: 23.5797\n",
      "Epoch 844/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.4080 - val_loss: 23.5643\n",
      "Epoch 845/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.3974 - val_loss: 21.4495\n",
      "Epoch 846/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.7391 - val_loss: 22.6949\n",
      "Epoch 847/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 15.4614 - val_loss: 23.3797\n",
      "Epoch 848/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 15.0448 - val_loss: 22.9254\n",
      "Epoch 849/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.4091 - val_loss: 24.9587\n",
      "Epoch 850/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 15.3752 - val_loss: 25.1636\n",
      "Epoch 851/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 14.8243 - val_loss: 22.3724\n",
      "Epoch 852/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 15.3378 - val_loss: 22.0246\n",
      "Epoch 853/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.0980 - val_loss: 23.7337\n",
      "Epoch 854/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 15.9025 - val_loss: 26.5094\n",
      "Epoch 855/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.1679 - val_loss: 23.7065\n",
      "Epoch 856/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 15.9589 - val_loss: 25.3086\n",
      "Epoch 857/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.9917 - val_loss: 22.5303\n",
      "Epoch 858/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.1906 - val_loss: 23.2457\n",
      "Epoch 859/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 13.9903 - val_loss: 21.7017\n",
      "Epoch 860/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 14.2434 - val_loss: 24.0130\n",
      "Epoch 861/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.3892 - val_loss: 28.0852\n",
      "Epoch 862/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 15.0667 - val_loss: 23.4966\n",
      "Epoch 863/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 14.9277 - val_loss: 24.9525\n",
      "Epoch 864/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 15.3189 - val_loss: 23.8714\n",
      "Epoch 865/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.5028 - val_loss: 21.8823\n",
      "Epoch 866/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.7557 - val_loss: 25.6542\n",
      "Epoch 867/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 16.4218 - val_loss: 24.5816\n",
      "Epoch 868/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 15.1682 - val_loss: 23.0408\n",
      "Epoch 869/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 15.8899 - val_loss: 25.1019\n",
      "Epoch 870/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.0371 - val_loss: 25.5021\n",
      "Epoch 871/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 16.1395 - val_loss: 22.1113\n",
      "Epoch 872/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.0721 - val_loss: 23.2689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|███████████████▌                                                                   | 3/16 [01:41<07:59, 36.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'batc_normalization': False, 'batch_size': 32, 'dropout': 0, 'epochs': 2000, 'first_neuron': 160, 'hidden_layers': 2, 'hidden_neuron': 100, 'kernel_initializer': 'normal', 'last_activation': 'linear', 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "Train on 1500 samples, validate on 375 samples\n",
      "Epoch 1/2000\n",
      "1500/1500 [==============================] - 0s 258us/sample - loss: 2174.0311 - val_loss: 2120.5982\n",
      "Epoch 2/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 1578.4372 - val_loss: 633.4795\n",
      "Epoch 3/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 524.1814 - val_loss: 384.9822\n",
      "Epoch 4/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 320.8311 - val_loss: 234.7130\n",
      "Epoch 5/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 184.8764 - val_loss: 139.6277\n",
      "Epoch 6/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 119.0043 - val_loss: 92.7467\n",
      "Epoch 7/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 78.3287 - val_loss: 63.7451\n",
      "Epoch 8/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 59.6291 - val_loss: 52.2815\n",
      "Epoch 9/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 45.7385 - val_loss: 42.0772\n",
      "Epoch 10/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 39.2428 - val_loss: 38.4293\n",
      "Epoch 11/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 35.1814 - val_loss: 40.3351\n",
      "Epoch 12/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 32.0152 - val_loss: 30.1172\n",
      "Epoch 13/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 29.1226 - val_loss: 31.3914\n",
      "Epoch 14/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 28.2161 - val_loss: 30.4375\n",
      "Epoch 15/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 27.9938 - val_loss: 32.0990\n",
      "Epoch 16/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 27.8392 - val_loss: 26.0075\n",
      "Epoch 17/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 26.9605 - val_loss: 26.1553\n",
      "Epoch 18/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 27.8358 - val_loss: 29.4716\n",
      "Epoch 19/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 27.8028 - val_loss: 29.4782\n",
      "Epoch 20/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 25.8535 - val_loss: 23.9170\n",
      "Epoch 21/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 25.1517 - val_loss: 26.3803\n",
      "Epoch 22/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 24.8007 - val_loss: 26.4957\n",
      "Epoch 23/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 24.5599 - val_loss: 27.1308\n",
      "Epoch 24/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 24.7880 - val_loss: 32.1336\n",
      "Epoch 25/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 26.6043 - val_loss: 27.2765\n",
      "Epoch 26/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 24.4449 - val_loss: 25.0464\n",
      "Epoch 27/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 25.9441 - val_loss: 24.6288\n",
      "Epoch 28/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 24.5145 - val_loss: 28.9050\n",
      "Epoch 29/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 25.1779 - val_loss: 25.1960\n",
      "Epoch 30/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 23.9632 - val_loss: 25.7331\n",
      "Epoch 31/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 23.5449 - val_loss: 24.0053\n",
      "Epoch 32/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 23.8595 - val_loss: 24.7238\n",
      "Epoch 33/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 25.3097 - val_loss: 27.8655\n",
      "Epoch 34/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 23.0716 - val_loss: 24.5889\n",
      "Epoch 35/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 23.6867 - val_loss: 25.1441\n",
      "Epoch 36/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 23.7199 - val_loss: 29.2985\n",
      "Epoch 37/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 25.7959 - val_loss: 39.1145\n",
      "Epoch 38/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 27.0102 - val_loss: 25.0949\n",
      "Epoch 39/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 26.0755 - val_loss: 30.5306\n",
      "Epoch 40/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 25.0492 - val_loss: 26.2630\n",
      "Epoch 41/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 23.3920 - val_loss: 24.0656\n",
      "Epoch 42/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 24.8337 - val_loss: 36.5166\n",
      "Epoch 43/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 26.0135 - val_loss: 24.0609\n",
      "Epoch 44/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 24.6558 - val_loss: 28.3398\n",
      "Epoch 45/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.5099 - val_loss: 24.9882\n",
      "Epoch 46/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 23.3776 - val_loss: 25.2854\n",
      "Epoch 47/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 23.3937 - val_loss: 27.0180\n",
      "Epoch 48/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 23.1163 - val_loss: 28.5100\n",
      "Epoch 49/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 25.6574 - val_loss: 27.1828\n",
      "Epoch 50/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 23.8372 - val_loss: 24.8180\n",
      "Epoch 51/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 23.2507 - val_loss: 26.5256\n",
      "Epoch 52/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 24.6601 - val_loss: 27.3109\n",
      "Epoch 53/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 23.1062 - val_loss: 27.2675\n",
      "Epoch 54/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 23.5124 - val_loss: 29.4560\n",
      "Epoch 55/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 26.0755 - val_loss: 22.9396\n",
      "Epoch 56/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 23.3896 - val_loss: 28.4282\n",
      "Epoch 57/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 24.2523 - val_loss: 31.6407\n",
      "Epoch 58/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 23.6965 - val_loss: 27.1668\n",
      "Epoch 59/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 26.1527 - val_loss: 29.7491\n",
      "Epoch 60/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 23.4141 - val_loss: 24.7527\n",
      "Epoch 61/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 22.9628 - val_loss: 23.4623\n",
      "Epoch 62/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 24.8559 - val_loss: 27.1671\n",
      "Epoch 63/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 26.3862 - val_loss: 27.6439\n",
      "Epoch 64/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 25.3599 - val_loss: 26.6776\n",
      "Epoch 65/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.0962 - val_loss: 23.6928\n",
      "Epoch 66/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 23.1367 - val_loss: 26.6809\n",
      "Epoch 67/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 25.3901 - val_loss: 28.1709\n",
      "Epoch 68/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 24.4528 - val_loss: 38.8542\n",
      "Epoch 69/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 24.7098 - val_loss: 24.8817\n",
      "Epoch 70/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 21.9525 - val_loss: 23.2230\n",
      "Epoch 71/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 22.0051 - val_loss: 24.9275\n",
      "Epoch 72/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 23.4937 - val_loss: 27.4876\n",
      "Epoch 73/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 21.5592 - val_loss: 25.2635\n",
      "Epoch 74/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 23.2757 - val_loss: 25.9962\n",
      "Epoch 75/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 24.4522 - val_loss: 28.8656\n",
      "Epoch 76/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.2678 - val_loss: 26.4721\n",
      "Epoch 77/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.7607 - val_loss: 27.1601\n",
      "Epoch 78/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 24.4198 - val_loss: 23.1691\n",
      "Epoch 79/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 22.1725 - val_loss: 27.6635\n",
      "Epoch 80/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 23.0740 - val_loss: 26.8109\n",
      "Epoch 81/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.3809 - val_loss: 26.4550\n",
      "Epoch 82/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 21.7453 - val_loss: 24.6113\n",
      "Epoch 83/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 24.4636 - val_loss: 30.2505\n",
      "Epoch 84/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 22.7732 - val_loss: 24.8851\n",
      "Epoch 85/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 22.6672 - val_loss: 27.7131\n",
      "Epoch 86/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.4118 - val_loss: 25.7591\n",
      "Epoch 87/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 23.1387 - val_loss: 23.0392\n",
      "Epoch 88/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 23.5456 - val_loss: 26.7358\n",
      "Epoch 89/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.3249 - val_loss: 24.1077\n",
      "Epoch 90/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.0338 - val_loss: 25.0660\n",
      "Epoch 91/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 20.7530 - val_loss: 24.4080\n",
      "Epoch 92/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 23.5403 - val_loss: 25.5288\n",
      "Epoch 93/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 23.1590 - val_loss: 24.0212\n",
      "Epoch 94/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 23.0765 - val_loss: 25.0256\n",
      "Epoch 95/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 23.5803 - val_loss: 24.6745\n",
      "Epoch 96/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 23.1357 - val_loss: 25.2825\n",
      "Epoch 97/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.8359 - val_loss: 33.0279\n",
      "Epoch 98/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 22.5613 - val_loss: 24.8440\n",
      "Epoch 99/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.9242 - val_loss: 27.5104\n",
      "Epoch 100/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.3992 - val_loss: 29.5299\n",
      "Epoch 101/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 23.6212 - val_loss: 29.9700\n",
      "Epoch 102/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 21.7245 - val_loss: 25.0264\n",
      "Epoch 103/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 22.0516 - val_loss: 22.9946\n",
      "Epoch 104/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.7343 - val_loss: 23.7344\n",
      "Epoch 105/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.1938 - val_loss: 26.5678\n",
      "Epoch 106/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.0839 - val_loss: 23.0375\n",
      "Epoch 107/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 20.6775 - val_loss: 23.9810\n",
      "Epoch 108/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.3370 - val_loss: 26.3424\n",
      "Epoch 109/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 21.1300 - val_loss: 28.9521\n",
      "Epoch 110/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.9151 - val_loss: 24.1611\n",
      "Epoch 111/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.2508 - val_loss: 25.1393\n",
      "Epoch 112/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.3371 - val_loss: 23.6205\n",
      "Epoch 113/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.7269 - val_loss: 27.3918\n",
      "Epoch 114/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.0608 - val_loss: 28.2915\n",
      "Epoch 115/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.0607 - val_loss: 24.7933\n",
      "Epoch 116/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.2797 - val_loss: 23.8362\n",
      "Epoch 117/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 20.9339 - val_loss: 24.1238\n",
      "Epoch 118/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.0716 - val_loss: 27.3852\n",
      "Epoch 119/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.8988 - val_loss: 24.4093\n",
      "Epoch 120/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.5715 - val_loss: 28.8099\n",
      "Epoch 121/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 25.5150 - val_loss: 28.9900\n",
      "Epoch 122/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.1215 - val_loss: 23.5480\n",
      "Epoch 123/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.2905 - val_loss: 24.2978\n",
      "Epoch 124/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.7757 - val_loss: 26.5744\n",
      "Epoch 125/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 22.1477 - val_loss: 23.8728\n",
      "Epoch 126/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.3091 - val_loss: 30.0427\n",
      "Epoch 127/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 23.1016 - val_loss: 24.4977\n",
      "Epoch 128/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.3335 - val_loss: 22.8565\n",
      "Epoch 129/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 21.0626 - val_loss: 26.2842\n",
      "Epoch 130/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.9111 - val_loss: 26.7329\n",
      "Epoch 131/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.9500 - val_loss: 23.1549\n",
      "Epoch 132/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.2133 - val_loss: 24.5512\n",
      "Epoch 133/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.5123 - val_loss: 24.8833\n",
      "Epoch 134/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 23.5568 - val_loss: 29.8973\n",
      "Epoch 135/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 22.0534 - val_loss: 23.7850\n",
      "Epoch 136/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.9010 - val_loss: 25.2735\n",
      "Epoch 137/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 21.2190 - val_loss: 26.6353\n",
      "Epoch 138/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 24.5840 - val_loss: 27.7659\n",
      "Epoch 139/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 23.8055 - val_loss: 26.1399\n",
      "Epoch 140/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 22.3224 - val_loss: 25.0689\n",
      "Epoch 141/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.6019 - val_loss: 27.9360\n",
      "Epoch 142/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 21.1362 - val_loss: 24.9112\n",
      "Epoch 143/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.9767 - val_loss: 25.3199\n",
      "Epoch 144/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.5707 - val_loss: 26.3389\n",
      "Epoch 145/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 20.7797 - val_loss: 24.3892\n",
      "Epoch 146/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 46us/sample - loss: 23.1549 - val_loss: 25.7537\n",
      "Epoch 147/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 21.5950 - val_loss: 28.8387\n",
      "Epoch 148/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.4838 - val_loss: 25.1046\n",
      "Epoch 149/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 21.7563 - val_loss: 23.9416\n",
      "Epoch 150/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 20.8373 - val_loss: 27.4323\n",
      "Epoch 151/2000\n",
      "1500/1500 [==============================] - 0s 53us/sample - loss: 23.2530 - val_loss: 24.1803\n",
      "Epoch 152/2000\n",
      "1500/1500 [==============================] - 0s 69us/sample - loss: 23.4732 - val_loss: 23.6482\n",
      "Epoch 153/2000\n",
      "1500/1500 [==============================] - 0s 57us/sample - loss: 20.7767 - val_loss: 24.3372\n",
      "Epoch 154/2000\n",
      "1500/1500 [==============================] - 0s 51us/sample - loss: 20.0836 - val_loss: 23.3998\n",
      "Epoch 155/2000\n",
      "1500/1500 [==============================] - 0s 50us/sample - loss: 22.0884 - val_loss: 26.9442\n",
      "Epoch 156/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 22.3345 - val_loss: 27.1031\n",
      "Epoch 157/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 20.9528 - val_loss: 24.0540\n",
      "Epoch 158/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.5568 - val_loss: 26.5988\n",
      "Epoch 159/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.8345 - val_loss: 23.2023\n",
      "Epoch 160/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 22.1584 - val_loss: 25.9090\n",
      "Epoch 161/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 20.9926 - val_loss: 24.6472\n",
      "Epoch 162/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.5359 - val_loss: 23.0541\n",
      "Epoch 163/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 21.5565 - val_loss: 23.2293\n",
      "Epoch 164/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.2413 - val_loss: 24.1784\n",
      "Epoch 165/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.9845 - val_loss: 23.8218\n",
      "Epoch 166/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.3246 - val_loss: 23.0669\n",
      "Epoch 167/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.3020 - val_loss: 23.1361\n",
      "Epoch 168/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.9894 - val_loss: 24.1235\n",
      "Epoch 169/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.0305 - val_loss: 25.1994\n",
      "Epoch 170/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.7430 - val_loss: 26.3899\n",
      "Epoch 171/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 20.5903 - val_loss: 25.5481\n",
      "Epoch 172/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.5209 - val_loss: 25.5218\n",
      "Epoch 173/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.4496 - val_loss: 23.5820\n",
      "Epoch 174/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 21.7138 - val_loss: 25.6514\n",
      "Epoch 175/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.9692 - val_loss: 27.2867\n",
      "Epoch 176/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 23.4518 - val_loss: 24.6918\n",
      "Epoch 177/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 20.5093 - val_loss: 24.7372\n",
      "Epoch 178/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 20.1659 - val_loss: 22.8000\n",
      "Epoch 179/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 21.0218 - val_loss: 25.1608\n",
      "Epoch 180/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 20.2870 - val_loss: 23.1062\n",
      "Epoch 181/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.3446 - val_loss: 31.9251\n",
      "Epoch 182/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 21.9458 - val_loss: 26.1339\n",
      "Epoch 183/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 19.8340 - val_loss: 22.9631\n",
      "Epoch 184/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.6199 - val_loss: 26.2010\n",
      "Epoch 185/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 22.5481 - val_loss: 25.2683\n",
      "Epoch 186/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.1221 - val_loss: 23.6646\n",
      "Epoch 187/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 22.5568 - val_loss: 30.0270\n",
      "Epoch 188/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 23.0054 - val_loss: 25.6452\n",
      "Epoch 189/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.6231 - val_loss: 28.9461\n",
      "Epoch 190/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 21.2163 - val_loss: 26.6918\n",
      "Epoch 191/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 20.6656 - val_loss: 23.8126\n",
      "Epoch 192/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.8896 - val_loss: 24.3470\n",
      "Epoch 193/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 20.5102 - val_loss: 25.9620\n",
      "Epoch 194/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 20.2754 - val_loss: 25.2666\n",
      "Epoch 195/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 20.8021 - val_loss: 25.6056\n",
      "Epoch 196/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 21.6049 - val_loss: 25.9040\n",
      "Epoch 197/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.5501 - val_loss: 22.4301\n",
      "Epoch 198/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 20.8562 - val_loss: 23.0248\n",
      "Epoch 199/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 21.0367 - val_loss: 22.0133\n",
      "Epoch 200/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 20.3603 - val_loss: 25.6569\n",
      "Epoch 201/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 19.3649 - val_loss: 24.5943\n",
      "Epoch 202/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 19.0034 - val_loss: 26.6634\n",
      "Epoch 203/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.9679 - val_loss: 23.1633\n",
      "Epoch 204/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.8499 - val_loss: 24.4923\n",
      "Epoch 205/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 20.7224 - val_loss: 24.1788\n",
      "Epoch 206/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 21.9766 - val_loss: 23.6259\n",
      "Epoch 207/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.7091 - val_loss: 26.6009\n",
      "Epoch 208/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.5037 - val_loss: 23.4687\n",
      "Epoch 209/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 22.5022 - val_loss: 25.2961\n",
      "Epoch 210/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 20.4941 - val_loss: 26.0467\n",
      "Epoch 211/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 21.2753 - val_loss: 22.7726\n",
      "Epoch 212/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 20.4160 - val_loss: 22.9889\n",
      "Epoch 213/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 19.5318 - val_loss: 26.9720\n",
      "Epoch 214/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.8601 - val_loss: 22.8692\n",
      "Epoch 215/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.7705 - val_loss: 26.0926\n",
      "Epoch 216/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.2367 - val_loss: 23.5422\n",
      "Epoch 217/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.6130 - val_loss: 23.3799\n",
      "Epoch 218/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.1640 - val_loss: 25.7032\n",
      "Epoch 219/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.4214 - val_loss: 25.5218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 220/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.0814 - val_loss: 23.9699\n",
      "Epoch 221/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 20.4633 - val_loss: 22.9208\n",
      "Epoch 222/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.5307 - val_loss: 28.9937\n",
      "Epoch 223/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 23.2428 - val_loss: 26.1051\n",
      "Epoch 224/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.9063 - val_loss: 24.4851\n",
      "Epoch 225/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 23.1027 - val_loss: 25.1552\n",
      "Epoch 226/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.3301 - val_loss: 25.5721\n",
      "Epoch 227/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 20.4833 - val_loss: 22.5725\n",
      "Epoch 228/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.4586 - val_loss: 25.0743\n",
      "Epoch 229/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.1454 - val_loss: 21.8657\n",
      "Epoch 230/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.2958 - val_loss: 23.8051\n",
      "Epoch 231/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.7768 - val_loss: 30.5060\n",
      "Epoch 232/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.2156 - val_loss: 23.7006\n",
      "Epoch 233/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.6744 - val_loss: 24.1151\n",
      "Epoch 234/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.0365 - val_loss: 25.9747\n",
      "Epoch 235/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.3752 - val_loss: 25.7107\n",
      "Epoch 236/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.2051 - val_loss: 22.3454\n",
      "Epoch 237/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 20.6575 - val_loss: 35.2124\n",
      "Epoch 238/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 21.7655 - val_loss: 26.6918\n",
      "Epoch 239/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 20.1585 - val_loss: 25.7630\n",
      "Epoch 240/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.7590 - val_loss: 28.8361\n",
      "Epoch 241/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.4620 - val_loss: 28.8576\n",
      "Epoch 242/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.1496 - val_loss: 26.3236\n",
      "Epoch 243/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 21.1232 - val_loss: 24.8680\n",
      "Epoch 244/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.6853 - val_loss: 30.3095\n",
      "Epoch 245/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.7888 - val_loss: 24.5558\n",
      "Epoch 246/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.4430 - val_loss: 24.0333\n",
      "Epoch 247/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 18.6883 - val_loss: 26.1703\n",
      "Epoch 248/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.2263 - val_loss: 27.5385\n",
      "Epoch 249/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 21.1742 - val_loss: 25.9471\n",
      "Epoch 250/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.4352 - val_loss: 22.4220\n",
      "Epoch 251/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.0749 - val_loss: 27.0789\n",
      "Epoch 252/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.1068 - val_loss: 22.9352\n",
      "Epoch 253/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.1416 - val_loss: 28.6752\n",
      "Epoch 254/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.8523 - val_loss: 22.7061\n",
      "Epoch 255/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.5898 - val_loss: 22.8228\n",
      "Epoch 256/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 18.6810 - val_loss: 22.2449\n",
      "Epoch 257/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.7050 - val_loss: 23.9200\n",
      "Epoch 258/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.2840 - val_loss: 24.0287\n",
      "Epoch 259/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 18.7942 - val_loss: 23.6471\n",
      "Epoch 260/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 19.1128 - val_loss: 25.6117\n",
      "Epoch 261/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 21.4292 - val_loss: 23.1736\n",
      "Epoch 262/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 24.1104 - val_loss: 28.6466\n",
      "Epoch 263/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 24.0040 - val_loss: 22.9482\n",
      "Epoch 264/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 19.9049 - val_loss: 26.8669\n",
      "Epoch 265/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.3500 - val_loss: 23.6753\n",
      "Epoch 266/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.4215 - val_loss: 25.7747\n",
      "Epoch 267/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.5973 - val_loss: 23.7398\n",
      "Epoch 268/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.1130 - val_loss: 23.6615\n",
      "Epoch 269/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.4301 - val_loss: 22.8065\n",
      "Epoch 270/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 19.4846 - val_loss: 27.3144\n",
      "Epoch 271/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.8055 - val_loss: 25.5420\n",
      "Epoch 272/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 19.2766 - val_loss: 23.3096\n",
      "Epoch 273/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.0046 - val_loss: 23.7229\n",
      "Epoch 274/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.7332 - val_loss: 26.7255\n",
      "Epoch 275/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 19.5719 - val_loss: 25.6842\n",
      "Epoch 276/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.1067 - val_loss: 28.0086\n",
      "Epoch 277/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 19.5342 - val_loss: 26.4490\n",
      "Epoch 278/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.2715 - val_loss: 23.1723\n",
      "Epoch 279/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.1152 - val_loss: 23.4071\n",
      "Epoch 280/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 19.4672 - val_loss: 24.5452\n",
      "Epoch 281/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.3845 - val_loss: 23.2812\n",
      "Epoch 282/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 19.9441 - val_loss: 22.9524\n",
      "Epoch 283/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 18.7690 - val_loss: 25.9043\n",
      "Epoch 284/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.9963 - val_loss: 24.9691\n",
      "Epoch 285/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 21.8585 - val_loss: 27.5252\n",
      "Epoch 286/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.7625 - val_loss: 24.0597\n",
      "Epoch 287/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 24.1969 - val_loss: 31.1838\n",
      "Epoch 288/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.3960 - val_loss: 28.6348\n",
      "Epoch 289/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.3474 - val_loss: 22.9944\n",
      "Epoch 290/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.5638 - val_loss: 24.6760\n",
      "Epoch 291/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 18.8335 - val_loss: 22.7539\n",
      "Epoch 292/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.8488 - val_loss: 24.1287\n",
      "Epoch 293/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 20.4632 - val_loss: 23.2683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 294/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 22.6009 - val_loss: 28.0383\n",
      "Epoch 295/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 20.7715 - val_loss: 23.5694\n",
      "Epoch 296/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.0896 - val_loss: 23.8022\n",
      "Epoch 297/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.4600 - val_loss: 22.4549\n",
      "Epoch 298/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.5604 - val_loss: 24.9641\n",
      "Epoch 299/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.4886 - val_loss: 26.9070\n",
      "Epoch 300/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.8117 - val_loss: 24.3339\n",
      "Epoch 301/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.2448 - val_loss: 25.7462\n",
      "Epoch 302/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.6805 - val_loss: 23.2104\n",
      "Epoch 303/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.4873 - val_loss: 27.5368\n",
      "Epoch 304/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 21.2271 - val_loss: 28.6090\n",
      "Epoch 305/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 18.8998 - val_loss: 23.9887\n",
      "Epoch 306/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.6038 - val_loss: 25.3923\n",
      "Epoch 307/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 19.3327 - val_loss: 23.8729\n",
      "Epoch 308/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.0707 - val_loss: 24.8520\n",
      "Epoch 309/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.6138 - val_loss: 24.2871\n",
      "Epoch 310/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 18.7900 - val_loss: 24.3094\n",
      "Epoch 311/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 19.1498 - val_loss: 23.6419\n",
      "Epoch 312/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.2418 - val_loss: 24.7531\n",
      "Epoch 313/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.4879 - val_loss: 25.7843\n",
      "Epoch 314/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 21.0382 - val_loss: 25.3681\n",
      "Epoch 315/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 20.7676 - val_loss: 27.5471\n",
      "Epoch 316/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.5541 - val_loss: 25.8508\n",
      "Epoch 317/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.2518 - val_loss: 26.6402\n",
      "Epoch 318/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 20.8789 - val_loss: 22.1026\n",
      "Epoch 319/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.8203 - val_loss: 23.7563\n",
      "Epoch 320/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 19.2160 - val_loss: 27.4246\n",
      "Epoch 321/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 19.1075 - val_loss: 23.0802\n",
      "Epoch 322/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.3760 - val_loss: 24.8773\n",
      "Epoch 323/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 19.9745 - val_loss: 24.9779\n",
      "Epoch 324/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.8931 - val_loss: 30.1431\n",
      "Epoch 325/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 19.8272 - val_loss: 23.7752\n",
      "Epoch 326/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 20.6996 - val_loss: 27.8947\n",
      "Epoch 327/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.7019 - val_loss: 23.2752\n",
      "Epoch 328/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 18.3268 - val_loss: 22.4350\n",
      "Epoch 329/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 19.4125 - val_loss: 27.8883\n",
      "Epoch 330/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 19.2648 - val_loss: 23.5209\n",
      "Epoch 331/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.2283 - val_loss: 25.2424\n",
      "Epoch 332/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.2419 - val_loss: 23.0503\n",
      "Epoch 333/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 18.1239 - val_loss: 25.0269\n",
      "Epoch 334/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.4399 - val_loss: 24.2476\n",
      "Epoch 335/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.8663 - val_loss: 27.5062\n",
      "Epoch 336/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.6476 - val_loss: 25.4907\n",
      "Epoch 337/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.3855 - val_loss: 24.5453\n",
      "Epoch 338/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.9065 - val_loss: 23.6786\n",
      "Epoch 339/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 20.2854 - val_loss: 24.0865\n",
      "Epoch 340/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.3920 - val_loss: 23.7350\n",
      "Epoch 341/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.3364 - val_loss: 24.9933\n",
      "Epoch 342/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.2056 - val_loss: 26.0372\n",
      "Epoch 343/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 18.7877 - val_loss: 24.0230\n",
      "Epoch 344/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.3618 - val_loss: 25.1106\n",
      "Epoch 345/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 21.6358 - val_loss: 22.3580\n",
      "Epoch 346/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.5294 - val_loss: 26.0248\n",
      "Epoch 347/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.2904 - val_loss: 26.1172\n",
      "Epoch 348/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.1799 - val_loss: 23.1595\n",
      "Epoch 349/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 17.4810 - val_loss: 22.8242\n",
      "Epoch 350/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.6115 - val_loss: 23.9376\n",
      "Epoch 351/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.4630 - val_loss: 26.6378\n",
      "Epoch 352/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 18.9486 - val_loss: 23.0147\n",
      "Epoch 353/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 17.7950 - val_loss: 24.7929\n",
      "Epoch 354/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.5360 - val_loss: 23.0442\n",
      "Epoch 355/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 17.8917 - val_loss: 23.4819\n",
      "Epoch 356/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 18.1065 - val_loss: 24.4081\n",
      "Epoch 357/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 18.0151 - val_loss: 24.9391\n",
      "Epoch 358/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.4305 - val_loss: 23.2873\n",
      "Epoch 359/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.3777 - val_loss: 25.3898\n",
      "Epoch 360/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 17.4916 - val_loss: 23.4572\n",
      "Epoch 361/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.7481 - val_loss: 25.3876\n",
      "Epoch 362/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 20.7712 - val_loss: 28.6177\n",
      "Epoch 363/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.2100 - val_loss: 24.6170\n",
      "Epoch 364/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.5255 - val_loss: 22.8955\n",
      "Epoch 365/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 20.6553 - val_loss: 25.5892\n",
      "Epoch 366/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 19.0613 - val_loss: 23.2341\n",
      "Epoch 367/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 17.6385 - val_loss: 23.1501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 368/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.3737 - val_loss: 25.3557\n",
      "Epoch 369/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.0277 - val_loss: 24.7382\n",
      "Epoch 370/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.2312 - val_loss: 25.0657\n",
      "Epoch 371/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 18.7908 - val_loss: 23.1291\n",
      "Epoch 372/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.5436 - val_loss: 25.3661\n",
      "Epoch 373/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.5673 - val_loss: 24.2032\n",
      "Epoch 374/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 18.9423 - val_loss: 24.3729\n",
      "Epoch 375/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.3976 - val_loss: 25.1205\n",
      "Epoch 376/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 18.8935 - val_loss: 23.3616\n",
      "Epoch 377/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.3835 - val_loss: 24.6821\n",
      "Epoch 378/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.2156 - val_loss: 22.7202\n",
      "Epoch 379/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 19.2187 - val_loss: 23.2636\n",
      "Epoch 380/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 18.7618 - val_loss: 26.5631\n",
      "Epoch 381/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.0702 - val_loss: 24.9072\n",
      "Epoch 382/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.1863 - val_loss: 30.9599\n",
      "Epoch 383/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 18.9832 - val_loss: 22.3923\n",
      "Epoch 384/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 20.3585 - val_loss: 24.7010\n",
      "Epoch 385/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.3010 - val_loss: 22.8339\n",
      "Epoch 386/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.1898 - val_loss: 23.4216\n",
      "Epoch 387/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 19.9344 - val_loss: 28.6779\n",
      "Epoch 388/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.9524 - val_loss: 22.8778\n",
      "Epoch 389/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 18.0499 - val_loss: 25.7515\n",
      "Epoch 390/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 17.4724 - val_loss: 25.0894\n",
      "Epoch 391/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.1974 - val_loss: 22.9956\n",
      "Epoch 392/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 20.3794 - val_loss: 27.2840\n",
      "Epoch 393/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 17.5044 - val_loss: 23.0353\n",
      "Epoch 394/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 17.8362 - val_loss: 23.7139\n",
      "Epoch 395/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 18.0249 - val_loss: 24.3769\n",
      "Epoch 396/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 18.9786 - val_loss: 24.0778\n",
      "Epoch 397/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.0091 - val_loss: 23.8520\n",
      "Epoch 398/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 18.3600 - val_loss: 22.9572\n",
      "Epoch 399/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 17.1567 - val_loss: 22.6831\n",
      "Epoch 400/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.1946 - val_loss: 23.4813\n",
      "Epoch 401/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 19.0013 - val_loss: 27.0859\n",
      "Epoch 402/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.2514 - val_loss: 25.7264\n",
      "Epoch 403/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 18.3657 - val_loss: 28.7511\n",
      "Epoch 404/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 18.5335 - val_loss: 22.6543\n",
      "Epoch 405/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 17.5121 - val_loss: 23.4440\n",
      "Epoch 406/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 17.3388 - val_loss: 25.9133\n",
      "Epoch 407/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.3901 - val_loss: 23.6928\n",
      "Epoch 408/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.2174 - val_loss: 29.2699\n",
      "Epoch 409/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 20.6774 - val_loss: 24.2788\n",
      "Epoch 410/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.4030 - val_loss: 27.6177\n",
      "Epoch 411/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.7624 - val_loss: 26.1677\n",
      "Epoch 412/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.4041 - val_loss: 27.8744\n",
      "Epoch 413/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 18.0059 - val_loss: 22.9368\n",
      "Epoch 414/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.7058 - val_loss: 27.5837\n",
      "Epoch 415/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 20.5927 - val_loss: 25.4745\n",
      "Epoch 416/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.2636 - val_loss: 23.1852\n",
      "Epoch 417/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 19.0018 - val_loss: 24.3002\n",
      "Epoch 418/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.5842 - val_loss: 24.2597\n",
      "Epoch 419/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 18.6546 - val_loss: 24.2572\n",
      "Epoch 420/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.7988 - val_loss: 25.3718\n",
      "Epoch 421/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 17.4102 - val_loss: 22.9009\n",
      "Epoch 422/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 17.3049 - val_loss: 26.4053\n",
      "Epoch 423/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 17.3005 - val_loss: 24.9291\n",
      "Epoch 424/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 19.0637 - val_loss: 23.7051\n",
      "Epoch 425/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.2240 - val_loss: 23.9655\n",
      "Epoch 426/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 18.2079 - val_loss: 23.6701\n",
      "Epoch 427/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 17.8341 - val_loss: 26.5997\n",
      "Epoch 428/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.2507 - val_loss: 26.5126\n",
      "Epoch 429/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 17.3340 - val_loss: 24.7110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|████████████████████▊                                                              | 4/16 [02:09<06:37, 33.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'batc_normalization': False, 'batch_size': 32, 'dropout': 0, 'epochs': 2000, 'first_neuron': 320, 'hidden_layers': 1, 'hidden_neuron': 50, 'kernel_initializer': 'normal', 'last_activation': 'linear', 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "Train on 1500 samples, validate on 375 samples\n",
      "Epoch 1/2000\n",
      "1500/1500 [==============================] - 0s 240us/sample - loss: 2182.6082 - val_loss: 2173.7532\n",
      "Epoch 2/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 2129.9048 - val_loss: 2052.1395\n",
      "Epoch 3/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 1871.7734 - val_loss: 1612.5707\n",
      "Epoch 4/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 1204.3022 - val_loss: 789.5451\n",
      "Epoch 5/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 646.4937 - val_loss: 546.2339\n",
      "Epoch 6/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 521.5046 - val_loss: 457.3999\n",
      "Epoch 7/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 435.2242 - val_loss: 376.0529\n",
      "Epoch 8/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 351.4802 - val_loss: 298.2598\n",
      "Epoch 9/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 269.6396 - val_loss: 224.4570\n",
      "Epoch 10/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 197.2566 - val_loss: 162.8782\n",
      "Epoch 11/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 143.1852 - val_loss: 120.6791\n",
      "Epoch 12/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 108.1069 - val_loss: 93.9778\n",
      "Epoch 13/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 82.3277 - val_loss: 71.9762\n",
      "Epoch 14/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 64.4214 - val_loss: 58.7027\n",
      "Epoch 15/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 51.5030 - val_loss: 48.1602\n",
      "Epoch 16/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 46.1849 - val_loss: 44.8401\n",
      "Epoch 17/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 39.2635 - val_loss: 39.5847\n",
      "Epoch 18/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 35.3860 - val_loss: 37.4394\n",
      "Epoch 19/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 33.2386 - val_loss: 34.0985\n",
      "Epoch 20/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 30.7811 - val_loss: 32.8934\n",
      "Epoch 21/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 30.4936 - val_loss: 29.9528\n",
      "Epoch 22/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 28.2305 - val_loss: 30.0421\n",
      "Epoch 23/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 28.2277 - val_loss: 26.9375\n",
      "Epoch 24/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 26.6284 - val_loss: 26.7015\n",
      "Epoch 25/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 25.3313 - val_loss: 27.3470\n",
      "Epoch 26/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 25.3514 - val_loss: 26.3233\n",
      "Epoch 27/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 25.0474 - val_loss: 25.8235\n",
      "Epoch 28/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 23.9788 - val_loss: 24.9125\n",
      "Epoch 29/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 23.6397 - val_loss: 27.9550\n",
      "Epoch 30/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 24.2621 - val_loss: 24.2183\n",
      "Epoch 31/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 23.9140 - val_loss: 24.7530\n",
      "Epoch 32/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 23.5786 - val_loss: 24.4726\n",
      "Epoch 33/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 23.7829 - val_loss: 25.5812\n",
      "Epoch 34/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 23.2310 - val_loss: 27.5583\n",
      "Epoch 35/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 23.3432 - val_loss: 29.4455\n",
      "Epoch 36/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 23.8336 - val_loss: 27.6363\n",
      "Epoch 37/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 23.0905 - val_loss: 25.0786\n",
      "Epoch 38/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 22.7899 - val_loss: 24.4676\n",
      "Epoch 39/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 22.8295 - val_loss: 23.6127\n",
      "Epoch 40/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 23.0875 - val_loss: 25.4839\n",
      "Epoch 41/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 22.4574 - val_loss: 25.8371\n",
      "Epoch 42/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 22.6989 - val_loss: 22.8134\n",
      "Epoch 43/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 22.6498 - val_loss: 24.1025\n",
      "Epoch 44/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 23.1099 - val_loss: 24.6916\n",
      "Epoch 45/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 22.5028 - val_loss: 22.9225\n",
      "Epoch 46/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 22.7417 - val_loss: 23.9563\n",
      "Epoch 47/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 22.0409 - val_loss: 22.8811\n",
      "Epoch 48/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 22.2170 - val_loss: 24.9739\n",
      "Epoch 49/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 22.9517 - val_loss: 23.3019\n",
      "Epoch 50/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 22.3769 - val_loss: 23.4738\n",
      "Epoch 51/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 22.1581 - val_loss: 24.3473\n",
      "Epoch 52/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 22.2635 - val_loss: 24.5717\n",
      "Epoch 53/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.7466 - val_loss: 24.6226\n",
      "Epoch 54/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 22.1902 - val_loss: 22.4687\n",
      "Epoch 55/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 22.5130 - val_loss: 25.1895\n",
      "Epoch 56/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 23.5273 - val_loss: 25.3468\n",
      "Epoch 57/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 22.5164 - val_loss: 24.7895\n",
      "Epoch 58/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 22.4654 - val_loss: 25.6002\n",
      "Epoch 59/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.8804 - val_loss: 24.0177\n",
      "Epoch 60/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.9267 - val_loss: 24.0248\n",
      "Epoch 61/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.8129 - val_loss: 23.7540\n",
      "Epoch 62/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 21.3172 - val_loss: 25.2119\n",
      "Epoch 63/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 22.0117 - val_loss: 26.0653\n",
      "Epoch 64/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 23.2460 - val_loss: 24.4553\n",
      "Epoch 65/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.8362 - val_loss: 24.4624\n",
      "Epoch 66/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 22.6171 - val_loss: 22.9997\n",
      "Epoch 67/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 21.8150 - val_loss: 24.8281\n",
      "Epoch 68/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 22.0817 - val_loss: 23.4750\n",
      "Epoch 69/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 22.5086 - val_loss: 23.8223\n",
      "Epoch 70/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 22.8642 - val_loss: 23.7520\n",
      "Epoch 71/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.9287 - val_loss: 25.0128\n",
      "Epoch 72/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.3649 - val_loss: 23.7740\n",
      "Epoch 73/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 22.2333 - val_loss: 25.1175\n",
      "Epoch 74/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.2657 - val_loss: 23.1791\n",
      "Epoch 75/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.5158 - val_loss: 22.8291\n",
      "Epoch 76/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.4911 - val_loss: 24.6130\n",
      "Epoch 77/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.5931 - val_loss: 24.8119\n",
      "Epoch 78/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 22.0101 - val_loss: 24.6219\n",
      "Epoch 79/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.7298 - val_loss: 26.2146\n",
      "Epoch 80/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 22.4021 - val_loss: 26.3628\n",
      "Epoch 81/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 22.6188 - val_loss: 22.4879\n",
      "Epoch 82/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 21.7815 - val_loss: 25.2818\n",
      "Epoch 83/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.4506 - val_loss: 23.1792\n",
      "Epoch 84/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 22.0047 - val_loss: 24.3612\n",
      "Epoch 85/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.1138 - val_loss: 22.4635\n",
      "Epoch 86/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 21.0557 - val_loss: 22.2577\n",
      "Epoch 87/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.9201 - val_loss: 26.2400\n",
      "Epoch 88/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 21.5208 - val_loss: 25.2928\n",
      "Epoch 89/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 22.0928 - val_loss: 23.0949\n",
      "Epoch 90/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.8254 - val_loss: 26.4230\n",
      "Epoch 91/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.5857 - val_loss: 23.2806\n",
      "Epoch 92/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.7760 - val_loss: 24.4168\n",
      "Epoch 93/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 21.6009 - val_loss: 22.7078\n",
      "Epoch 94/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.3021 - val_loss: 23.1309\n",
      "Epoch 95/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 22.0840 - val_loss: 24.0065\n",
      "Epoch 96/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 20.9661 - val_loss: 24.0114\n",
      "Epoch 97/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.0524 - val_loss: 23.2448\n",
      "Epoch 98/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 21.8372 - val_loss: 25.4047\n",
      "Epoch 99/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.8750 - val_loss: 24.3000\n",
      "Epoch 100/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 22.4786 - val_loss: 23.5525\n",
      "Epoch 101/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.2751 - val_loss: 24.3278\n",
      "Epoch 102/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.1039 - val_loss: 22.1229\n",
      "Epoch 103/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 21.1129 - val_loss: 24.1300\n",
      "Epoch 104/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 20.9770 - val_loss: 26.7981\n",
      "Epoch 105/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.5649 - val_loss: 24.1676\n",
      "Epoch 106/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 21.1300 - val_loss: 23.9611\n",
      "Epoch 107/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 21.3186 - val_loss: 24.0153\n",
      "Epoch 108/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.4533 - val_loss: 25.3117\n",
      "Epoch 109/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.3711 - val_loss: 22.4958\n",
      "Epoch 110/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.0783 - val_loss: 22.9983\n",
      "Epoch 111/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.1142 - val_loss: 26.2388\n",
      "Epoch 112/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 21.9296 - val_loss: 23.2826\n",
      "Epoch 113/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.8128 - val_loss: 24.5255\n",
      "Epoch 114/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.4408 - val_loss: 23.5173\n",
      "Epoch 115/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.5707 - val_loss: 22.4032\n",
      "Epoch 116/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.5726 - val_loss: 23.1534\n",
      "Epoch 117/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 21.3081 - val_loss: 24.7859\n",
      "Epoch 118/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.1004 - val_loss: 22.4899\n",
      "Epoch 119/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.1685 - val_loss: 23.5035\n",
      "Epoch 120/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 20.9651 - val_loss: 22.3960\n",
      "Epoch 121/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.1565 - val_loss: 22.9691\n",
      "Epoch 122/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.6847 - val_loss: 23.6841\n",
      "Epoch 123/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.6676 - val_loss: 24.4385\n",
      "Epoch 124/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.1544 - val_loss: 23.2139\n",
      "Epoch 125/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.7769 - val_loss: 23.3907\n",
      "Epoch 126/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.7020 - val_loss: 22.5658\n",
      "Epoch 127/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.3774 - val_loss: 24.3462\n",
      "Epoch 128/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.8502 - val_loss: 23.4657\n",
      "Epoch 129/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.5792 - val_loss: 23.9210\n",
      "Epoch 130/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 20.5097 - val_loss: 23.2755\n",
      "Epoch 131/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 21.7895 - val_loss: 23.3444\n",
      "Epoch 132/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.1520 - val_loss: 22.3442\n",
      "Epoch 133/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.9271 - val_loss: 22.7985\n",
      "Epoch 134/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.7617 - val_loss: 23.2773\n",
      "Epoch 135/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.4354 - val_loss: 23.2004\n",
      "Epoch 136/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.2674 - val_loss: 23.5780\n",
      "Epoch 137/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 21.1919 - val_loss: 23.5198\n",
      "Epoch 138/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.9423 - val_loss: 22.7712\n",
      "Epoch 139/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.8215 - val_loss: 23.4923\n",
      "Epoch 140/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.0155 - val_loss: 22.3133\n",
      "Epoch 141/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 21.0186 - val_loss: 21.9747\n",
      "Epoch 142/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.7614 - val_loss: 24.3185\n",
      "Epoch 143/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.2902 - val_loss: 22.1255\n",
      "Epoch 144/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.0891 - val_loss: 22.9017\n",
      "Epoch 145/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 21.1026 - val_loss: 24.9730\n",
      "Epoch 146/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.7039 - val_loss: 24.6344\n",
      "Epoch 147/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.9164 - val_loss: 22.8002\n",
      "Epoch 148/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.3987 - val_loss: 23.3170\n",
      "Epoch 149/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.6707 - val_loss: 22.3839\n",
      "Epoch 150/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 21.4755 - val_loss: 22.8685\n",
      "Epoch 151/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 20.7163 - val_loss: 23.7652\n",
      "Epoch 152/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 20.9517 - val_loss: 24.1961\n",
      "Epoch 153/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.4828 - val_loss: 23.0230\n",
      "Epoch 154/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.5096 - val_loss: 24.7071\n",
      "Epoch 155/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.4636 - val_loss: 23.6127\n",
      "Epoch 156/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.0187 - val_loss: 22.9656\n",
      "Epoch 157/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.6266 - val_loss: 26.4404\n",
      "Epoch 158/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.7798 - val_loss: 23.8545\n",
      "Epoch 159/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.5280 - val_loss: 25.9984\n",
      "Epoch 160/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.5953 - val_loss: 22.9379\n",
      "Epoch 161/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.9274 - val_loss: 23.2277\n",
      "Epoch 162/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 22.0412 - val_loss: 24.2586\n",
      "Epoch 163/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.8563 - val_loss: 23.2574\n",
      "Epoch 164/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.0456 - val_loss: 23.1059\n",
      "Epoch 165/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.4142 - val_loss: 25.3943\n",
      "Epoch 166/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.9419 - val_loss: 23.5841\n",
      "Epoch 167/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.5299 - val_loss: 23.9995\n",
      "Epoch 168/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.5734 - val_loss: 23.3338\n",
      "Epoch 169/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.3860 - val_loss: 24.4758\n",
      "Epoch 170/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.5085 - val_loss: 23.3260\n",
      "Epoch 171/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.7116 - val_loss: 22.9553\n",
      "Epoch 172/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.2384 - val_loss: 23.6747\n",
      "Epoch 173/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 21.5697 - val_loss: 23.5737\n",
      "Epoch 174/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 20.8599 - val_loss: 22.2526\n",
      "Epoch 175/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 21.1527 - val_loss: 24.3318\n",
      "Epoch 176/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.8253 - val_loss: 23.8972\n",
      "Epoch 177/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.2677 - val_loss: 24.3632\n",
      "Epoch 178/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.6400 - val_loss: 24.1050\n",
      "Epoch 179/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.0564 - val_loss: 22.9057\n",
      "Epoch 180/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 20.7480 - val_loss: 23.1246\n",
      "Epoch 181/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 20.5428 - val_loss: 24.1703\n",
      "Epoch 182/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.0364 - val_loss: 22.1520\n",
      "Epoch 183/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.0051 - val_loss: 23.3908\n",
      "Epoch 184/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 20.9455 - val_loss: 26.6034\n",
      "Epoch 185/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.3472 - val_loss: 22.6189\n",
      "Epoch 186/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.5358 - val_loss: 21.9180\n",
      "Epoch 187/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 20.4394 - val_loss: 25.6496\n",
      "Epoch 188/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.1224 - val_loss: 22.6533\n",
      "Epoch 189/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.3189 - val_loss: 23.6069\n",
      "Epoch 190/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 20.2269 - val_loss: 24.1749\n",
      "Epoch 191/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.8958 - val_loss: 23.3014\n",
      "Epoch 192/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.1871 - val_loss: 24.0223\n",
      "Epoch 193/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.0402 - val_loss: 22.6689\n",
      "Epoch 194/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.6598 - val_loss: 22.1550\n",
      "Epoch 195/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 20.1445 - val_loss: 25.7035\n",
      "Epoch 196/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 20.8319 - val_loss: 24.1077\n",
      "Epoch 197/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.6768 - val_loss: 23.9987\n",
      "Epoch 198/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.4950 - val_loss: 22.5787\n",
      "Epoch 199/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 20.1622 - val_loss: 22.1886\n",
      "Epoch 200/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.6719 - val_loss: 22.0028\n",
      "Epoch 201/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 20.2632 - val_loss: 22.1475\n",
      "Epoch 202/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.9353 - val_loss: 22.5266\n",
      "Epoch 203/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.8525 - val_loss: 22.9370\n",
      "Epoch 204/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 20.2325 - val_loss: 23.4451\n",
      "Epoch 205/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.4634 - val_loss: 24.3944\n",
      "Epoch 206/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.7125 - val_loss: 23.2851\n",
      "Epoch 207/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 20.4313 - val_loss: 24.8038\n",
      "Epoch 208/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 20.3051 - val_loss: 22.2492\n",
      "Epoch 209/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.3335 - val_loss: 23.5833\n",
      "Epoch 210/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.1135 - val_loss: 22.4308\n",
      "Epoch 211/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 21.0891 - val_loss: 23.3321\n",
      "Epoch 212/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.2977 - val_loss: 26.8296\n",
      "Epoch 213/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.5675 - val_loss: 22.4658\n",
      "Epoch 214/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 20.2266 - val_loss: 22.6527\n",
      "Epoch 215/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.9087 - val_loss: 23.2294\n",
      "Epoch 216/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 20.1950 - val_loss: 23.8108\n",
      "Epoch 217/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.9105 - val_loss: 23.2329\n",
      "Epoch 218/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.9184 - val_loss: 22.1802\n",
      "Epoch 219/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.7276 - val_loss: 22.1663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 220/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.8339 - val_loss: 24.5047\n",
      "Epoch 221/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.1483 - val_loss: 24.3630\n",
      "Epoch 222/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.4232 - val_loss: 23.6439\n",
      "Epoch 223/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 22.0773 - val_loss: 23.3871\n",
      "Epoch 224/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.4154 - val_loss: 24.0221\n",
      "Epoch 225/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 20.0503 - val_loss: 23.4187\n",
      "Epoch 226/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.2920 - val_loss: 23.1472\n",
      "Epoch 227/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 20.1827 - val_loss: 22.6442\n",
      "Epoch 228/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.9807 - val_loss: 23.2205\n",
      "Epoch 229/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.6239 - val_loss: 24.5030\n",
      "Epoch 230/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 21.7434 - val_loss: 26.1653\n",
      "Epoch 231/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.0753 - val_loss: 22.6813\n",
      "Epoch 232/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.9591 - val_loss: 23.4388\n",
      "Epoch 233/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 19.7018 - val_loss: 25.3035\n",
      "Epoch 234/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.7763 - val_loss: 23.6268\n",
      "Epoch 235/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.6107 - val_loss: 24.0954\n",
      "Epoch 236/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.5253 - val_loss: 22.3685\n",
      "Epoch 237/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.6859 - val_loss: 25.6920\n",
      "Epoch 238/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.5122 - val_loss: 23.6162\n",
      "Epoch 239/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.4857 - val_loss: 25.7694\n",
      "Epoch 240/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 20.6691 - val_loss: 23.0946\n",
      "Epoch 241/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.7267 - val_loss: 22.2986\n",
      "Epoch 242/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 20.2677 - val_loss: 23.8403\n",
      "Epoch 243/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.9138 - val_loss: 23.9337\n",
      "Epoch 244/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 20.3329 - val_loss: 22.7674\n",
      "Epoch 245/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.6582 - val_loss: 22.5848\n",
      "Epoch 246/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.6607 - val_loss: 23.8698\n",
      "Epoch 247/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.8980 - val_loss: 22.6484\n",
      "Epoch 248/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 20.7393 - val_loss: 24.4387\n",
      "Epoch 249/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.6194 - val_loss: 22.1562\n",
      "Epoch 250/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.5938 - val_loss: 23.4046\n",
      "Epoch 251/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.4957 - val_loss: 23.5572\n",
      "Epoch 252/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.4788 - val_loss: 22.8228\n",
      "Epoch 253/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 20.1954 - val_loss: 23.0800\n",
      "Epoch 254/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.6173 - val_loss: 23.2227\n",
      "Epoch 255/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 19.6154 - val_loss: 24.6507\n",
      "Epoch 256/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.2210 - val_loss: 22.9880\n",
      "Epoch 257/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.6255 - val_loss: 22.1534\n",
      "Epoch 258/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.2181 - val_loss: 22.2735\n",
      "Epoch 259/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.3214 - val_loss: 22.7144\n",
      "Epoch 260/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 20.5741 - val_loss: 24.3124\n",
      "Epoch 261/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.7835 - val_loss: 22.9406\n",
      "Epoch 262/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.0300 - val_loss: 21.8863\n",
      "Epoch 263/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.7848 - val_loss: 22.6369\n",
      "Epoch 264/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.4103 - val_loss: 23.7025\n",
      "Epoch 265/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.2156 - val_loss: 23.6948\n",
      "Epoch 266/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.4070 - val_loss: 23.2809\n",
      "Epoch 267/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.4606 - val_loss: 26.3327\n",
      "Epoch 268/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.6965 - val_loss: 22.9314\n",
      "Epoch 269/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.7615 - val_loss: 24.0140\n",
      "Epoch 270/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.7470 - val_loss: 22.9938\n",
      "Epoch 271/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.2291 - val_loss: 23.8242\n",
      "Epoch 272/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 21.1536 - val_loss: 25.0218\n",
      "Epoch 273/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.1569 - val_loss: 22.3948\n",
      "Epoch 274/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.3219 - val_loss: 24.3436\n",
      "Epoch 275/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.4653 - val_loss: 22.0042\n",
      "Epoch 276/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 19.2841 - val_loss: 22.6470\n",
      "Epoch 277/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.9591 - val_loss: 22.5417\n",
      "Epoch 278/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.1493 - val_loss: 22.5044\n",
      "Epoch 279/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.2734 - val_loss: 23.3486\n",
      "Epoch 280/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.3993 - val_loss: 23.0935\n",
      "Epoch 281/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.0100 - val_loss: 22.7541\n",
      "Epoch 282/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.5571 - val_loss: 24.9818\n",
      "Epoch 283/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.0199 - val_loss: 22.3762\n",
      "Epoch 284/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.4700 - val_loss: 23.4227\n",
      "Epoch 285/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.9971 - val_loss: 22.9226\n",
      "Epoch 286/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.0618 - val_loss: 23.8793\n",
      "Epoch 287/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.4681 - val_loss: 22.7920\n",
      "Epoch 288/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.2083 - val_loss: 21.9685\n",
      "Epoch 289/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.1281 - val_loss: 22.7105\n",
      "Epoch 290/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.7925 - val_loss: 22.1808\n",
      "Epoch 291/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.1419 - val_loss: 23.2749\n",
      "Epoch 292/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.7268 - val_loss: 26.1129\n",
      "Epoch 293/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.2743 - val_loss: 27.2930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 294/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.5671 - val_loss: 22.6367\n",
      "Epoch 295/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.4954 - val_loss: 22.0999\n",
      "Epoch 296/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.0420 - val_loss: 22.2694\n",
      "Epoch 297/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.8985 - val_loss: 23.7988\n",
      "Epoch 298/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.7773 - val_loss: 22.7075\n",
      "Epoch 299/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.8871 - val_loss: 24.2495\n",
      "Epoch 300/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 20.3019 - val_loss: 23.1863\n",
      "Epoch 301/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.1634 - val_loss: 22.6514\n",
      "Epoch 302/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.2881 - val_loss: 24.9119\n",
      "Epoch 303/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.3135 - val_loss: 23.4182\n",
      "Epoch 304/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.7692 - val_loss: 23.3591\n",
      "Epoch 305/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.7319 - val_loss: 25.9174\n",
      "Epoch 306/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.7314 - val_loss: 22.4260\n",
      "Epoch 307/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.6372 - val_loss: 23.3088\n",
      "Epoch 308/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.5172 - val_loss: 22.4483\n",
      "Epoch 309/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.2073 - val_loss: 22.9110\n",
      "Epoch 310/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 20.1578 - val_loss: 25.0073\n",
      "Epoch 311/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.1680 - val_loss: 22.8560\n",
      "Epoch 312/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.7348 - val_loss: 25.7145\n",
      "Epoch 313/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.0934 - val_loss: 22.4876\n",
      "Epoch 314/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.7706 - val_loss: 23.6183\n",
      "Epoch 315/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.6859 - val_loss: 24.9100\n",
      "Epoch 316/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.7630 - val_loss: 24.0054\n",
      "Epoch 317/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.1578 - val_loss: 22.3529\n",
      "Epoch 318/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.0635 - val_loss: 23.6763\n",
      "Epoch 319/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.6131 - val_loss: 23.5118\n",
      "Epoch 320/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.0222 - val_loss: 22.9998\n",
      "Epoch 321/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.4344 - val_loss: 24.2110\n",
      "Epoch 322/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 20.3906 - val_loss: 23.1850\n",
      "Epoch 323/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.3636 - val_loss: 23.0570\n",
      "Epoch 324/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.1949 - val_loss: 23.0125\n",
      "Epoch 325/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.9857 - val_loss: 25.1719\n",
      "Epoch 326/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.4236 - val_loss: 23.3286\n",
      "Epoch 327/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.2275 - val_loss: 22.5750\n",
      "Epoch 328/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.9293 - val_loss: 24.0681\n",
      "Epoch 329/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.9046 - val_loss: 24.0608\n",
      "Epoch 330/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.7722 - val_loss: 22.7367\n",
      "Epoch 331/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.1812 - val_loss: 24.2202\n",
      "Epoch 332/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.4022 - val_loss: 24.7826\n",
      "Epoch 333/2000\n",
      "1500/1500 [==============================] - 0s 59us/sample - loss: 19.5900 - val_loss: 25.9680\n",
      "Epoch 334/2000\n",
      "1500/1500 [==============================] - 0s 60us/sample - loss: 19.5215 - val_loss: 23.9596\n",
      "Epoch 335/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 19.0475 - val_loss: 21.7452\n",
      "Epoch 336/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.1754 - val_loss: 23.8653\n",
      "Epoch 337/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.5162 - val_loss: 22.0563\n",
      "Epoch 338/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.6153 - val_loss: 24.8611\n",
      "Epoch 339/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.2126 - val_loss: 21.5783\n",
      "Epoch 340/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.9378 - val_loss: 22.5975\n",
      "Epoch 341/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.4055 - val_loss: 23.8665\n",
      "Epoch 342/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.9483 - val_loss: 23.6759\n",
      "Epoch 343/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 19.9743 - val_loss: 25.0086\n",
      "Epoch 344/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.9600 - val_loss: 25.3519\n",
      "Epoch 345/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.0965 - val_loss: 21.9897\n",
      "Epoch 346/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.7997 - val_loss: 21.9921\n",
      "Epoch 347/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.0532 - val_loss: 22.3485\n",
      "Epoch 348/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.3646 - val_loss: 23.0257\n",
      "Epoch 349/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.1774 - val_loss: 22.5186\n",
      "Epoch 350/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.1340 - val_loss: 24.5418\n",
      "Epoch 351/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.2333 - val_loss: 22.5001\n",
      "Epoch 352/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.8731 - val_loss: 21.8168\n",
      "Epoch 353/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.8232 - val_loss: 22.7079\n",
      "Epoch 354/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.9012 - val_loss: 23.2277\n",
      "Epoch 355/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.7387 - val_loss: 23.3079\n",
      "Epoch 356/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.4796 - val_loss: 23.2601\n",
      "Epoch 357/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.3308 - val_loss: 22.5412\n",
      "Epoch 358/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.6762 - val_loss: 22.8559\n",
      "Epoch 359/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.5202 - val_loss: 24.3555\n",
      "Epoch 360/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.9701 - val_loss: 22.0459\n",
      "Epoch 361/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 18.7308 - val_loss: 22.7534\n",
      "Epoch 362/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.5025 - val_loss: 23.2109\n",
      "Epoch 363/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.0979 - val_loss: 23.4171\n",
      "Epoch 364/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.2915 - val_loss: 22.9535\n",
      "Epoch 365/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.7850 - val_loss: 26.8524\n",
      "Epoch 366/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 20.0738 - val_loss: 23.3352\n",
      "Epoch 367/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.0351 - val_loss: 23.4201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 368/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.4868 - val_loss: 22.8012\n",
      "Epoch 369/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.5774 - val_loss: 22.8181\n",
      "Epoch 370/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.0078 - val_loss: 23.3526\n",
      "Epoch 371/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.0815 - val_loss: 22.2751\n",
      "Epoch 372/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.3812 - val_loss: 23.6289\n",
      "Epoch 373/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.3379 - val_loss: 23.3773\n",
      "Epoch 374/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.8466 - val_loss: 23.2817\n",
      "Epoch 375/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.2874 - val_loss: 24.6813\n",
      "Epoch 376/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.0449 - val_loss: 25.1627\n",
      "Epoch 377/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.1103 - val_loss: 22.9340\n",
      "Epoch 378/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 18.7340 - val_loss: 23.1073\n",
      "Epoch 379/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.5932 - val_loss: 23.7623\n",
      "Epoch 380/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 18.6190 - val_loss: 23.3938\n",
      "Epoch 381/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 18.2175 - val_loss: 23.2881\n",
      "Epoch 382/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.7963 - val_loss: 21.7417\n",
      "Epoch 383/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 19.0204 - val_loss: 23.9269\n",
      "Epoch 384/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.5225 - val_loss: 25.0217\n",
      "Epoch 385/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.2561 - val_loss: 24.1963\n",
      "Epoch 386/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.8450 - val_loss: 24.3517\n",
      "Epoch 387/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.0178 - val_loss: 23.4440\n",
      "Epoch 388/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.0863 - val_loss: 22.9554\n",
      "Epoch 389/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.2607 - val_loss: 23.2698\n",
      "Epoch 390/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.7530 - val_loss: 21.9938\n",
      "Epoch 391/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.0446 - val_loss: 24.1859\n",
      "Epoch 392/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.6642 - val_loss: 23.6221\n",
      "Epoch 393/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.2624 - val_loss: 22.4759\n",
      "Epoch 394/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 20.0018 - val_loss: 21.8333\n",
      "Epoch 395/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.9175 - val_loss: 23.3447\n",
      "Epoch 396/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.2617 - val_loss: 21.4714\n",
      "Epoch 397/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.4565 - val_loss: 21.7917\n",
      "Epoch 398/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.7110 - val_loss: 22.9693\n",
      "Epoch 399/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.4129 - val_loss: 22.3926\n",
      "Epoch 400/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.7970 - val_loss: 22.3601\n",
      "Epoch 401/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.7605 - val_loss: 22.4403\n",
      "Epoch 402/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.6884 - val_loss: 21.5257\n",
      "Epoch 403/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.3983 - val_loss: 23.4400\n",
      "Epoch 404/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.4644 - val_loss: 23.6999\n",
      "Epoch 405/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.9196 - val_loss: 23.1036\n",
      "Epoch 406/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.3219 - val_loss: 24.5109\n",
      "Epoch 407/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.6873 - val_loss: 23.8956\n",
      "Epoch 408/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.6350 - val_loss: 23.6649\n",
      "Epoch 409/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.5573 - val_loss: 23.4208\n",
      "Epoch 410/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.3717 - val_loss: 25.6564\n",
      "Epoch 411/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.3736 - val_loss: 23.7962\n",
      "Epoch 412/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.5877 - val_loss: 23.8910\n",
      "Epoch 413/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 18.3801 - val_loss: 22.8026\n",
      "Epoch 414/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.0984 - val_loss: 22.2383\n",
      "Epoch 415/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.8450 - val_loss: 26.2711\n",
      "Epoch 416/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.7826 - val_loss: 22.8290\n",
      "Epoch 417/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.4373 - val_loss: 21.7800\n",
      "Epoch 418/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.3541 - val_loss: 25.1278\n",
      "Epoch 419/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.4794 - val_loss: 23.2225\n",
      "Epoch 420/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.8959 - val_loss: 22.3825\n",
      "Epoch 421/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.8549 - val_loss: 21.6026\n",
      "Epoch 422/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.3650 - val_loss: 22.3777\n",
      "Epoch 423/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.9777 - val_loss: 25.1589\n",
      "Epoch 424/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.7738 - val_loss: 22.6687\n",
      "Epoch 425/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.0960 - val_loss: 22.6747\n",
      "Epoch 426/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.7263 - val_loss: 22.8921\n",
      "Epoch 427/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.2838 - val_loss: 23.6261\n",
      "Epoch 428/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.2741 - val_loss: 22.0398\n",
      "Epoch 429/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.5268 - val_loss: 23.4654\n",
      "Epoch 430/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.2157 - val_loss: 24.2374\n",
      "Epoch 431/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.6132 - val_loss: 24.7553\n",
      "Epoch 432/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.3364 - val_loss: 22.4254\n",
      "Epoch 433/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.4569 - val_loss: 22.5262\n",
      "Epoch 434/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.8216 - val_loss: 22.1036\n",
      "Epoch 435/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 17.8299 - val_loss: 22.5810\n",
      "Epoch 436/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 18.3660 - val_loss: 24.7548\n",
      "Epoch 437/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.6155 - val_loss: 25.0158\n",
      "Epoch 438/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.0890 - val_loss: 24.3427\n",
      "Epoch 439/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.3999 - val_loss: 23.3004\n",
      "Epoch 440/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.1501 - val_loss: 22.3657\n",
      "Epoch 441/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.6743 - val_loss: 23.3543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 442/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.9077 - val_loss: 21.8715\n",
      "Epoch 443/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.8443 - val_loss: 25.0564\n",
      "Epoch 444/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.3333 - val_loss: 22.7571\n",
      "Epoch 445/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.1695 - val_loss: 24.3608\n",
      "Epoch 446/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.3169 - val_loss: 22.8356\n",
      "Epoch 447/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 17.9356 - val_loss: 23.7063\n",
      "Epoch 448/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.4013 - val_loss: 22.8079\n",
      "Epoch 449/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.8914 - val_loss: 23.6310\n",
      "Epoch 450/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.9436 - val_loss: 23.5353\n",
      "Epoch 451/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.3751 - val_loss: 22.9352\n",
      "Epoch 452/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.0105 - val_loss: 22.5595\n",
      "Epoch 453/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.1349 - val_loss: 22.2617\n",
      "Epoch 454/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.1929 - val_loss: 22.9999\n",
      "Epoch 455/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.2455 - val_loss: 23.1638\n",
      "Epoch 456/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.6136 - val_loss: 22.8956\n",
      "Epoch 457/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.1323 - val_loss: 25.5105\n",
      "Epoch 458/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.9335 - val_loss: 21.9843\n",
      "Epoch 459/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.2104 - val_loss: 22.6406\n",
      "Epoch 460/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.0126 - val_loss: 22.9096\n",
      "Epoch 461/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.1509 - val_loss: 21.9391\n",
      "Epoch 462/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.9163 - val_loss: 23.3020\n",
      "Epoch 463/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.7254 - val_loss: 22.2913\n",
      "Epoch 464/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.2816 - val_loss: 22.4706\n",
      "Epoch 465/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 18.3569 - val_loss: 23.0346\n",
      "Epoch 466/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.5243 - val_loss: 21.8480\n",
      "Epoch 467/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.4722 - val_loss: 22.2858\n",
      "Epoch 468/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.0142 - val_loss: 21.9015\n",
      "Epoch 469/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.2852 - val_loss: 22.2680\n",
      "Epoch 470/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.8447 - val_loss: 22.6297\n",
      "Epoch 471/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.3827 - val_loss: 21.7130\n",
      "Epoch 472/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 17.6468 - val_loss: 22.5441\n",
      "Epoch 473/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.1709 - val_loss: 26.3381\n",
      "Epoch 474/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.5373 - val_loss: 24.5801\n",
      "Epoch 475/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.8084 - val_loss: 22.9871\n",
      "Epoch 476/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.5103 - val_loss: 22.0836\n",
      "Epoch 477/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.3107 - val_loss: 23.2305\n",
      "Epoch 478/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 17.9483 - val_loss: 22.8219\n",
      "Epoch 479/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.5640 - val_loss: 24.6205\n",
      "Epoch 480/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.6067 - val_loss: 22.5763\n",
      "Epoch 481/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.0827 - val_loss: 24.1642\n",
      "Epoch 482/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 17.7753 - val_loss: 23.0666\n",
      "Epoch 483/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 19.1792 - val_loss: 23.7149\n",
      "Epoch 484/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.0844 - val_loss: 22.6714\n",
      "Epoch 485/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 17.8393 - val_loss: 22.6586\n",
      "Epoch 486/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.6742 - val_loss: 23.3807\n",
      "Epoch 487/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.0074 - val_loss: 23.9363\n",
      "Epoch 488/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.6159 - val_loss: 22.0100\n",
      "Epoch 489/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.1723 - val_loss: 22.9144\n",
      "Epoch 490/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.9529 - val_loss: 23.8836\n",
      "Epoch 491/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.5947 - val_loss: 25.4488\n",
      "Epoch 492/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.9802 - val_loss: 24.4540\n",
      "Epoch 493/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.1530 - val_loss: 23.0967\n",
      "Epoch 494/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.2746 - val_loss: 23.5537\n",
      "Epoch 495/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.0200 - val_loss: 21.7957\n",
      "Epoch 496/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.2892 - val_loss: 23.4266\n",
      "Epoch 497/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.0678 - val_loss: 22.4428\n",
      "Epoch 498/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.7034 - val_loss: 21.9196\n",
      "Epoch 499/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 17.5074 - val_loss: 23.1791\n",
      "Epoch 500/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.2167 - val_loss: 24.0434\n",
      "Epoch 501/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.1721 - val_loss: 24.0020\n",
      "Epoch 502/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 17.6567 - val_loss: 24.1395\n",
      "Epoch 503/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.0722 - val_loss: 22.2219\n",
      "Epoch 504/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.3012 - val_loss: 22.1867\n",
      "Epoch 505/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.6617 - val_loss: 22.3116\n",
      "Epoch 506/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.0122 - val_loss: 23.0108\n",
      "Epoch 507/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.7192 - val_loss: 22.4002\n",
      "Epoch 508/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.5001 - val_loss: 22.2820\n",
      "Epoch 509/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 17.3110 - val_loss: 22.4218\n",
      "Epoch 510/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.2946 - val_loss: 22.8254\n",
      "Epoch 511/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.7946 - val_loss: 23.2343\n",
      "Epoch 512/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.8397 - val_loss: 22.5112\n",
      "Epoch 513/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.1098 - val_loss: 23.1253\n",
      "Epoch 514/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.7764 - val_loss: 23.1556\n",
      "Epoch 515/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 17.4015 - val_loss: 21.7064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 516/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.8981 - val_loss: 22.4172\n",
      "Epoch 517/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.6916 - val_loss: 22.3288\n",
      "Epoch 518/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 17.9022 - val_loss: 23.3435\n",
      "Epoch 519/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.0280 - val_loss: 23.4769\n",
      "Epoch 520/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 17.9840 - val_loss: 25.0223\n",
      "Epoch 521/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.9976 - val_loss: 23.3420\n",
      "Epoch 522/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.5704 - val_loss: 23.3665\n",
      "Epoch 523/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.9958 - val_loss: 24.5594\n",
      "Epoch 524/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 17.4093 - val_loss: 22.7850\n",
      "Epoch 525/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.7436 - val_loss: 23.7285\n",
      "Epoch 526/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.1255 - val_loss: 23.0574\n",
      "Epoch 527/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.6370 - val_loss: 21.7377\n",
      "Epoch 528/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.2229 - val_loss: 22.3225\n",
      "Epoch 529/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.0517 - val_loss: 25.2025\n",
      "Epoch 530/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.8417 - val_loss: 23.4993\n",
      "Epoch 531/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.3235 - val_loss: 22.6716\n",
      "Epoch 532/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.2191 - val_loss: 23.3337\n",
      "Epoch 533/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.0033 - val_loss: 22.1100\n",
      "Epoch 534/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.5903 - val_loss: 23.7177\n",
      "Epoch 535/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.8441 - val_loss: 21.8116\n",
      "Epoch 536/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.5900 - val_loss: 22.2593\n",
      "Epoch 537/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.5049 - val_loss: 23.1869\n",
      "Epoch 538/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 17.3252 - val_loss: 23.9553\n",
      "Epoch 539/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.6339 - val_loss: 22.6551\n",
      "Epoch 540/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 17.4872 - val_loss: 24.9109\n",
      "Epoch 541/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.8026 - val_loss: 23.1262\n",
      "Epoch 542/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.7029 - val_loss: 24.3060\n",
      "Epoch 543/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.8840 - val_loss: 22.7944\n",
      "Epoch 544/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.6820 - val_loss: 22.3365\n",
      "Epoch 545/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.0219 - val_loss: 24.5458\n",
      "Epoch 546/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.3126 - val_loss: 23.9127\n",
      "Epoch 547/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.3301 - val_loss: 22.0023\n",
      "Epoch 548/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.7073 - val_loss: 21.9887\n",
      "Epoch 549/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.3429 - val_loss: 22.4199\n",
      "Epoch 550/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.4508 - val_loss: 23.5828\n",
      "Epoch 551/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.4218 - val_loss: 24.0747\n",
      "Epoch 552/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.2108 - val_loss: 23.1648\n",
      "Epoch 553/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 17.1141 - val_loss: 22.9660\n",
      "Epoch 554/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.1976 - val_loss: 23.8729\n",
      "Epoch 555/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.8048 - val_loss: 23.5177\n",
      "Epoch 556/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.8519 - val_loss: 24.4612\n",
      "Epoch 557/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 17.1231 - val_loss: 22.8502\n",
      "Epoch 558/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.1934 - val_loss: 22.1660\n",
      "Epoch 559/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.1902 - val_loss: 22.1292\n",
      "Epoch 560/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.2445 - val_loss: 21.6935\n",
      "Epoch 561/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.1283 - val_loss: 22.9966\n",
      "Epoch 562/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.1941 - val_loss: 23.8640\n",
      "Epoch 563/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.4201 - val_loss: 22.8242\n",
      "Epoch 564/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.8455 - val_loss: 22.5600\n",
      "Epoch 565/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.9380 - val_loss: 23.3095\n",
      "Epoch 566/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 17.7668 - val_loss: 23.3200\n",
      "Epoch 567/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.9700 - val_loss: 22.3543\n",
      "Epoch 568/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.2871 - val_loss: 21.8708\n",
      "Epoch 569/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 17.3235 - val_loss: 22.4126\n",
      "Epoch 570/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.0423 - val_loss: 22.1508\n",
      "Epoch 571/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.2754 - val_loss: 23.1521\n",
      "Epoch 572/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.5251 - val_loss: 23.4164\n",
      "Epoch 573/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.7067 - val_loss: 22.6954\n",
      "Epoch 574/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.6559 - val_loss: 21.8267\n",
      "Epoch 575/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.3393 - val_loss: 21.5452\n",
      "Epoch 576/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 17.2219 - val_loss: 22.5728\n",
      "Epoch 577/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.3996 - val_loss: 22.2323\n",
      "Epoch 578/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 17.1654 - val_loss: 25.7772\n",
      "Epoch 579/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.5322 - val_loss: 23.3226\n",
      "Epoch 580/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 17.1886 - val_loss: 21.7534\n",
      "Epoch 581/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.6794 - val_loss: 21.8778\n",
      "Epoch 582/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.1149 - val_loss: 21.8001\n",
      "Epoch 583/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 17.1194 - val_loss: 23.1352\n",
      "Epoch 584/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.5022 - val_loss: 22.6311\n",
      "Epoch 585/2000\n",
      "1500/1500 [==============================] - 0s 36us/sample - loss: 17.2724 - val_loss: 22.7401\n",
      "Epoch 586/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.2583 - val_loss: 21.7106\n",
      "Epoch 587/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.8223 - val_loss: 24.9102\n",
      "Epoch 588/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 17.3790 - val_loss: 22.1488\n",
      "Epoch 589/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 16.8815 - val_loss: 23.1690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 590/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.1324 - val_loss: 22.5536\n",
      "Epoch 591/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.7807 - val_loss: 23.9600\n",
      "Epoch 592/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 18.0071 - val_loss: 23.1688\n",
      "Epoch 593/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.0127 - val_loss: 22.5512\n",
      "Epoch 594/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.4125 - val_loss: 24.9153\n",
      "Epoch 595/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.8758 - val_loss: 23.5449\n",
      "Epoch 596/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 17.3792 - val_loss: 22.9975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|█████████████████████████▉                                                         | 5/16 [02:44<06:11, 33.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'batc_normalization': False, 'batch_size': 32, 'dropout': 0, 'epochs': 2000, 'first_neuron': 320, 'hidden_layers': 1, 'hidden_neuron': 100, 'kernel_initializer': 'normal', 'last_activation': 'linear', 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "Train on 1500 samples, validate on 375 samples\n",
      "Epoch 1/2000\n",
      "1500/1500 [==============================] - 0s 319us/sample - loss: 2180.3696 - val_loss: 2164.8880\n",
      "Epoch 2/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 2092.0147 - val_loss: 1959.1957\n",
      "Epoch 3/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 1658.0377 - val_loss: 1225.0351\n",
      "Epoch 4/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 821.3499 - val_loss: 588.2378\n",
      "Epoch 5/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 557.3361 - val_loss: 485.1053\n",
      "Epoch 6/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 451.2879 - val_loss: 388.5747\n",
      "Epoch 7/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 355.7515 - val_loss: 299.6197\n",
      "Epoch 8/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 264.5375 - val_loss: 214.5966\n",
      "Epoch 9/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 184.4067 - val_loss: 152.6731\n",
      "Epoch 10/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 131.1646 - val_loss: 108.8481\n",
      "Epoch 11/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 96.0543 - val_loss: 82.9573\n",
      "Epoch 12/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 72.9016 - val_loss: 63.6135\n",
      "Epoch 13/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 57.6347 - val_loss: 56.4806\n",
      "Epoch 14/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 46.9017 - val_loss: 45.8682\n",
      "Epoch 15/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 39.7587 - val_loss: 40.7724\n",
      "Epoch 16/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 35.3627 - val_loss: 36.5518\n",
      "Epoch 17/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 33.2407 - val_loss: 34.4351\n",
      "Epoch 18/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 30.5847 - val_loss: 33.3833\n",
      "Epoch 19/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 29.4216 - val_loss: 29.2137\n",
      "Epoch 20/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 27.8526 - val_loss: 28.8126\n",
      "Epoch 21/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 26.5560 - val_loss: 27.8543\n",
      "Epoch 22/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 26.0307 - val_loss: 27.5668\n",
      "Epoch 23/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 25.9928 - val_loss: 26.9665\n",
      "Epoch 24/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 24.6776 - val_loss: 24.5960\n",
      "Epoch 25/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 23.9438 - val_loss: 27.4071\n",
      "Epoch 26/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 24.7812 - val_loss: 23.5816\n",
      "Epoch 27/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 24.0408 - val_loss: 24.5977\n",
      "Epoch 28/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 23.9406 - val_loss: 24.3154\n",
      "Epoch 29/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 24.2510 - val_loss: 23.9337\n",
      "Epoch 30/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 23.1876 - val_loss: 26.0286\n",
      "Epoch 31/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 23.4099 - val_loss: 24.7057\n",
      "Epoch 32/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 23.0973 - val_loss: 23.9261\n",
      "Epoch 33/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 23.2920 - val_loss: 24.5268\n",
      "Epoch 34/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 22.5323 - val_loss: 25.7028\n",
      "Epoch 35/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.7684 - val_loss: 23.6703\n",
      "Epoch 36/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 23.0020 - val_loss: 23.6477\n",
      "Epoch 37/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 23.1561 - val_loss: 24.3326\n",
      "Epoch 38/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.3642 - val_loss: 23.3283\n",
      "Epoch 39/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.6322 - val_loss: 25.7699\n",
      "Epoch 40/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 23.0448 - val_loss: 24.1751\n",
      "Epoch 41/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 23.3648 - val_loss: 24.8851\n",
      "Epoch 42/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 23.1887 - val_loss: 23.0276\n",
      "Epoch 43/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 22.4515 - val_loss: 27.5372\n",
      "Epoch 44/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.5657 - val_loss: 24.1682\n",
      "Epoch 45/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 22.2555 - val_loss: 24.9563\n",
      "Epoch 46/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 23.0093 - val_loss: 28.5513\n",
      "Epoch 47/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 22.6454 - val_loss: 24.0073\n",
      "Epoch 48/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 22.4012 - val_loss: 25.1879\n",
      "Epoch 49/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.5866 - val_loss: 24.0695\n",
      "Epoch 50/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.8725 - val_loss: 24.8589\n",
      "Epoch 51/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 22.4353 - val_loss: 25.5180\n",
      "Epoch 52/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 21.9425 - val_loss: 24.3449\n",
      "Epoch 53/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.9575 - val_loss: 23.4769\n",
      "Epoch 54/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 24.8009 - val_loss: 24.2469\n",
      "Epoch 55/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.8187 - val_loss: 25.4102\n",
      "Epoch 56/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.2747 - val_loss: 24.3965\n",
      "Epoch 57/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 22.2300 - val_loss: 24.2841\n",
      "Epoch 58/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 21.5252 - val_loss: 23.1927\n",
      "Epoch 59/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.8104 - val_loss: 24.1781\n",
      "Epoch 60/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.3966 - val_loss: 23.9193\n",
      "Epoch 61/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 22.5128 - val_loss: 24.8357\n",
      "Epoch 62/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.3469 - val_loss: 25.8193\n",
      "Epoch 63/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 22.2943 - val_loss: 26.2263\n",
      "Epoch 64/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.1841 - val_loss: 24.4054\n",
      "Epoch 65/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.5421 - val_loss: 22.9603\n",
      "Epoch 66/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.8190 - val_loss: 23.4799\n",
      "Epoch 67/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.8468 - val_loss: 24.4320\n",
      "Epoch 68/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 22.9277 - val_loss: 25.4346\n",
      "Epoch 69/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.5667 - val_loss: 22.9124\n",
      "Epoch 70/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.0546 - val_loss: 23.7245\n",
      "Epoch 71/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 22.1909 - val_loss: 23.8727\n",
      "Epoch 72/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.8842 - val_loss: 24.9401\n",
      "Epoch 73/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 21.7663 - val_loss: 23.2181\n",
      "Epoch 74/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.5751 - val_loss: 23.3944\n",
      "Epoch 75/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.2568 - val_loss: 24.3343\n",
      "Epoch 76/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 22.3765 - val_loss: 25.0938\n",
      "Epoch 77/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 21.1596 - val_loss: 24.1695\n",
      "Epoch 78/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 21.8037 - val_loss: 24.7627\n",
      "Epoch 79/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 23.0151 - val_loss: 27.0635\n",
      "Epoch 80/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 21.9726 - val_loss: 24.3315\n",
      "Epoch 81/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.0171 - val_loss: 23.8395\n",
      "Epoch 82/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 21.5168 - val_loss: 24.5685\n",
      "Epoch 83/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 21.5386 - val_loss: 23.3232\n",
      "Epoch 84/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 21.9162 - val_loss: 24.1810\n",
      "Epoch 85/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 22.9822 - val_loss: 25.4149\n",
      "Epoch 86/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 21.2478 - val_loss: 22.9174\n",
      "Epoch 87/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 21.4918 - val_loss: 24.7608\n",
      "Epoch 88/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.6960 - val_loss: 25.6347\n",
      "Epoch 89/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.2958 - val_loss: 22.6665\n",
      "Epoch 90/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.4034 - val_loss: 23.5124\n",
      "Epoch 91/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.0644 - val_loss: 23.0204\n",
      "Epoch 92/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 21.4499 - val_loss: 23.8769\n",
      "Epoch 93/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 21.1366 - val_loss: 28.8304\n",
      "Epoch 94/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.9533 - val_loss: 27.0175\n",
      "Epoch 95/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 23.1011 - val_loss: 24.4922\n",
      "Epoch 96/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 21.2056 - val_loss: 24.4479\n",
      "Epoch 97/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.0563 - val_loss: 23.1254\n",
      "Epoch 98/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 21.9687 - val_loss: 23.7856\n",
      "Epoch 99/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 21.3983 - val_loss: 27.5512\n",
      "Epoch 100/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 21.8635 - val_loss: 24.1516\n",
      "Epoch 101/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 21.8575 - val_loss: 23.1728\n",
      "Epoch 102/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 20.7028 - val_loss: 23.7902\n",
      "Epoch 103/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 21.0472 - val_loss: 24.0197\n",
      "Epoch 104/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.6400 - val_loss: 24.0904\n",
      "Epoch 105/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.9221 - val_loss: 23.0806\n",
      "Epoch 106/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 22.4806 - val_loss: 25.6325\n",
      "Epoch 107/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 21.4526 - val_loss: 24.7377\n",
      "Epoch 108/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 21.6937 - val_loss: 24.5972\n",
      "Epoch 109/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.0221 - val_loss: 24.3159\n",
      "Epoch 110/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 22.1487 - val_loss: 24.3721\n",
      "Epoch 111/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.3616 - val_loss: 25.1059\n",
      "Epoch 112/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.9603 - val_loss: 23.9160\n",
      "Epoch 113/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 21.1174 - val_loss: 24.2983\n",
      "Epoch 114/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 21.5946 - val_loss: 24.2759\n",
      "Epoch 115/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 21.0461 - val_loss: 24.8599\n",
      "Epoch 116/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 21.4220 - val_loss: 24.6837\n",
      "Epoch 117/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.7515 - val_loss: 23.0840\n",
      "Epoch 118/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.9580 - val_loss: 25.8405\n",
      "Epoch 119/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 21.3344 - val_loss: 22.6849\n",
      "Epoch 120/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.1633 - val_loss: 22.9668\n",
      "Epoch 121/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 21.7963 - val_loss: 25.5184\n",
      "Epoch 122/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.2184 - val_loss: 23.7593\n",
      "Epoch 123/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 21.4430 - val_loss: 24.6488\n",
      "Epoch 124/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.4397 - val_loss: 24.1591\n",
      "Epoch 125/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 21.4859 - val_loss: 23.9018\n",
      "Epoch 126/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.9889 - val_loss: 25.6692\n",
      "Epoch 127/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 20.9344 - val_loss: 26.9209\n",
      "Epoch 128/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.8030 - val_loss: 26.1600\n",
      "Epoch 129/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.8630 - val_loss: 22.8564\n",
      "Epoch 130/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 21.3873 - val_loss: 23.8218\n",
      "Epoch 131/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.1114 - val_loss: 24.5279\n",
      "Epoch 132/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 20.7338 - val_loss: 23.8370\n",
      "Epoch 133/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 20.6945 - val_loss: 22.7101\n",
      "Epoch 134/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.8155 - val_loss: 22.5808\n",
      "Epoch 135/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.3786 - val_loss: 22.8431\n",
      "Epoch 136/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 21.6249 - val_loss: 24.1602\n",
      "Epoch 137/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.0340 - val_loss: 24.0281\n",
      "Epoch 138/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 20.4192 - val_loss: 23.1927\n",
      "Epoch 139/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 20.7101 - val_loss: 24.0372\n",
      "Epoch 140/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 20.4101 - val_loss: 24.8748\n",
      "Epoch 141/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 21.3576 - val_loss: 23.5475\n",
      "Epoch 142/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.6123 - val_loss: 23.0909\n",
      "Epoch 143/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.4247 - val_loss: 23.8435\n",
      "Epoch 144/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 20.5458 - val_loss: 22.9678\n",
      "Epoch 145/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 20.3122 - val_loss: 25.0978\n",
      "Epoch 146/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 43us/sample - loss: 20.5378 - val_loss: 24.7817\n",
      "Epoch 147/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.6751 - val_loss: 23.0296\n",
      "Epoch 148/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 20.6869 - val_loss: 23.9066\n",
      "Epoch 149/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.5877 - val_loss: 24.6141\n",
      "Epoch 150/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 20.8697 - val_loss: 24.0130\n",
      "Epoch 151/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 22.1270 - val_loss: 24.0093\n",
      "Epoch 152/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.4644 - val_loss: 24.6104\n",
      "Epoch 153/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.3542 - val_loss: 24.3208\n",
      "Epoch 154/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.3371 - val_loss: 23.4616\n",
      "Epoch 155/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 21.4818 - val_loss: 23.0690\n",
      "Epoch 156/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.6274 - val_loss: 23.6322\n",
      "Epoch 157/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 21.2360 - val_loss: 23.1728\n",
      "Epoch 158/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 20.9091 - val_loss: 27.5481\n",
      "Epoch 159/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 20.6991 - val_loss: 23.0970\n",
      "Epoch 160/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 20.5734 - val_loss: 23.4553\n",
      "Epoch 161/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.5185 - val_loss: 23.3640\n",
      "Epoch 162/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 19.8196 - val_loss: 24.4054\n",
      "Epoch 163/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.8107 - val_loss: 25.7409\n",
      "Epoch 164/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 20.7063 - val_loss: 23.0162\n",
      "Epoch 165/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 20.3674 - val_loss: 24.1664\n",
      "Epoch 166/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.0979 - val_loss: 23.3789\n",
      "Epoch 167/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 20.1974 - val_loss: 23.3933\n",
      "Epoch 168/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 20.1861 - val_loss: 22.6840\n",
      "Epoch 169/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 20.1437 - val_loss: 25.0487\n",
      "Epoch 170/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 21.1425 - val_loss: 25.9935\n",
      "Epoch 171/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.6218 - val_loss: 25.3507\n",
      "Epoch 172/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 20.3414 - val_loss: 24.2950\n",
      "Epoch 173/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 20.6615 - val_loss: 23.2544\n",
      "Epoch 174/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 20.0947 - val_loss: 23.5135\n",
      "Epoch 175/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.9483 - val_loss: 22.9263\n",
      "Epoch 176/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.0789 - val_loss: 23.3623\n",
      "Epoch 177/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.8974 - val_loss: 23.0585\n",
      "Epoch 178/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 20.5719 - val_loss: 22.6228\n",
      "Epoch 179/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 19.9365 - val_loss: 23.0665\n",
      "Epoch 180/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 20.4290 - val_loss: 24.4326\n",
      "Epoch 181/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 20.2415 - val_loss: 22.5405\n",
      "Epoch 182/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 20.1655 - val_loss: 23.8326\n",
      "Epoch 183/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.7679 - val_loss: 23.8773\n",
      "Epoch 184/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 19.7420 - val_loss: 24.9077\n",
      "Epoch 185/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 20.4506 - val_loss: 24.8989\n",
      "Epoch 186/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 19.8658 - val_loss: 22.3921\n",
      "Epoch 187/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.5474 - val_loss: 26.7332\n",
      "Epoch 188/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 20.5538 - val_loss: 24.8404\n",
      "Epoch 189/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 20.2321 - val_loss: 24.7303\n",
      "Epoch 190/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.7038 - val_loss: 23.2342\n",
      "Epoch 191/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.0357 - val_loss: 25.3610\n",
      "Epoch 192/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 20.8804 - val_loss: 25.0418\n",
      "Epoch 193/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.8845 - val_loss: 23.5782\n",
      "Epoch 194/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 20.1848 - val_loss: 24.4257\n",
      "Epoch 195/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 19.8770 - val_loss: 26.7987\n",
      "Epoch 196/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.1210 - val_loss: 23.3572\n",
      "Epoch 197/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 20.2571 - val_loss: 22.3178\n",
      "Epoch 198/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 19.9157 - val_loss: 23.4968\n",
      "Epoch 199/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 20.8875 - val_loss: 27.7958\n",
      "Epoch 200/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 19.7611 - val_loss: 23.9550\n",
      "Epoch 201/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 20.2668 - val_loss: 24.9646\n",
      "Epoch 202/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.1677 - val_loss: 23.1093\n",
      "Epoch 203/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 20.4434 - val_loss: 24.8636\n",
      "Epoch 204/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 19.7500 - val_loss: 23.2223\n",
      "Epoch 205/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.2373 - val_loss: 24.8244\n",
      "Epoch 206/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.6195 - val_loss: 23.5775\n",
      "Epoch 207/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.4181 - val_loss: 23.3239\n",
      "Epoch 208/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 20.3363 - val_loss: 24.4206\n",
      "Epoch 209/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 19.9212 - val_loss: 25.1701\n",
      "Epoch 210/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.9691 - val_loss: 23.4985\n",
      "Epoch 211/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 19.8964 - val_loss: 23.1487\n",
      "Epoch 212/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.4369 - val_loss: 22.6663\n",
      "Epoch 213/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 20.0584 - val_loss: 25.0891\n",
      "Epoch 214/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 20.5155 - val_loss: 25.2719\n",
      "Epoch 215/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 20.2947 - val_loss: 25.9337\n",
      "Epoch 216/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 19.6359 - val_loss: 24.1945\n",
      "Epoch 217/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 19.6621 - val_loss: 23.8171\n",
      "Epoch 218/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 19.6522 - val_loss: 26.3197\n",
      "Epoch 219/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 20.1855 - val_loss: 23.2886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 220/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 20.6684 - val_loss: 25.1595\n",
      "Epoch 221/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 19.9245 - val_loss: 24.1720\n",
      "Epoch 222/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 20.0613 - val_loss: 24.7249\n",
      "Epoch 223/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 19.8678 - val_loss: 22.5423\n",
      "Epoch 224/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 20.4504 - val_loss: 23.1971\n",
      "Epoch 225/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 20.1321 - val_loss: 23.3365\n",
      "Epoch 226/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 19.8019 - val_loss: 25.2016\n",
      "Epoch 227/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 20.3141 - val_loss: 24.6940\n",
      "Epoch 228/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 19.4653 - val_loss: 22.8210\n",
      "Epoch 229/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 19.3751 - val_loss: 24.0826\n",
      "Epoch 230/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 19.5775 - val_loss: 25.1990\n",
      "Epoch 231/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 19.7810 - val_loss: 23.4809\n",
      "Epoch 232/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 20.1839 - val_loss: 23.7446\n",
      "Epoch 233/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 21.0046 - val_loss: 24.1651\n",
      "Epoch 234/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 19.9325 - val_loss: 25.7431\n",
      "Epoch 235/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 19.8276 - val_loss: 23.5664\n",
      "Epoch 236/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 19.5195 - val_loss: 22.5608\n",
      "Epoch 237/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.6495 - val_loss: 23.0013\n",
      "Epoch 238/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 19.7669 - val_loss: 23.7758\n",
      "Epoch 239/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 20.3593 - val_loss: 25.0220\n",
      "Epoch 240/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 19.6976 - val_loss: 22.4931\n",
      "Epoch 241/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 20.1741 - val_loss: 25.1423\n",
      "Epoch 242/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 19.2975 - val_loss: 24.5606\n",
      "Epoch 243/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 19.5713 - val_loss: 22.7576\n",
      "Epoch 244/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 19.8178 - val_loss: 23.5047\n",
      "Epoch 245/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 19.3959 - val_loss: 23.2225\n",
      "Epoch 246/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 19.4937 - val_loss: 27.1932\n",
      "Epoch 247/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 20.5361 - val_loss: 27.2354\n",
      "Epoch 248/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.8069 - val_loss: 23.5997\n",
      "Epoch 249/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 19.9332 - val_loss: 22.5452\n",
      "Epoch 250/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.1053 - val_loss: 23.5647\n",
      "Epoch 251/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 20.4579 - val_loss: 23.9781\n",
      "Epoch 252/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 19.2728 - val_loss: 23.5804\n",
      "Epoch 253/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.6421 - val_loss: 25.5465\n",
      "Epoch 254/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.5983 - val_loss: 23.7058\n",
      "Epoch 255/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 19.7162 - val_loss: 25.2271\n",
      "Epoch 256/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 19.0863 - val_loss: 22.6908\n",
      "Epoch 257/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 19.4067 - val_loss: 24.0554\n",
      "Epoch 258/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 20.0309 - val_loss: 23.6559\n",
      "Epoch 259/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 19.5648 - val_loss: 24.2251\n",
      "Epoch 260/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.9137 - val_loss: 25.5782\n",
      "Epoch 261/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.2613 - val_loss: 23.9483\n",
      "Epoch 262/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 20.1659 - val_loss: 22.2077\n",
      "Epoch 263/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 19.7345 - val_loss: 23.1882\n",
      "Epoch 264/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.9676 - val_loss: 24.0941\n",
      "Epoch 265/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 18.8884 - val_loss: 22.9999\n",
      "Epoch 266/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 20.2838 - val_loss: 23.0594\n",
      "Epoch 267/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 20.1152 - val_loss: 26.2942\n",
      "Epoch 268/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 20.0904 - val_loss: 23.6518\n",
      "Epoch 269/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.3216 - val_loss: 24.5254\n",
      "Epoch 270/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 19.3156 - val_loss: 23.2000\n",
      "Epoch 271/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 19.0260 - val_loss: 24.5376\n",
      "Epoch 272/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 19.0649 - val_loss: 27.3048\n",
      "Epoch 273/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 19.3415 - val_loss: 24.8330\n",
      "Epoch 274/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 19.5330 - val_loss: 24.8576\n",
      "Epoch 275/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.4722 - val_loss: 23.1331\n",
      "Epoch 276/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.5091 - val_loss: 23.2467\n",
      "Epoch 277/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.1820 - val_loss: 23.9572\n",
      "Epoch 278/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 20.5038 - val_loss: 23.6801\n",
      "Epoch 279/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 19.1891 - val_loss: 22.5034\n",
      "Epoch 280/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 19.6498 - val_loss: 24.1828\n",
      "Epoch 281/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.3216 - val_loss: 23.2114\n",
      "Epoch 282/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.5302 - val_loss: 22.5516\n",
      "Epoch 283/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 19.3375 - val_loss: 22.9011\n",
      "Epoch 284/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.1071 - val_loss: 24.6989\n",
      "Epoch 285/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 19.4762 - val_loss: 23.7970\n",
      "Epoch 286/2000\n",
      "1500/1500 [==============================] - 0s 52us/sample - loss: 19.0332 - val_loss: 22.6915\n",
      "Epoch 287/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 19.3902 - val_loss: 22.9588\n",
      "Epoch 288/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 19.6871 - val_loss: 26.3816\n",
      "Epoch 289/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 19.2997 - val_loss: 23.3842\n",
      "Epoch 290/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 18.9015 - val_loss: 23.9925\n",
      "Epoch 291/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 18.9339 - val_loss: 24.4884\n",
      "Epoch 292/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 20.2972 - val_loss: 23.6963\n",
      "Epoch 293/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 20.6463 - val_loss: 25.9684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 294/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 19.6743 - val_loss: 26.5979\n",
      "Epoch 295/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 20.1688 - val_loss: 23.6994\n",
      "Epoch 296/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 19.3787 - val_loss: 22.6967\n",
      "Epoch 297/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 19.4576 - val_loss: 25.8570\n",
      "Epoch 298/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 19.1072 - val_loss: 23.7166\n",
      "Epoch 299/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 19.5537 - val_loss: 23.3800\n",
      "Epoch 300/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 19.5614 - val_loss: 25.1355\n",
      "Epoch 301/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 19.4334 - val_loss: 23.1745\n",
      "Epoch 302/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 18.3313 - val_loss: 22.7015\n",
      "Epoch 303/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 19.2459 - val_loss: 23.2060\n",
      "Epoch 304/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 19.2927 - val_loss: 24.0472\n",
      "Epoch 305/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.2637 - val_loss: 24.1665\n",
      "Epoch 306/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 18.4215 - val_loss: 22.7562\n",
      "Epoch 307/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 19.7225 - val_loss: 24.4334\n",
      "Epoch 308/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 19.7812 - val_loss: 23.3815\n",
      "Epoch 309/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 19.2604 - val_loss: 25.2213\n",
      "Epoch 310/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 20.0703 - val_loss: 24.3702\n",
      "Epoch 311/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 19.3993 - val_loss: 23.4303\n",
      "Epoch 312/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 19.5654 - val_loss: 26.1779\n",
      "Epoch 313/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 19.1911 - val_loss: 22.5643\n",
      "Epoch 314/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.3326 - val_loss: 22.0919\n",
      "Epoch 315/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.3773 - val_loss: 23.6369\n",
      "Epoch 316/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 18.9272 - val_loss: 24.4797\n",
      "Epoch 317/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 18.6309 - val_loss: 23.8003\n",
      "Epoch 318/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 19.0170 - val_loss: 23.7131\n",
      "Epoch 319/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.0755 - val_loss: 23.4545\n",
      "Epoch 320/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 19.0899 - val_loss: 23.2028\n",
      "Epoch 321/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 19.1237 - val_loss: 25.3384\n",
      "Epoch 322/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 19.6009 - val_loss: 23.2100\n",
      "Epoch 323/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 19.0411 - val_loss: 24.2661\n",
      "Epoch 324/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 19.4239 - val_loss: 23.6695\n",
      "Epoch 325/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 19.4185 - val_loss: 24.5435\n",
      "Epoch 326/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 19.3916 - val_loss: 26.0506\n",
      "Epoch 327/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 19.6886 - val_loss: 24.6614\n",
      "Epoch 328/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.3560 - val_loss: 27.5050\n",
      "Epoch 329/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.2589 - val_loss: 23.6856\n",
      "Epoch 330/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 19.4560 - val_loss: 25.3458\n",
      "Epoch 331/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 20.1147 - val_loss: 22.9670\n",
      "Epoch 332/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 18.9922 - val_loss: 24.2927\n",
      "Epoch 333/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 19.0876 - val_loss: 22.6811\n",
      "Epoch 334/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 19.0394 - val_loss: 24.3241\n",
      "Epoch 335/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.1733 - val_loss: 23.5571\n",
      "Epoch 336/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.4234 - val_loss: 26.3737\n",
      "Epoch 337/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.8270 - val_loss: 23.2123\n",
      "Epoch 338/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 18.9642 - val_loss: 25.9271\n",
      "Epoch 339/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 18.8760 - val_loss: 25.3777\n",
      "Epoch 340/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 19.6959 - val_loss: 24.2395\n",
      "Epoch 341/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.8800 - val_loss: 24.7012\n",
      "Epoch 342/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.8775 - val_loss: 24.2585\n",
      "Epoch 343/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 18.6730 - val_loss: 24.3280\n",
      "Epoch 344/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 18.4892 - val_loss: 22.7161\n",
      "Epoch 345/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 18.7591 - val_loss: 25.6760\n",
      "Epoch 346/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 19.0099 - val_loss: 26.9508\n",
      "Epoch 347/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.1142 - val_loss: 26.7220\n",
      "Epoch 348/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.1726 - val_loss: 23.3064\n",
      "Epoch 349/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 18.7787 - val_loss: 23.9430\n",
      "Epoch 350/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.0736 - val_loss: 24.5864\n",
      "Epoch 351/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.8995 - val_loss: 24.1765\n",
      "Epoch 352/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.2548 - val_loss: 23.7054\n",
      "Epoch 353/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 18.9802 - val_loss: 26.2434\n",
      "Epoch 354/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 20.0633 - val_loss: 25.4222\n",
      "Epoch 355/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 19.2091 - val_loss: 24.2816\n",
      "Epoch 356/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 18.6147 - val_loss: 24.2063\n",
      "Epoch 357/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 19.6010 - val_loss: 23.4558\n",
      "Epoch 358/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.6567 - val_loss: 23.8699\n",
      "Epoch 359/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 18.4930 - val_loss: 23.3730\n",
      "Epoch 360/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.6286 - val_loss: 26.3945\n",
      "Epoch 361/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 18.8144 - val_loss: 25.0338\n",
      "Epoch 362/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 18.9378 - val_loss: 22.8266\n",
      "Epoch 363/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 19.0988 - val_loss: 28.0990\n",
      "Epoch 364/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 19.1753 - val_loss: 23.7154\n",
      "Epoch 365/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 18.9754 - val_loss: 26.8384\n",
      "Epoch 366/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 19.3368 - val_loss: 24.3269\n",
      "Epoch 367/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.8234 - val_loss: 23.4060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 368/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 18.2081 - val_loss: 23.5690\n",
      "Epoch 369/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.5095 - val_loss: 27.8642\n",
      "Epoch 370/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 19.6539 - val_loss: 25.1770\n",
      "Epoch 371/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.4400 - val_loss: 25.7531\n",
      "Epoch 372/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 19.3656 - val_loss: 23.3078\n",
      "Epoch 373/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.3961 - val_loss: 23.3474\n",
      "Epoch 374/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.4330 - val_loss: 22.8352\n",
      "Epoch 375/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.3597 - val_loss: 24.4308\n",
      "Epoch 376/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 18.8146 - val_loss: 23.2096\n",
      "Epoch 377/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 18.1842 - val_loss: 22.1087\n",
      "Epoch 378/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.3067 - val_loss: 23.9240\n",
      "Epoch 379/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.5136 - val_loss: 25.9241\n",
      "Epoch 380/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 18.3640 - val_loss: 23.7293\n",
      "Epoch 381/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 18.3845 - val_loss: 23.7711\n",
      "Epoch 382/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 18.2996 - val_loss: 23.0766\n",
      "Epoch 383/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 17.9450 - val_loss: 25.4361\n",
      "Epoch 384/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.3524 - val_loss: 23.5627\n",
      "Epoch 385/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 19.1261 - val_loss: 23.9873\n",
      "Epoch 386/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 18.4093 - val_loss: 22.9660\n",
      "Epoch 387/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 18.8217 - val_loss: 24.0105\n",
      "Epoch 388/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 18.2148 - val_loss: 22.7182\n",
      "Epoch 389/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 18.8688 - val_loss: 24.0743\n",
      "Epoch 390/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 18.0437 - val_loss: 25.2788\n",
      "Epoch 391/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.5460 - val_loss: 24.7755\n",
      "Epoch 392/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.3672 - val_loss: 23.4047\n",
      "Epoch 393/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 18.2588 - val_loss: 23.0425\n",
      "Epoch 394/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.1304 - val_loss: 22.7316\n",
      "Epoch 395/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 18.3372 - val_loss: 24.0722\n",
      "Epoch 396/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 18.2183 - val_loss: 23.1069\n",
      "Epoch 397/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.1842 - val_loss: 24.4884\n",
      "Epoch 398/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.8437 - val_loss: 24.5441\n",
      "Epoch 399/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 18.7864 - val_loss: 23.5853\n",
      "Epoch 400/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 18.7215 - val_loss: 27.6494\n",
      "Epoch 401/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 19.3167 - val_loss: 23.9260\n",
      "Epoch 402/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 18.5883 - val_loss: 23.3258\n",
      "Epoch 403/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 19.4359 - val_loss: 24.0597\n",
      "Epoch 404/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.0068 - val_loss: 23.4828\n",
      "Epoch 405/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 17.8715 - val_loss: 24.5681\n",
      "Epoch 406/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 18.2776 - val_loss: 23.6106\n",
      "Epoch 407/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 18.5625 - val_loss: 24.4908\n",
      "Epoch 408/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 19.5688 - val_loss: 24.5295\n",
      "Epoch 409/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 18.0545 - val_loss: 23.1662\n",
      "Epoch 410/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 17.9660 - val_loss: 23.9113\n",
      "Epoch 411/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.0945 - val_loss: 23.2039\n",
      "Epoch 412/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 17.8455 - val_loss: 23.7833\n",
      "Epoch 413/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.4064 - val_loss: 23.3934\n",
      "Epoch 414/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 18.0707 - val_loss: 23.7075\n",
      "Epoch 415/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 17.9281 - val_loss: 23.1522\n",
      "Epoch 416/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.9193 - val_loss: 23.9570\n",
      "Epoch 417/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 18.3786 - val_loss: 22.8422\n",
      "Epoch 418/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.1657 - val_loss: 24.1028\n",
      "Epoch 419/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 17.9189 - val_loss: 23.5678\n",
      "Epoch 420/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.6906 - val_loss: 24.3488\n",
      "Epoch 421/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 19.3952 - val_loss: 23.8928\n",
      "Epoch 422/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.7665 - val_loss: 22.4798\n",
      "Epoch 423/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.4971 - val_loss: 22.7818\n",
      "Epoch 424/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.0973 - val_loss: 23.0483\n",
      "Epoch 425/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 18.4187 - val_loss: 22.5340\n",
      "Epoch 426/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 18.4264 - val_loss: 22.9181\n",
      "Epoch 427/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 17.8125 - val_loss: 22.8157\n",
      "Epoch 428/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 18.3038 - val_loss: 22.7812\n",
      "Epoch 429/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 17.9408 - val_loss: 23.4948\n",
      "Epoch 430/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 18.0353 - val_loss: 23.2012\n",
      "Epoch 431/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.8589 - val_loss: 24.9438\n",
      "Epoch 432/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 19.8954 - val_loss: 25.4544\n",
      "Epoch 433/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.6332 - val_loss: 24.7245\n",
      "Epoch 434/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 19.2919 - val_loss: 24.0375\n",
      "Epoch 435/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.4606 - val_loss: 24.8385\n",
      "Epoch 436/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 18.1992 - val_loss: 23.6641\n",
      "Epoch 437/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 18.7154 - val_loss: 23.3816\n",
      "Epoch 438/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 18.6651 - val_loss: 24.7009\n",
      "Epoch 439/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.4251 - val_loss: 24.1250\n",
      "Epoch 440/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 18.1404 - val_loss: 22.4865\n",
      "Epoch 441/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 17.8213 - val_loss: 23.9304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 442/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.5437 - val_loss: 25.4814\n",
      "Epoch 443/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 17.7246 - val_loss: 23.1100\n",
      "Epoch 444/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 17.7160 - val_loss: 23.5482\n",
      "Epoch 445/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 18.0483 - val_loss: 24.0768\n",
      "Epoch 446/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 19.1006 - val_loss: 23.9752\n",
      "Epoch 447/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.3652 - val_loss: 26.2681\n",
      "Epoch 448/2000\n",
      "1500/1500 [==============================] - 0s 52us/sample - loss: 18.6254 - val_loss: 23.6771\n",
      "Epoch 449/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 18.1554 - val_loss: 25.9889\n",
      "Epoch 450/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 19.3875 - val_loss: 22.7950\n",
      "Epoch 451/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 17.6677 - val_loss: 23.8333\n",
      "Epoch 452/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 18.9298 - val_loss: 24.7661\n",
      "Epoch 453/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.6601 - val_loss: 27.6055\n",
      "Epoch 454/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 18.5183 - val_loss: 23.7427\n",
      "Epoch 455/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 18.5560 - val_loss: 23.9291\n",
      "Epoch 456/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 19.1669 - val_loss: 24.6914\n",
      "Epoch 457/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 17.8535 - val_loss: 23.3607\n",
      "Epoch 458/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 17.9875 - val_loss: 24.2805\n",
      "Epoch 459/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 17.5695 - val_loss: 24.8610\n",
      "Epoch 460/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 17.4257 - val_loss: 22.7090\n",
      "Epoch 461/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.3202 - val_loss: 23.4556\n",
      "Epoch 462/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 17.6588 - val_loss: 22.8910\n",
      "Epoch 463/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 17.9166 - val_loss: 25.9334\n",
      "Epoch 464/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 17.8265 - val_loss: 22.8062\n",
      "Epoch 465/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 17.8466 - val_loss: 23.3085\n",
      "Epoch 466/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.3599 - val_loss: 28.4872\n",
      "Epoch 467/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.6080 - val_loss: 25.6042\n",
      "Epoch 468/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 17.9158 - val_loss: 23.5628\n",
      "Epoch 469/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 17.6232 - val_loss: 23.2768\n",
      "Epoch 470/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 17.9449 - val_loss: 22.9904\n",
      "Epoch 471/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 18.0684 - val_loss: 23.6440\n",
      "Epoch 472/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.0301 - val_loss: 23.2013\n",
      "Epoch 473/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 17.3067 - val_loss: 24.0676\n",
      "Epoch 474/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 17.9993 - val_loss: 23.0014\n",
      "Epoch 475/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 19.4776 - val_loss: 23.0700\n",
      "Epoch 476/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 18.4183 - val_loss: 23.5489\n",
      "Epoch 477/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.3285 - val_loss: 23.3833\n",
      "Epoch 478/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 17.9466 - val_loss: 23.7048\n",
      "Epoch 479/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 18.2673 - val_loss: 23.2904\n",
      "Epoch 480/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 17.0737 - val_loss: 23.2409\n",
      "Epoch 481/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 17.4685 - val_loss: 23.6106\n",
      "Epoch 482/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 18.4124 - val_loss: 26.4322\n",
      "Epoch 483/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 18.4246 - val_loss: 25.3728\n",
      "Epoch 484/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 18.0784 - val_loss: 25.0834\n",
      "Epoch 485/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 18.0558 - val_loss: 23.3827\n",
      "Epoch 486/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 17.5938 - val_loss: 23.1804\n",
      "Epoch 487/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 17.5826 - val_loss: 24.1417\n",
      "Epoch 488/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 17.4321 - val_loss: 22.9185\n",
      "Epoch 489/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 17.4472 - val_loss: 23.2215\n",
      "Epoch 490/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 17.9323 - val_loss: 22.3950\n",
      "Epoch 491/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 17.9943 - val_loss: 24.7895\n",
      "Epoch 492/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.9548 - val_loss: 23.6777\n",
      "Epoch 493/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.4298 - val_loss: 23.0167\n",
      "Epoch 494/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 17.9000 - val_loss: 22.2220\n",
      "Epoch 495/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 18.6148 - val_loss: 25.4331\n",
      "Epoch 496/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 17.6327 - val_loss: 23.7167\n",
      "Epoch 497/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 17.1964 - val_loss: 22.6357\n",
      "Epoch 498/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 17.8108 - val_loss: 24.4561\n",
      "Epoch 499/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 17.6266 - val_loss: 22.9835\n",
      "Epoch 500/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 17.1855 - val_loss: 22.8144\n",
      "Epoch 501/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 17.2412 - val_loss: 23.8994\n",
      "Epoch 502/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 17.5391 - val_loss: 23.1041\n",
      "Epoch 503/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 17.5968 - val_loss: 22.5814\n",
      "Epoch 504/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 18.5112 - val_loss: 23.1509\n",
      "Epoch 505/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 17.7986 - val_loss: 23.2892\n",
      "Epoch 506/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 17.8662 - val_loss: 25.5293\n",
      "Epoch 507/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 17.6745 - val_loss: 22.9222\n",
      "Epoch 508/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.1452 - val_loss: 25.6714\n",
      "Epoch 509/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 17.0653 - val_loss: 23.2933\n",
      "Epoch 510/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 17.7795 - val_loss: 24.1255\n",
      "Epoch 511/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 18.3126 - val_loss: 24.6688\n",
      "Epoch 512/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 17.8710 - val_loss: 26.4592\n",
      "Epoch 513/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 17.9047 - val_loss: 23.8294\n",
      "Epoch 514/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.3348 - val_loss: 24.1441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███████████████████████████████▏                                                   | 6/16 [03:18<05:37, 33.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'batc_normalization': False, 'batch_size': 32, 'dropout': 0, 'epochs': 2000, 'first_neuron': 320, 'hidden_layers': 2, 'hidden_neuron': 50, 'kernel_initializer': 'normal', 'last_activation': 'linear', 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "Train on 1500 samples, validate on 375 samples\n",
      "Epoch 1/2000\n",
      "1500/1500 [==============================] - 0s 259us/sample - loss: 2179.4921 - val_loss: 2150.3790\n",
      "Epoch 2/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 1844.4372 - val_loss: 1109.1203\n",
      "Epoch 3/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 634.6219 - val_loss: 462.2561\n",
      "Epoch 4/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 398.6764 - val_loss: 301.6526\n",
      "Epoch 5/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 251.3683 - val_loss: 180.7824\n",
      "Epoch 6/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 151.8331 - val_loss: 113.8627\n",
      "Epoch 7/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 100.1329 - val_loss: 80.6285\n",
      "Epoch 8/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 71.5325 - val_loss: 61.0445\n",
      "Epoch 9/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 51.8050 - val_loss: 49.0485\n",
      "Epoch 10/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 43.9208 - val_loss: 44.3111\n",
      "Epoch 11/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 38.6202 - val_loss: 35.4800\n",
      "Epoch 12/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 34.9695 - val_loss: 36.1105\n",
      "Epoch 13/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 33.3625 - val_loss: 31.2284\n",
      "Epoch 14/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 31.4257 - val_loss: 35.8955\n",
      "Epoch 15/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 29.5352 - val_loss: 29.7927\n",
      "Epoch 16/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 26.6299 - val_loss: 24.5854\n",
      "Epoch 17/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 26.3255 - val_loss: 29.1450\n",
      "Epoch 18/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 24.9895 - val_loss: 26.1057\n",
      "Epoch 19/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 25.9630 - val_loss: 28.5931\n",
      "Epoch 20/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 27.2773 - val_loss: 28.4511\n",
      "Epoch 21/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 25.7152 - val_loss: 29.0479\n",
      "Epoch 22/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 25.0924 - val_loss: 25.9895\n",
      "Epoch 23/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 24.2573 - val_loss: 27.2244\n",
      "Epoch 24/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 28.6843 - val_loss: 25.5698\n",
      "Epoch 25/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 24.0481 - val_loss: 24.4471\n",
      "Epoch 26/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 24.8596 - val_loss: 24.8814\n",
      "Epoch 27/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 24.2378 - val_loss: 32.3277\n",
      "Epoch 28/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 24.9643 - val_loss: 26.2001\n",
      "Epoch 29/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 23.9044 - val_loss: 23.9357\n",
      "Epoch 30/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 24.3499 - val_loss: 26.5913\n",
      "Epoch 31/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 23.8160 - val_loss: 25.7728\n",
      "Epoch 32/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 23.6036 - val_loss: 27.0698\n",
      "Epoch 33/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 24.6034 - val_loss: 26.6396\n",
      "Epoch 34/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 25.0862 - val_loss: 27.1950\n",
      "Epoch 35/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 25.5258 - val_loss: 27.0717\n",
      "Epoch 36/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 24.1250 - val_loss: 23.6081\n",
      "Epoch 37/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 23.2904 - val_loss: 28.6540\n",
      "Epoch 38/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.7291 - val_loss: 25.4545\n",
      "Epoch 39/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 26.1862 - val_loss: 26.4703\n",
      "Epoch 40/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 23.2877 - val_loss: 25.2820\n",
      "Epoch 41/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 22.8462 - val_loss: 25.8656\n",
      "Epoch 42/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 22.2484 - val_loss: 27.4924\n",
      "Epoch 43/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 25.0145 - val_loss: 23.3480\n",
      "Epoch 44/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 24.0865 - val_loss: 24.9789\n",
      "Epoch 45/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 23.0722 - val_loss: 24.8773\n",
      "Epoch 46/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 23.7452 - val_loss: 27.2055\n",
      "Epoch 47/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 23.6653 - val_loss: 27.7459\n",
      "Epoch 48/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 22.5230 - val_loss: 24.8725\n",
      "Epoch 49/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.5564 - val_loss: 26.2119\n",
      "Epoch 50/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 22.6222 - val_loss: 27.4064\n",
      "Epoch 51/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 22.6982 - val_loss: 25.1334\n",
      "Epoch 52/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 23.0034 - val_loss: 25.0283\n",
      "Epoch 53/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 24.4281 - val_loss: 27.4039\n",
      "Epoch 54/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 25.5188 - val_loss: 28.3363\n",
      "Epoch 55/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 24.6847 - val_loss: 28.0504\n",
      "Epoch 56/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 23.1745 - val_loss: 23.2218\n",
      "Epoch 57/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 23.0011 - val_loss: 23.8551\n",
      "Epoch 58/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.5937 - val_loss: 28.2277\n",
      "Epoch 59/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 23.6095 - val_loss: 23.7558\n",
      "Epoch 60/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.9170 - val_loss: 24.0373\n",
      "Epoch 61/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.8653 - val_loss: 24.9621\n",
      "Epoch 62/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 23.2813 - val_loss: 22.6955\n",
      "Epoch 63/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 23.1013 - val_loss: 25.0293\n",
      "Epoch 64/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 24.1968 - val_loss: 24.7178\n",
      "Epoch 65/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 22.9922 - val_loss: 26.3315\n",
      "Epoch 66/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 22.4556 - val_loss: 25.7437\n",
      "Epoch 67/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 21.5876 - val_loss: 22.6706\n",
      "Epoch 68/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.8501 - val_loss: 25.6645\n",
      "Epoch 69/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 23.5386 - val_loss: 26.5721\n",
      "Epoch 70/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 22.7183 - val_loss: 24.1312\n",
      "Epoch 71/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 24.7894 - val_loss: 30.3011\n",
      "Epoch 72/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 24.7944 - val_loss: 26.4027\n",
      "Epoch 73/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 22.4011 - val_loss: 24.2166\n",
      "Epoch 74/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 21.7378 - val_loss: 26.1583\n",
      "Epoch 75/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 22.1650 - val_loss: 22.4207\n",
      "Epoch 76/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 22.2202 - val_loss: 24.2364\n",
      "Epoch 77/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.7634 - val_loss: 23.3892\n",
      "Epoch 78/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.5123 - val_loss: 24.7406\n",
      "Epoch 79/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 24.1644 - val_loss: 24.5522\n",
      "Epoch 80/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 21.9051 - val_loss: 24.0802\n",
      "Epoch 81/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 21.3109 - val_loss: 29.3333\n",
      "Epoch 82/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 22.4677 - val_loss: 26.4966\n",
      "Epoch 83/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 21.6566 - val_loss: 25.5571\n",
      "Epoch 84/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 22.7259 - val_loss: 23.8258\n",
      "Epoch 85/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 23.2860 - val_loss: 24.8619\n",
      "Epoch 86/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 21.6594 - val_loss: 24.5383\n",
      "Epoch 87/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 22.6487 - val_loss: 24.4768\n",
      "Epoch 88/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 22.6339 - val_loss: 29.2137\n",
      "Epoch 89/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 22.2598 - val_loss: 24.7739\n",
      "Epoch 90/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.4879 - val_loss: 24.8364\n",
      "Epoch 91/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 21.5075 - val_loss: 24.7987\n",
      "Epoch 92/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 23.1679 - val_loss: 25.4230\n",
      "Epoch 93/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.6211 - val_loss: 24.1819\n",
      "Epoch 94/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.5094 - val_loss: 24.7344\n",
      "Epoch 95/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 23.3604 - val_loss: 25.1135\n",
      "Epoch 96/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 22.3428 - val_loss: 23.4903\n",
      "Epoch 97/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 24.8979 - val_loss: 26.1329\n",
      "Epoch 98/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.9197 - val_loss: 24.6924\n",
      "Epoch 99/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.8922 - val_loss: 26.2267\n",
      "Epoch 100/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.4292 - val_loss: 26.6183\n",
      "Epoch 101/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 24.0386 - val_loss: 25.2231\n",
      "Epoch 102/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 21.6424 - val_loss: 24.6906\n",
      "Epoch 103/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 22.7865 - val_loss: 23.1609\n",
      "Epoch 104/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 22.5842 - val_loss: 24.7545\n",
      "Epoch 105/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.1204 - val_loss: 23.6376\n",
      "Epoch 106/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 22.1320 - val_loss: 24.4353\n",
      "Epoch 107/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 22.6583 - val_loss: 27.5799\n",
      "Epoch 108/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 23.6354 - val_loss: 30.2349\n",
      "Epoch 109/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 23.8632 - val_loss: 26.3189\n",
      "Epoch 110/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.7926 - val_loss: 26.0929\n",
      "Epoch 111/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 21.8736 - val_loss: 27.3140\n",
      "Epoch 112/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.5920 - val_loss: 24.4477\n",
      "Epoch 113/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.4415 - val_loss: 26.4021\n",
      "Epoch 114/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 21.3929 - val_loss: 23.4982\n",
      "Epoch 115/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.2529 - val_loss: 23.6694\n",
      "Epoch 116/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 20.9786 - val_loss: 26.0016\n",
      "Epoch 117/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.2713 - val_loss: 24.1822\n",
      "Epoch 118/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 22.1168 - val_loss: 27.5495\n",
      "Epoch 119/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 21.3621 - val_loss: 26.0337\n",
      "Epoch 120/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.5717 - val_loss: 24.8956\n",
      "Epoch 121/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 23.1443 - val_loss: 25.6017\n",
      "Epoch 122/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.4832 - val_loss: 25.6703\n",
      "Epoch 123/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 22.1415 - val_loss: 26.0493\n",
      "Epoch 124/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 21.0115 - val_loss: 24.4687\n",
      "Epoch 125/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.1391 - val_loss: 24.5022\n",
      "Epoch 126/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.3863 - val_loss: 26.8201\n",
      "Epoch 127/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 23.3058 - val_loss: 24.5445\n",
      "Epoch 128/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.5309 - val_loss: 25.0140\n",
      "Epoch 129/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 20.8962 - val_loss: 28.4048\n",
      "Epoch 130/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.8111 - val_loss: 23.9712\n",
      "Epoch 131/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 24.0943 - val_loss: 27.9605\n",
      "Epoch 132/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 22.6211 - val_loss: 23.2420\n",
      "Epoch 133/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.5446 - val_loss: 25.8316\n",
      "Epoch 134/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 19.9292 - val_loss: 25.8912\n",
      "Epoch 135/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 21.8702 - val_loss: 27.5889\n",
      "Epoch 136/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.2241 - val_loss: 27.1666\n",
      "Epoch 137/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 21.4338 - val_loss: 26.6392\n",
      "Epoch 138/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 21.8647 - val_loss: 24.6913\n",
      "Epoch 139/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 20.7442 - val_loss: 23.5871\n",
      "Epoch 140/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.1544 - val_loss: 29.1580\n",
      "Epoch 141/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 21.1646 - val_loss: 23.5426\n",
      "Epoch 142/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 20.4171 - val_loss: 24.3241\n",
      "Epoch 143/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.1790 - val_loss: 23.0104\n",
      "Epoch 144/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 20.8331 - val_loss: 24.7730\n",
      "Epoch 145/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 21.2605 - val_loss: 23.7676\n",
      "Epoch 146/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 48us/sample - loss: 21.9235 - val_loss: 24.5879\n",
      "Epoch 147/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 20.7300 - val_loss: 23.1978\n",
      "Epoch 148/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.1723 - val_loss: 27.0268\n",
      "Epoch 149/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 21.8268 - val_loss: 26.1104\n",
      "Epoch 150/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 20.6548 - val_loss: 23.7712\n",
      "Epoch 151/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.1786 - val_loss: 25.3326\n",
      "Epoch 152/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.1119 - val_loss: 25.1626\n",
      "Epoch 153/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 22.1240 - val_loss: 23.3810\n",
      "Epoch 154/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.4282 - val_loss: 25.9004\n",
      "Epoch 155/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.7205 - val_loss: 25.8051\n",
      "Epoch 156/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.8559 - val_loss: 22.9943\n",
      "Epoch 157/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.9663 - val_loss: 27.1716\n",
      "Epoch 158/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 22.9033 - val_loss: 24.8722\n",
      "Epoch 159/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 19.8000 - val_loss: 24.8904\n",
      "Epoch 160/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.4874 - val_loss: 24.5377\n",
      "Epoch 161/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.1196 - val_loss: 25.8174\n",
      "Epoch 162/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.6221 - val_loss: 24.1278\n",
      "Epoch 163/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 21.9939 - val_loss: 25.9029\n",
      "Epoch 164/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.8430 - val_loss: 22.8006\n",
      "Epoch 165/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.6175 - val_loss: 25.8758\n",
      "Epoch 166/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 20.6817 - val_loss: 28.1484\n",
      "Epoch 167/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.3855 - val_loss: 22.2887\n",
      "Epoch 168/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.9060 - val_loss: 25.0542\n",
      "Epoch 169/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 20.3658 - val_loss: 22.5061\n",
      "Epoch 170/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.0461 - val_loss: 24.9596\n",
      "Epoch 171/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.0540 - val_loss: 24.9290\n",
      "Epoch 172/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.1937 - val_loss: 23.8697\n",
      "Epoch 173/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.0046 - val_loss: 23.9459\n",
      "Epoch 174/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.5318 - val_loss: 23.5217\n",
      "Epoch 175/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 20.5440 - val_loss: 24.1924\n",
      "Epoch 176/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.8236 - val_loss: 24.4382\n",
      "Epoch 177/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.3840 - val_loss: 24.7618\n",
      "Epoch 178/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 20.8489 - val_loss: 25.0693\n",
      "Epoch 179/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 21.2824 - val_loss: 25.5359\n",
      "Epoch 180/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.3459 - val_loss: 26.9144\n",
      "Epoch 181/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 20.8294 - val_loss: 25.6018\n",
      "Epoch 182/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 21.6638 - val_loss: 25.7961\n",
      "Epoch 183/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 20.6895 - val_loss: 23.1239\n",
      "Epoch 184/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.7367 - val_loss: 24.4130\n",
      "Epoch 185/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.2023 - val_loss: 23.8134\n",
      "Epoch 186/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 19.4429 - val_loss: 25.6112\n",
      "Epoch 187/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 19.6232 - val_loss: 27.4003\n",
      "Epoch 188/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.3155 - val_loss: 25.5268\n",
      "Epoch 189/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 19.3425 - val_loss: 25.1831\n",
      "Epoch 190/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.9171 - val_loss: 25.2337\n",
      "Epoch 191/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 19.3615 - val_loss: 24.7414\n",
      "Epoch 192/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.6860 - val_loss: 31.4718\n",
      "Epoch 193/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.4645 - val_loss: 24.5900\n",
      "Epoch 194/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 20.0791 - val_loss: 23.6632\n",
      "Epoch 195/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.1495 - val_loss: 24.2038\n",
      "Epoch 196/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.3956 - val_loss: 25.0570\n",
      "Epoch 197/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 21.7129 - val_loss: 23.9631\n",
      "Epoch 198/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.7561 - val_loss: 22.7321\n",
      "Epoch 199/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 20.5746 - val_loss: 22.8478\n",
      "Epoch 200/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 20.3303 - val_loss: 26.3891\n",
      "Epoch 201/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 19.7928 - val_loss: 22.9859\n",
      "Epoch 202/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 20.2192 - val_loss: 24.7919\n",
      "Epoch 203/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.9921 - val_loss: 24.2141\n",
      "Epoch 204/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.9123 - val_loss: 31.7226\n",
      "Epoch 205/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 22.4086 - val_loss: 24.2871\n",
      "Epoch 206/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.9931 - val_loss: 28.2459\n",
      "Epoch 207/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 18.9843 - val_loss: 24.4065\n",
      "Epoch 208/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.2098 - val_loss: 23.2720\n",
      "Epoch 209/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 20.7987 - val_loss: 27.2467\n",
      "Epoch 210/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 19.6570 - val_loss: 25.2927\n",
      "Epoch 211/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 19.8186 - val_loss: 24.1210\n",
      "Epoch 212/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 20.4993 - val_loss: 24.7654\n",
      "Epoch 213/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 19.9677 - val_loss: 22.7811\n",
      "Epoch 214/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 19.9050 - val_loss: 23.8833\n",
      "Epoch 215/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 19.9380 - val_loss: 23.4292\n",
      "Epoch 216/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.2904 - val_loss: 25.2382\n",
      "Epoch 217/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.5192 - val_loss: 30.5343\n",
      "Epoch 218/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 19.9952 - val_loss: 23.4065\n",
      "Epoch 219/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.2543 - val_loss: 24.2682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 220/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.8273 - val_loss: 27.1305\n",
      "Epoch 221/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 18.9591 - val_loss: 22.4556\n",
      "Epoch 222/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 19.9410 - val_loss: 31.8054\n",
      "Epoch 223/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.8841 - val_loss: 23.9155\n",
      "Epoch 224/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 19.2608 - val_loss: 22.4665\n",
      "Epoch 225/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.4911 - val_loss: 25.6283\n",
      "Epoch 226/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 19.9717 - val_loss: 24.4431\n",
      "Epoch 227/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.2195 - val_loss: 24.6758\n",
      "Epoch 228/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 18.6180 - val_loss: 22.3850\n",
      "Epoch 229/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 22.4069 - val_loss: 24.1856\n",
      "Epoch 230/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 20.9069 - val_loss: 32.4915\n",
      "Epoch 231/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 20.6969 - val_loss: 24.4714\n",
      "Epoch 232/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 20.8677 - val_loss: 25.4046\n",
      "Epoch 233/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.5459 - val_loss: 25.1242\n",
      "Epoch 234/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 20.2481 - val_loss: 25.6881\n",
      "Epoch 235/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.3508 - val_loss: 24.0387\n",
      "Epoch 236/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 19.8072 - val_loss: 27.0490\n",
      "Epoch 237/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 19.5585 - val_loss: 23.1083\n",
      "Epoch 238/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.6029 - val_loss: 24.8705\n",
      "Epoch 239/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 20.1846 - val_loss: 23.3989\n",
      "Epoch 240/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.0787 - val_loss: 24.1557\n",
      "Epoch 241/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 20.2466 - val_loss: 23.3327\n",
      "Epoch 242/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.5786 - val_loss: 23.3234\n",
      "Epoch 243/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 18.9951 - val_loss: 23.1299\n",
      "Epoch 244/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.9338 - val_loss: 27.3247\n",
      "Epoch 245/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 20.2540 - val_loss: 22.4950\n",
      "Epoch 246/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.1713 - val_loss: 24.0716\n",
      "Epoch 247/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.6341 - val_loss: 22.3658\n",
      "Epoch 248/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 19.1769 - val_loss: 22.8160\n",
      "Epoch 249/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.4387 - val_loss: 27.9134\n",
      "Epoch 250/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.1231 - val_loss: 24.7150\n",
      "Epoch 251/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.4001 - val_loss: 24.4825\n",
      "Epoch 252/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 20.2329 - val_loss: 23.1310\n",
      "Epoch 253/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 20.1403 - val_loss: 23.0492\n",
      "Epoch 254/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.3271 - val_loss: 25.1093\n",
      "Epoch 255/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.8755 - val_loss: 23.2226\n",
      "Epoch 256/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 17.9422 - val_loss: 24.9550\n",
      "Epoch 257/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.6694 - val_loss: 24.1595\n",
      "Epoch 258/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.3828 - val_loss: 24.4243\n",
      "Epoch 259/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 19.9249 - val_loss: 23.5327\n",
      "Epoch 260/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 19.9889 - val_loss: 25.1959\n",
      "Epoch 261/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.7184 - val_loss: 26.0655\n",
      "Epoch 262/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.6489 - val_loss: 23.1668\n",
      "Epoch 263/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.5975 - val_loss: 27.0651\n",
      "Epoch 264/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 21.4199 - val_loss: 22.4214\n",
      "Epoch 265/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.6502 - val_loss: 25.0390\n",
      "Epoch 266/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 19.4927 - val_loss: 22.5194\n",
      "Epoch 267/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.2795 - val_loss: 25.3769\n",
      "Epoch 268/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.2882 - val_loss: 22.7872\n",
      "Epoch 269/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.7755 - val_loss: 22.2678\n",
      "Epoch 270/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.6171 - val_loss: 23.4034\n",
      "Epoch 271/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 19.3459 - val_loss: 25.3587\n",
      "Epoch 272/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.8569 - val_loss: 31.2679\n",
      "Epoch 273/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.1597 - val_loss: 24.0567\n",
      "Epoch 274/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 18.1982 - val_loss: 24.0019\n",
      "Epoch 275/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.9711 - val_loss: 28.7576\n",
      "Epoch 276/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 19.6062 - val_loss: 22.0913\n",
      "Epoch 277/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.9442 - val_loss: 22.5642\n",
      "Epoch 278/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.7301 - val_loss: 23.4579\n",
      "Epoch 279/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.8148 - val_loss: 26.5264\n",
      "Epoch 280/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 23.4987 - val_loss: 23.2902\n",
      "Epoch 281/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 18.9647 - val_loss: 26.7267\n",
      "Epoch 282/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 18.4027 - val_loss: 22.2888\n",
      "Epoch 283/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.6495 - val_loss: 24.4555\n",
      "Epoch 284/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.7099 - val_loss: 29.8118\n",
      "Epoch 285/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 21.9659 - val_loss: 23.7451\n",
      "Epoch 286/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.9384 - val_loss: 22.6226\n",
      "Epoch 287/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 18.3002 - val_loss: 22.5203\n",
      "Epoch 288/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.5267 - val_loss: 23.9191\n",
      "Epoch 289/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.2368 - val_loss: 24.3720\n",
      "Epoch 290/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.3519 - val_loss: 23.3835\n",
      "Epoch 291/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.9148 - val_loss: 23.8764\n",
      "Epoch 292/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 18.2001 - val_loss: 25.4525\n",
      "Epoch 293/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.4567 - val_loss: 22.5982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 294/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.0839 - val_loss: 23.9345\n",
      "Epoch 295/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.7309 - val_loss: 25.9061\n",
      "Epoch 296/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.3394 - val_loss: 23.6966\n",
      "Epoch 297/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 19.5024 - val_loss: 24.0429\n",
      "Epoch 298/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.6112 - val_loss: 25.6287\n",
      "Epoch 299/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.8616 - val_loss: 23.3318\n",
      "Epoch 300/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 18.2726 - val_loss: 23.9001\n",
      "Epoch 301/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 18.4884 - val_loss: 22.4339\n",
      "Epoch 302/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 18.5793 - val_loss: 24.5663\n",
      "Epoch 303/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.0948 - val_loss: 22.3097\n",
      "Epoch 304/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.2510 - val_loss: 23.8712\n",
      "Epoch 305/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 18.9033 - val_loss: 25.3141\n",
      "Epoch 306/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.6281 - val_loss: 23.2538\n",
      "Epoch 307/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.7259 - val_loss: 25.0647\n",
      "Epoch 308/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.4105 - val_loss: 25.1075\n",
      "Epoch 309/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 20.9078 - val_loss: 30.4841\n",
      "Epoch 310/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.6201 - val_loss: 24.7606\n",
      "Epoch 311/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 18.6748 - val_loss: 25.1181\n",
      "Epoch 312/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.4007 - val_loss: 23.9101\n",
      "Epoch 313/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.4240 - val_loss: 27.4699\n",
      "Epoch 314/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 19.3616 - val_loss: 23.6007\n",
      "Epoch 315/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.9786 - val_loss: 22.5077\n",
      "Epoch 316/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.1579 - val_loss: 22.9290\n",
      "Epoch 317/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.7346 - val_loss: 24.5384\n",
      "Epoch 318/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 20.1301 - val_loss: 24.4709\n",
      "Epoch 319/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.1850 - val_loss: 23.6639\n",
      "Epoch 320/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 18.9092 - val_loss: 22.8474\n",
      "Epoch 321/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.2148 - val_loss: 22.8702\n",
      "Epoch 322/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.9667 - val_loss: 22.9510\n",
      "Epoch 323/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 18.9016 - val_loss: 24.7536\n",
      "Epoch 324/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.7327 - val_loss: 22.3455\n",
      "Epoch 325/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.3750 - val_loss: 24.6123\n",
      "Epoch 326/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 18.9593 - val_loss: 24.7766\n",
      "Epoch 327/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.1056 - val_loss: 22.6406\n",
      "Epoch 328/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 17.7580 - val_loss: 25.9167\n",
      "Epoch 329/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 17.5822 - val_loss: 26.2430\n",
      "Epoch 330/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 17.7460 - val_loss: 22.1537\n",
      "Epoch 331/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.1953 - val_loss: 26.4895\n",
      "Epoch 332/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.5137 - val_loss: 23.2829\n",
      "Epoch 333/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.6234 - val_loss: 23.0528\n",
      "Epoch 334/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.1900 - val_loss: 24.2663\n",
      "Epoch 335/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.5711 - val_loss: 22.4218\n",
      "Epoch 336/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.0477 - val_loss: 23.0378\n",
      "Epoch 337/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.6506 - val_loss: 21.8722\n",
      "Epoch 338/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.2988 - val_loss: 26.5185\n",
      "Epoch 339/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 17.3313 - val_loss: 23.2675\n",
      "Epoch 340/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.0137 - val_loss: 26.0959\n",
      "Epoch 341/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 17.7890 - val_loss: 22.8946\n",
      "Epoch 342/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.0021 - val_loss: 22.0405\n",
      "Epoch 343/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 17.8965 - val_loss: 23.8034\n",
      "Epoch 344/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.1511 - val_loss: 23.7624\n",
      "Epoch 345/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.8143 - val_loss: 24.7567\n",
      "Epoch 346/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.2692 - val_loss: 22.9236\n",
      "Epoch 347/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 18.8346 - val_loss: 22.6574\n",
      "Epoch 348/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.6468 - val_loss: 24.7588\n",
      "Epoch 349/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 19.6755 - val_loss: 25.0836\n",
      "Epoch 350/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 18.3094 - val_loss: 21.8748\n",
      "Epoch 351/2000\n",
      "1500/1500 [==============================] - 0s 56us/sample - loss: 18.7156 - val_loss: 22.3599\n",
      "Epoch 352/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 17.1621 - val_loss: 22.7158\n",
      "Epoch 353/2000\n",
      "1500/1500 [==============================] - 0s 67us/sample - loss: 17.5310 - val_loss: 24.7179\n",
      "Epoch 354/2000\n",
      "1500/1500 [==============================] - 0s 59us/sample - loss: 18.2914 - val_loss: 23.7933\n",
      "Epoch 355/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 18.6163 - val_loss: 23.5125\n",
      "Epoch 356/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.7565 - val_loss: 25.6084\n",
      "Epoch 357/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.6855 - val_loss: 25.2916\n",
      "Epoch 358/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.9433 - val_loss: 21.2568\n",
      "Epoch 359/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.5273 - val_loss: 25.2989\n",
      "Epoch 360/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.3133 - val_loss: 24.1096\n",
      "Epoch 361/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.3921 - val_loss: 23.6539\n",
      "Epoch 362/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 17.0837 - val_loss: 24.2662\n",
      "Epoch 363/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.6680 - val_loss: 22.7791\n",
      "Epoch 364/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.4925 - val_loss: 24.2279\n",
      "Epoch 365/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.7042 - val_loss: 22.9581\n",
      "Epoch 366/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.2421 - val_loss: 27.2670\n",
      "Epoch 367/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.4151 - val_loss: 23.0406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 368/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 17.1375 - val_loss: 23.1945\n",
      "Epoch 369/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.8777 - val_loss: 22.2790\n",
      "Epoch 370/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 18.3899 - val_loss: 22.3298\n",
      "Epoch 371/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.5943 - val_loss: 24.3937\n",
      "Epoch 372/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.4964 - val_loss: 26.2876\n",
      "Epoch 373/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 17.4251 - val_loss: 22.8608\n",
      "Epoch 374/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 16.8333 - val_loss: 28.6572\n",
      "Epoch 375/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 18.9047 - val_loss: 23.7294\n",
      "Epoch 376/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 17.2354 - val_loss: 25.1417\n",
      "Epoch 377/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 17.5261 - val_loss: 25.7071\n",
      "Epoch 378/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 18.2482 - val_loss: 23.7785\n",
      "Epoch 379/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 17.8901 - val_loss: 23.7125\n",
      "Epoch 380/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.0694 - val_loss: 24.3627\n",
      "Epoch 381/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 18.5445 - val_loss: 24.0753\n",
      "Epoch 382/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 18.5630 - val_loss: 23.1568\n",
      "Epoch 383/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 18.6569 - val_loss: 22.2904\n",
      "Epoch 384/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.8223 - val_loss: 30.1897\n",
      "Epoch 385/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 17.4675 - val_loss: 23.0476\n",
      "Epoch 386/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.4064 - val_loss: 22.4377\n",
      "Epoch 387/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 16.7312 - val_loss: 23.9722\n",
      "Epoch 388/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.8106 - val_loss: 22.5003\n",
      "Epoch 389/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 16.9971 - val_loss: 22.8166\n",
      "Epoch 390/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 19.4174 - val_loss: 25.4290\n",
      "Epoch 391/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.7189 - val_loss: 23.6505\n",
      "Epoch 392/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 18.7223 - val_loss: 26.4613\n",
      "Epoch 393/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 18.7967 - val_loss: 22.7399\n",
      "Epoch 394/2000\n",
      "1500/1500 [==============================] - 0s 44us/sample - loss: 17.4285 - val_loss: 24.3496\n",
      "Epoch 395/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.3770 - val_loss: 24.0277\n",
      "Epoch 396/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.5782 - val_loss: 22.3435\n",
      "Epoch 397/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 21.2680 - val_loss: 25.3769\n",
      "Epoch 398/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 18.7834 - val_loss: 26.9066\n",
      "Epoch 399/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.3768 - val_loss: 25.9021\n",
      "Epoch 400/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.0202 - val_loss: 22.1973\n",
      "Epoch 401/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.7119 - val_loss: 23.7257\n",
      "Epoch 402/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 18.4122 - val_loss: 23.8278\n",
      "Epoch 403/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.4358 - val_loss: 24.4713\n",
      "Epoch 404/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.9418 - val_loss: 26.6517\n",
      "Epoch 405/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.8501 - val_loss: 29.0790\n",
      "Epoch 406/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.0604 - val_loss: 27.1190\n",
      "Epoch 407/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.3964 - val_loss: 22.3406\n",
      "Epoch 408/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 16.6991 - val_loss: 28.8930\n",
      "Epoch 409/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.9744 - val_loss: 24.8283\n",
      "Epoch 410/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 18.0066 - val_loss: 24.1861\n",
      "Epoch 411/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 17.9680 - val_loss: 23.3420\n",
      "Epoch 412/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.5213 - val_loss: 24.4802\n",
      "Epoch 413/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 17.8067 - val_loss: 22.8805\n",
      "Epoch 414/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 16.9016 - val_loss: 23.7176\n",
      "Epoch 415/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.7499 - val_loss: 27.2262\n",
      "Epoch 416/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.8752 - val_loss: 25.5794\n",
      "Epoch 417/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.2448 - val_loss: 27.3783\n",
      "Epoch 418/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 18.4600 - val_loss: 23.0846\n",
      "Epoch 419/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 17.5158 - val_loss: 25.5673\n",
      "Epoch 420/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.2414 - val_loss: 23.3334\n",
      "Epoch 421/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 17.0990 - val_loss: 23.6029\n",
      "Epoch 422/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.9169 - val_loss: 23.9019\n",
      "Epoch 423/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 17.4018 - val_loss: 22.8108\n",
      "Epoch 424/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 16.6772 - val_loss: 21.6903\n",
      "Epoch 425/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.3160 - val_loss: 22.5788\n",
      "Epoch 426/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 17.6833 - val_loss: 23.7598\n",
      "Epoch 427/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 19.9873 - val_loss: 26.9045\n",
      "Epoch 428/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.8610 - val_loss: 27.1822\n",
      "Epoch 429/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.1231 - val_loss: 25.4892\n",
      "Epoch 430/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 16.3434 - val_loss: 22.2984\n",
      "Epoch 431/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 17.5580 - val_loss: 25.3863\n",
      "Epoch 432/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 16.7517 - val_loss: 22.6780\n",
      "Epoch 433/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 16.1379 - val_loss: 22.4198\n",
      "Epoch 434/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.7713 - val_loss: 26.9999\n",
      "Epoch 435/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.1185 - val_loss: 25.3290\n",
      "Epoch 436/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.1498 - val_loss: 25.3810\n",
      "Epoch 437/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 17.8847 - val_loss: 22.3407\n",
      "Epoch 438/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 18.0144 - val_loss: 22.9201\n",
      "Epoch 439/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 17.5714 - val_loss: 26.4570\n",
      "Epoch 440/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 18.6689 - val_loss: 21.8266\n",
      "Epoch 441/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 17.1812 - val_loss: 23.1049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 442/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 17.1644 - val_loss: 24.7027\n",
      "Epoch 443/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 17.9157 - val_loss: 22.7748\n",
      "Epoch 444/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.3609 - val_loss: 23.1170\n",
      "Epoch 445/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.2335 - val_loss: 23.4535\n",
      "Epoch 446/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 16.9417 - val_loss: 23.4807\n",
      "Epoch 447/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 16.9358 - val_loss: 24.9477\n",
      "Epoch 448/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 17.3435 - val_loss: 22.7076\n",
      "Epoch 449/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.7939 - val_loss: 22.9328\n",
      "Epoch 450/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 17.4111 - val_loss: 25.6005\n",
      "Epoch 451/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 19.1017 - val_loss: 23.1562\n",
      "Epoch 452/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 18.4408 - val_loss: 21.9629\n",
      "Epoch 453/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.5804 - val_loss: 24.9615\n",
      "Epoch 454/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 17.9046 - val_loss: 25.2626\n",
      "Epoch 455/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 20.2134 - val_loss: 24.1517\n",
      "Epoch 456/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 17.4530 - val_loss: 25.9637\n",
      "Epoch 457/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 17.6389 - val_loss: 27.8352\n",
      "Epoch 458/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 16.5861 - val_loss: 23.7801\n",
      "Epoch 459/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 17.0936 - val_loss: 25.6471\n",
      "Epoch 460/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 18.1972 - val_loss: 25.8399\n",
      "Epoch 461/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 18.1183 - val_loss: 26.2436\n",
      "Epoch 462/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.9975 - val_loss: 24.1111\n",
      "Epoch 463/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 18.5042 - val_loss: 23.3027\n",
      "Epoch 464/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.8102 - val_loss: 25.8701\n",
      "Epoch 465/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 17.2885 - val_loss: 23.5345\n",
      "Epoch 466/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 17.4130 - val_loss: 23.1237\n",
      "Epoch 467/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 18.3534 - val_loss: 23.4036\n",
      "Epoch 468/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 17.5978 - val_loss: 26.4428\n",
      "Epoch 469/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 16.8221 - val_loss: 24.9378\n",
      "Epoch 470/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 17.5343 - val_loss: 22.9249\n",
      "Epoch 471/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 16.6497 - val_loss: 24.0670\n",
      "Epoch 472/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 16.6009 - val_loss: 25.9279\n",
      "Epoch 473/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.8609 - val_loss: 24.6859\n",
      "Epoch 474/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 17.0184 - val_loss: 21.5624\n",
      "Epoch 475/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 16.9544 - val_loss: 22.6370\n",
      "Epoch 476/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 16.4400 - val_loss: 21.7675\n",
      "Epoch 477/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 16.6991 - val_loss: 24.1674\n",
      "Epoch 478/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.5194 - val_loss: 23.1611\n",
      "Epoch 479/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.3835 - val_loss: 21.7821\n",
      "Epoch 480/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 17.3386 - val_loss: 24.3196\n",
      "Epoch 481/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 19.3138 - val_loss: 23.0194\n",
      "Epoch 482/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 16.1279 - val_loss: 23.8973\n",
      "Epoch 483/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 18.0512 - val_loss: 23.9910\n",
      "Epoch 484/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 17.9045 - val_loss: 24.8392\n",
      "Epoch 485/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 16.8734 - val_loss: 24.1348\n",
      "Epoch 486/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.8907 - val_loss: 26.2292\n",
      "Epoch 487/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 18.4250 - val_loss: 24.6429\n",
      "Epoch 488/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 15.9903 - val_loss: 25.8531\n",
      "Epoch 489/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 16.5900 - val_loss: 29.5015\n",
      "Epoch 490/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 17.8011 - val_loss: 23.4345\n",
      "Epoch 491/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 16.6382 - val_loss: 22.0801\n",
      "Epoch 492/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 16.5200 - val_loss: 26.9096\n",
      "Epoch 493/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 18.1711 - val_loss: 23.7369\n",
      "Epoch 494/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 16.4688 - val_loss: 24.5552\n",
      "Epoch 495/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 16.9413 - val_loss: 25.0783\n",
      "Epoch 496/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 17.1647 - val_loss: 22.2451\n",
      "Epoch 497/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 17.5694 - val_loss: 25.1354\n",
      "Epoch 498/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 16.5573 - val_loss: 26.8447\n",
      "Epoch 499/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 16.9396 - val_loss: 23.9165\n",
      "Epoch 500/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 16.7029 - val_loss: 23.8481\n",
      "Epoch 501/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.2269 - val_loss: 26.4602\n",
      "Epoch 502/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.8689 - val_loss: 24.4954\n",
      "Epoch 503/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 16.9141 - val_loss: 25.4312\n",
      "Epoch 504/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.9776 - val_loss: 22.3450\n",
      "Epoch 505/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.1860 - val_loss: 22.9445\n",
      "Epoch 506/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 17.0986 - val_loss: 24.6769\n",
      "Epoch 507/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 18.0014 - val_loss: 22.3861\n",
      "Epoch 508/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 16.1091 - val_loss: 22.7689\n",
      "Epoch 509/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 16.9273 - val_loss: 23.3821\n",
      "Epoch 510/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 17.1537 - val_loss: 25.0241\n",
      "Epoch 511/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 16.2695 - val_loss: 24.9584\n",
      "Epoch 512/2000\n",
      "1500/1500 [==============================] - 0s 38us/sample - loss: 17.3534 - val_loss: 22.9999\n",
      "Epoch 513/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 16.0519 - val_loss: 22.9271\n",
      "Epoch 514/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 16.1663 - val_loss: 24.7562\n",
      "Epoch 515/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 16.4156 - val_loss: 22.3667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 516/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 17.1295 - val_loss: 22.7349\n",
      "Epoch 517/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 16.2601 - val_loss: 23.7625\n",
      "Epoch 518/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 16.7476 - val_loss: 23.2184\n",
      "Epoch 519/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 17.2773 - val_loss: 22.9713\n",
      "Epoch 520/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 16.1060 - val_loss: 24.1482\n",
      "Epoch 521/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 16.3748 - val_loss: 23.7729\n",
      "Epoch 522/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 15.9039 - val_loss: 22.5930\n",
      "Epoch 523/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 15.6530 - val_loss: 24.5689\n",
      "Epoch 524/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 17.5890 - val_loss: 23.2952\n",
      "Epoch 525/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 15.8209 - val_loss: 22.9102\n",
      "Epoch 526/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 17.3545 - val_loss: 24.2801\n",
      "Epoch 527/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 16.0019 - val_loss: 22.4896\n",
      "Epoch 528/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 16.1083 - val_loss: 25.4497\n",
      "Epoch 529/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 16.9429 - val_loss: 24.0225\n",
      "Epoch 530/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 16.4507 - val_loss: 23.5678\n",
      "Epoch 531/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 18.3129 - val_loss: 24.5768\n",
      "Epoch 532/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 17.7338 - val_loss: 25.7425\n",
      "Epoch 533/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 17.6289 - val_loss: 23.8267\n",
      "Epoch 534/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 16.9859 - val_loss: 27.6481\n",
      "Epoch 535/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 16.4776 - val_loss: 22.2906\n",
      "Epoch 536/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 16.7640 - val_loss: 27.4148\n",
      "Epoch 537/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 21.0134 - val_loss: 27.4510\n",
      "Epoch 538/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.6338 - val_loss: 26.3438\n",
      "Epoch 539/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 16.4487 - val_loss: 23.6447\n",
      "Epoch 540/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 15.8501 - val_loss: 24.2027\n",
      "Epoch 541/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 16.2831 - val_loss: 24.2539\n",
      "Epoch 542/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 16.8112 - val_loss: 25.0507\n",
      "Epoch 543/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 17.3634 - val_loss: 22.8499\n",
      "Epoch 544/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 15.7714 - val_loss: 22.3027\n",
      "Epoch 545/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 16.3343 - val_loss: 26.3856\n",
      "Epoch 546/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 16.9677 - val_loss: 23.6488\n",
      "Epoch 547/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 15.5150 - val_loss: 23.0715\n",
      "Epoch 548/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 16.5469 - val_loss: 23.4881\n",
      "Epoch 549/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 16.3835 - val_loss: 25.0253\n",
      "Epoch 550/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 16.5111 - val_loss: 24.0892\n",
      "Epoch 551/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 17.7846 - val_loss: 24.6544\n",
      "Epoch 552/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 16.8990 - val_loss: 23.7534\n",
      "Epoch 553/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 17.1983 - val_loss: 23.6256\n",
      "Epoch 554/2000\n",
      "1500/1500 [==============================] - 0s 42us/sample - loss: 17.0214 - val_loss: 23.8115\n",
      "Epoch 555/2000\n",
      "1500/1500 [==============================] - 0s 43us/sample - loss: 16.9731 - val_loss: 23.0113\n",
      "Epoch 556/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 16.3600 - val_loss: 25.3209\n",
      "Epoch 557/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 16.9117 - val_loss: 24.9980\n",
      "Epoch 558/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 15.8150 - val_loss: 24.1627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|████████████████████████████████████▎                                              | 7/16 [03:52<05:06, 34.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'batc_normalization': False, 'batch_size': 32, 'dropout': 0, 'epochs': 2000, 'first_neuron': 320, 'hidden_layers': 2, 'hidden_neuron': 100, 'kernel_initializer': 'normal', 'last_activation': 'linear', 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "Train on 1500 samples, validate on 375 samples\n",
      "Epoch 1/2000\n",
      "1500/1500 [==============================] - 0s 269us/sample - loss: 2170.4388 - val_loss: 2099.9574\n",
      "Epoch 2/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 1452.8741 - val_loss: 546.2851\n",
      "Epoch 3/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 489.5989 - val_loss: 349.3794\n",
      "Epoch 4/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 290.1947 - val_loss: 206.8452\n",
      "Epoch 5/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 166.2880 - val_loss: 123.9630\n",
      "Epoch 6/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 104.4460 - val_loss: 82.7434\n",
      "Epoch 7/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 72.3528 - val_loss: 60.5242\n",
      "Epoch 8/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 55.3066 - val_loss: 48.2631\n",
      "Epoch 9/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 44.0483 - val_loss: 44.6714\n",
      "Epoch 10/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 37.6198 - val_loss: 36.1157\n",
      "Epoch 11/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 34.4294 - val_loss: 35.9636\n",
      "Epoch 12/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 32.4337 - val_loss: 35.2855\n",
      "Epoch 13/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 30.5993 - val_loss: 30.7586\n",
      "Epoch 14/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 29.0280 - val_loss: 31.3315\n",
      "Epoch 15/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 29.6765 - val_loss: 34.5419\n",
      "Epoch 16/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 27.2088 - val_loss: 26.5539\n",
      "Epoch 17/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 27.1592 - val_loss: 24.7266\n",
      "Epoch 18/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 26.4528 - val_loss: 27.2363\n",
      "Epoch 19/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 27.5043 - val_loss: 26.3448\n",
      "Epoch 20/2000\n",
      "1500/1500 [==============================] - 0s 50us/sample - loss: 26.6502 - val_loss: 28.5946\n",
      "Epoch 21/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 27.3060 - val_loss: 27.6676\n",
      "Epoch 22/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 27.9814 - val_loss: 27.8073\n",
      "Epoch 23/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 24.2917 - val_loss: 27.6191\n",
      "Epoch 24/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 27.8413 - val_loss: 26.9469\n",
      "Epoch 25/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 24.2470 - val_loss: 25.4287\n",
      "Epoch 26/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 23.3058 - val_loss: 26.7421\n",
      "Epoch 27/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 25.9595 - val_loss: 28.3922\n",
      "Epoch 28/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 23.5973 - val_loss: 24.2093\n",
      "Epoch 29/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 24.3790 - val_loss: 29.2480\n",
      "Epoch 30/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 25.4779 - val_loss: 25.7419\n",
      "Epoch 31/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 25.5416 - val_loss: 26.4684\n",
      "Epoch 32/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 25.8688 - val_loss: 29.7840\n",
      "Epoch 33/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 24.7373 - val_loss: 22.9105\n",
      "Epoch 34/2000\n",
      "1500/1500 [==============================] - 0s 51us/sample - loss: 24.3072 - val_loss: 30.5212\n",
      "Epoch 35/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 23.8846 - val_loss: 28.7637\n",
      "Epoch 36/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 26.0768 - val_loss: 28.2935\n",
      "Epoch 37/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 22.8138 - val_loss: 23.9069\n",
      "Epoch 38/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 26.0274 - val_loss: 24.9606\n",
      "Epoch 39/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 23.6442 - val_loss: 24.7247\n",
      "Epoch 40/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 22.4529 - val_loss: 25.4363\n",
      "Epoch 41/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 23.2203 - val_loss: 23.6835\n",
      "Epoch 42/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 22.7013 - val_loss: 23.1237\n",
      "Epoch 43/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 23.7649 - val_loss: 26.1182\n",
      "Epoch 44/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 23.7425 - val_loss: 27.9624\n",
      "Epoch 45/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 24.6267 - val_loss: 23.2071\n",
      "Epoch 46/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 24.3833 - val_loss: 25.7180\n",
      "Epoch 47/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 25.3987 - val_loss: 23.3900\n",
      "Epoch 48/2000\n",
      "1500/1500 [==============================] - 0s 51us/sample - loss: 22.1443 - val_loss: 28.7180\n",
      "Epoch 49/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 23.8893 - val_loss: 26.8085\n",
      "Epoch 50/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 23.1749 - val_loss: 25.0351\n",
      "Epoch 51/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 25.6015 - val_loss: 31.7772\n",
      "Epoch 52/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 24.4401 - val_loss: 26.7462\n",
      "Epoch 53/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 23.6006 - val_loss: 29.4317\n",
      "Epoch 54/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 23.9652 - val_loss: 22.7233\n",
      "Epoch 55/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 22.5287 - val_loss: 25.2941\n",
      "Epoch 56/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 22.6514 - val_loss: 27.3685\n",
      "Epoch 57/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 25.2076 - val_loss: 28.6326\n",
      "Epoch 58/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 25.8358 - val_loss: 24.1739\n",
      "Epoch 59/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 25.1841 - val_loss: 30.8016\n",
      "Epoch 60/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 25.8433 - val_loss: 27.1999\n",
      "Epoch 61/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 22.9161 - val_loss: 25.3793\n",
      "Epoch 62/2000\n",
      "1500/1500 [==============================] - 0s 54us/sample - loss: 22.0831 - val_loss: 24.0975\n",
      "Epoch 63/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 22.4731 - val_loss: 24.3746\n",
      "Epoch 64/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 22.2284 - val_loss: 25.9001\n",
      "Epoch 65/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 22.2053 - val_loss: 23.8789\n",
      "Epoch 66/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 22.7552 - val_loss: 26.7627\n",
      "Epoch 67/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 22.8672 - val_loss: 25.8411\n",
      "Epoch 68/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 24.5213 - val_loss: 23.5485\n",
      "Epoch 69/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 24.0436 - val_loss: 24.2199\n",
      "Epoch 70/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 22.7626 - val_loss: 28.6184\n",
      "Epoch 71/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 24.3603 - val_loss: 28.0806\n",
      "Epoch 72/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 22.7092 - val_loss: 24.1242\n",
      "Epoch 73/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 22.8769 - val_loss: 25.4445\n",
      "Epoch 74/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 21.6805 - val_loss: 24.4542\n",
      "Epoch 75/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 23.7278 - val_loss: 23.0229\n",
      "Epoch 76/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 23.8854 - val_loss: 33.3764\n",
      "Epoch 77/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 23.9208 - val_loss: 25.0857\n",
      "Epoch 78/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 21.9656 - val_loss: 24.1551\n",
      "Epoch 79/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 22.0721 - val_loss: 26.0891\n",
      "Epoch 80/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 25.3681 - val_loss: 25.7814\n",
      "Epoch 81/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 21.0736 - val_loss: 26.3568\n",
      "Epoch 82/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 21.9960 - val_loss: 24.4611\n",
      "Epoch 83/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 22.5714 - val_loss: 27.4804\n",
      "Epoch 84/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 21.5397 - val_loss: 23.0302\n",
      "Epoch 85/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 23.6756 - val_loss: 32.6959\n",
      "Epoch 86/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 24.2003 - val_loss: 24.0395\n",
      "Epoch 87/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 23.0448 - val_loss: 26.1445\n",
      "Epoch 88/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 22.4057 - val_loss: 25.2279\n",
      "Epoch 89/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 22.0697 - val_loss: 24.4135\n",
      "Epoch 90/2000\n",
      "1500/1500 [==============================] - 0s 51us/sample - loss: 23.8673 - val_loss: 24.2958\n",
      "Epoch 91/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 24.0411 - val_loss: 26.3316\n",
      "Epoch 92/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 24.4840 - val_loss: 25.1031\n",
      "Epoch 93/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 21.7538 - val_loss: 24.2260\n",
      "Epoch 94/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 22.3695 - val_loss: 23.9823\n",
      "Epoch 95/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 21.0257 - val_loss: 23.9070\n",
      "Epoch 96/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 22.2298 - val_loss: 29.1066\n",
      "Epoch 97/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 22.5134 - val_loss: 24.1553\n",
      "Epoch 98/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 20.9611 - val_loss: 25.2830\n",
      "Epoch 99/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 22.8181 - val_loss: 28.2158\n",
      "Epoch 100/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 23.0030 - val_loss: 26.3811\n",
      "Epoch 101/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 22.0366 - val_loss: 25.0488\n",
      "Epoch 102/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 22.0468 - val_loss: 23.6240\n",
      "Epoch 103/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 21.9051 - val_loss: 26.9109\n",
      "Epoch 104/2000\n",
      "1500/1500 [==============================] - 0s 50us/sample - loss: 21.8188 - val_loss: 25.2018\n",
      "Epoch 105/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 22.4035 - val_loss: 26.4463\n",
      "Epoch 106/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 21.8090 - val_loss: 29.5300\n",
      "Epoch 107/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 24.5685 - val_loss: 22.5178\n",
      "Epoch 108/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 21.4802 - val_loss: 26.0225\n",
      "Epoch 109/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 21.3794 - val_loss: 22.9097\n",
      "Epoch 110/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 20.4889 - val_loss: 22.3617\n",
      "Epoch 111/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 21.1017 - val_loss: 29.2136\n",
      "Epoch 112/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 23.5203 - val_loss: 25.8143\n",
      "Epoch 113/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 22.7726 - val_loss: 29.3770\n",
      "Epoch 114/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 23.3743 - val_loss: 30.4504\n",
      "Epoch 115/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 23.9275 - val_loss: 24.7649\n",
      "Epoch 116/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 19.9815 - val_loss: 25.1413\n",
      "Epoch 117/2000\n",
      "1500/1500 [==============================] - 0s 52us/sample - loss: 21.9738 - val_loss: 25.1799\n",
      "Epoch 118/2000\n",
      "1500/1500 [==============================] - 0s 53us/sample - loss: 20.9817 - val_loss: 29.2925\n",
      "Epoch 119/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 21.2549 - val_loss: 24.9353\n",
      "Epoch 120/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 23.2530 - val_loss: 25.8474\n",
      "Epoch 121/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 20.9744 - val_loss: 24.1490\n",
      "Epoch 122/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 20.2054 - val_loss: 27.2863\n",
      "Epoch 123/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 21.3776 - val_loss: 22.8404\n",
      "Epoch 124/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 21.0189 - val_loss: 27.1971\n",
      "Epoch 125/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 21.6056 - val_loss: 23.2348\n",
      "Epoch 126/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 20.6304 - val_loss: 22.4894\n",
      "Epoch 127/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 21.7553 - val_loss: 25.8092\n",
      "Epoch 128/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 19.9208 - val_loss: 25.0040\n",
      "Epoch 129/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 20.9017 - val_loss: 26.6129\n",
      "Epoch 130/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 21.4440 - val_loss: 27.1317\n",
      "Epoch 131/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 23.8511 - val_loss: 27.5471\n",
      "Epoch 132/2000\n",
      "1500/1500 [==============================] - 0s 51us/sample - loss: 24.2507 - val_loss: 27.9403\n",
      "Epoch 133/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 22.0666 - val_loss: 28.4503\n",
      "Epoch 134/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 20.1680 - val_loss: 24.7743\n",
      "Epoch 135/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 20.4838 - val_loss: 25.8266\n",
      "Epoch 136/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 20.4361 - val_loss: 25.6558\n",
      "Epoch 137/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 21.9015 - val_loss: 24.9855\n",
      "Epoch 138/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 21.0829 - val_loss: 23.5387\n",
      "Epoch 139/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 20.7905 - val_loss: 24.0464\n",
      "Epoch 140/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 19.7946 - val_loss: 25.2288\n",
      "Epoch 141/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 20.3536 - val_loss: 24.8455\n",
      "Epoch 142/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 20.9728 - val_loss: 30.3365\n",
      "Epoch 143/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 23.2717 - val_loss: 27.3419\n",
      "Epoch 144/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 21.1843 - val_loss: 22.5140\n",
      "Epoch 145/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 23.7172 - val_loss: 26.0102\n",
      "Epoch 146/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 51us/sample - loss: 22.2329 - val_loss: 26.0774\n",
      "Epoch 147/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 22.7973 - val_loss: 22.6225\n",
      "Epoch 148/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 21.6194 - val_loss: 28.5466\n",
      "Epoch 149/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 22.9862 - val_loss: 27.2521\n",
      "Epoch 150/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 21.0528 - val_loss: 25.2857\n",
      "Epoch 151/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 23.4958 - val_loss: 26.3119\n",
      "Epoch 152/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 21.0738 - val_loss: 22.6728\n",
      "Epoch 153/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 20.5945 - val_loss: 25.2769\n",
      "Epoch 154/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 20.8457 - val_loss: 25.8531\n",
      "Epoch 155/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 20.4461 - val_loss: 23.3789\n",
      "Epoch 156/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 19.4669 - val_loss: 25.0024\n",
      "Epoch 157/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 21.1775 - val_loss: 25.4326\n",
      "Epoch 158/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 21.0032 - val_loss: 25.2135\n",
      "Epoch 159/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 22.8195 - val_loss: 23.8647\n",
      "Epoch 160/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 21.2146 - val_loss: 25.5001\n",
      "Epoch 161/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 21.4212 - val_loss: 28.0847\n",
      "Epoch 162/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 20.3764 - val_loss: 22.2841\n",
      "Epoch 163/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 22.5184 - val_loss: 25.7642\n",
      "Epoch 164/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 21.2866 - val_loss: 25.3920\n",
      "Epoch 165/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 20.8285 - val_loss: 24.7206\n",
      "Epoch 166/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 20.7487 - val_loss: 23.1257\n",
      "Epoch 167/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 20.8725 - val_loss: 25.2816\n",
      "Epoch 168/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 20.6742 - val_loss: 24.2550\n",
      "Epoch 169/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 20.9483 - val_loss: 25.3694\n",
      "Epoch 170/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 20.8603 - val_loss: 24.2328\n",
      "Epoch 171/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 19.7650 - val_loss: 22.8196\n",
      "Epoch 172/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 21.0347 - val_loss: 23.0508\n",
      "Epoch 173/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 18.6590 - val_loss: 24.7011\n",
      "Epoch 174/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 19.7542 - val_loss: 29.9109\n",
      "Epoch 175/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 23.8816 - val_loss: 22.8331\n",
      "Epoch 176/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 20.1769 - val_loss: 23.6897\n",
      "Epoch 177/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 19.5713 - val_loss: 25.4928\n",
      "Epoch 178/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 20.4231 - val_loss: 22.8152\n",
      "Epoch 179/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 20.0172 - val_loss: 28.3606\n",
      "Epoch 180/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 21.6599 - val_loss: 25.2399\n",
      "Epoch 181/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 20.1726 - val_loss: 22.7734\n",
      "Epoch 182/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 20.2927 - val_loss: 23.0458\n",
      "Epoch 183/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 21.4906 - val_loss: 27.6005\n",
      "Epoch 184/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 20.2817 - val_loss: 22.9108\n",
      "Epoch 185/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 19.7910 - val_loss: 26.9127\n",
      "Epoch 186/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 20.7080 - val_loss: 26.4067\n",
      "Epoch 187/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 20.2184 - val_loss: 22.4485\n",
      "Epoch 188/2000\n",
      "1500/1500 [==============================] - 0s 50us/sample - loss: 18.4000 - val_loss: 25.0889\n",
      "Epoch 189/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 19.6707 - val_loss: 24.1309\n",
      "Epoch 190/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 19.4769 - val_loss: 22.6588\n",
      "Epoch 191/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 19.6129 - val_loss: 27.6350\n",
      "Epoch 192/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 19.6651 - val_loss: 24.6453\n",
      "Epoch 193/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 18.4884 - val_loss: 24.1692\n",
      "Epoch 194/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 20.4374 - val_loss: 23.2059\n",
      "Epoch 195/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 20.0242 - val_loss: 23.2228\n",
      "Epoch 196/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 18.7208 - val_loss: 26.6691\n",
      "Epoch 197/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 20.1077 - val_loss: 25.1658\n",
      "Epoch 198/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 21.7040 - val_loss: 22.3743\n",
      "Epoch 199/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 20.6347 - val_loss: 27.1729\n",
      "Epoch 200/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 19.1522 - val_loss: 22.5301\n",
      "Epoch 201/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 18.3681 - val_loss: 23.4667\n",
      "Epoch 202/2000\n",
      "1500/1500 [==============================] - 0s 53us/sample - loss: 19.1845 - val_loss: 23.7963\n",
      "Epoch 203/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 19.5239 - val_loss: 24.2169\n",
      "Epoch 204/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 18.6617 - val_loss: 21.9407\n",
      "Epoch 205/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 20.1210 - val_loss: 24.5747\n",
      "Epoch 206/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 19.5425 - val_loss: 24.4065\n",
      "Epoch 207/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 19.2581 - val_loss: 23.3177\n",
      "Epoch 208/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 21.9806 - val_loss: 26.6027\n",
      "Epoch 209/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 20.5449 - val_loss: 21.9978\n",
      "Epoch 210/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 17.9248 - val_loss: 23.4724\n",
      "Epoch 211/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 19.6791 - val_loss: 32.6685\n",
      "Epoch 212/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 21.0520 - val_loss: 30.1137\n",
      "Epoch 213/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 19.9264 - val_loss: 22.7548\n",
      "Epoch 214/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 19.8441 - val_loss: 23.9735\n",
      "Epoch 215/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 18.6620 - val_loss: 23.8749\n",
      "Epoch 216/2000\n",
      "1500/1500 [==============================] - 0s 51us/sample - loss: 19.5259 - val_loss: 25.5893\n",
      "Epoch 217/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 19.6579 - val_loss: 26.6172\n",
      "Epoch 218/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 20.3090 - val_loss: 25.5356\n",
      "Epoch 219/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 21.0123 - val_loss: 25.0816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 220/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 20.4163 - val_loss: 25.3600\n",
      "Epoch 221/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 19.7806 - val_loss: 24.6642\n",
      "Epoch 222/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 19.3292 - val_loss: 22.9142\n",
      "Epoch 223/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 18.4147 - val_loss: 25.5120\n",
      "Epoch 224/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 20.5903 - val_loss: 25.8266\n",
      "Epoch 225/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 19.6822 - val_loss: 25.6572\n",
      "Epoch 226/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 18.8584 - val_loss: 26.2630\n",
      "Epoch 227/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 18.8563 - val_loss: 28.0261\n",
      "Epoch 228/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 18.5694 - val_loss: 23.8846\n",
      "Epoch 229/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 19.0576 - val_loss: 26.6776\n",
      "Epoch 230/2000\n",
      "1500/1500 [==============================] - 0s 53us/sample - loss: 18.9328 - val_loss: 23.5905\n",
      "Epoch 231/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 19.0403 - val_loss: 22.9563\n",
      "Epoch 232/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 18.7702 - val_loss: 22.2401\n",
      "Epoch 233/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 19.8951 - val_loss: 24.7637\n",
      "Epoch 234/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 19.6970 - val_loss: 24.4844\n",
      "Epoch 235/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 20.6021 - val_loss: 23.5056\n",
      "Epoch 236/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 20.3606 - val_loss: 23.5389\n",
      "Epoch 237/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 18.3063 - val_loss: 25.6932\n",
      "Epoch 238/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 19.7281 - val_loss: 21.5671\n",
      "Epoch 239/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 22.2469 - val_loss: 25.4092\n",
      "Epoch 240/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 19.6386 - val_loss: 22.9221\n",
      "Epoch 241/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 19.1117 - val_loss: 22.9286\n",
      "Epoch 242/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 18.8720 - val_loss: 25.3345\n",
      "Epoch 243/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 19.5570 - val_loss: 25.3965\n",
      "Epoch 244/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 18.0352 - val_loss: 27.7317\n",
      "Epoch 245/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 19.9314 - val_loss: 24.7634\n",
      "Epoch 246/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 20.0351 - val_loss: 23.9959\n",
      "Epoch 247/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 19.7160 - val_loss: 24.9015\n",
      "Epoch 248/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 18.9929 - val_loss: 25.2115\n",
      "Epoch 249/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 17.7068 - val_loss: 26.2845\n",
      "Epoch 250/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 18.6223 - val_loss: 24.5756\n",
      "Epoch 251/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 19.3827 - val_loss: 22.6884\n",
      "Epoch 252/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 20.5344 - val_loss: 22.9064\n",
      "Epoch 253/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 18.9858 - val_loss: 23.9379\n",
      "Epoch 254/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 19.5500 - val_loss: 29.0425\n",
      "Epoch 255/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 18.7966 - val_loss: 26.5970\n",
      "Epoch 256/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 20.4734 - val_loss: 23.6969\n",
      "Epoch 257/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 23.3295 - val_loss: 29.6590\n",
      "Epoch 258/2000\n",
      "1500/1500 [==============================] - 0s 53us/sample - loss: 18.8989 - val_loss: 25.5556\n",
      "Epoch 259/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 18.5575 - val_loss: 23.8657\n",
      "Epoch 260/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 18.4109 - val_loss: 23.7288\n",
      "Epoch 261/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 18.7638 - val_loss: 22.0613\n",
      "Epoch 262/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 21.0644 - val_loss: 26.3076\n",
      "Epoch 263/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 20.4774 - val_loss: 29.2562\n",
      "Epoch 264/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 22.6754 - val_loss: 26.0631\n",
      "Epoch 265/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 18.2977 - val_loss: 22.2895\n",
      "Epoch 266/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 20.3236 - val_loss: 28.0943\n",
      "Epoch 267/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 21.9879 - val_loss: 23.0164\n",
      "Epoch 268/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 18.7800 - val_loss: 26.4269\n",
      "Epoch 269/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 20.1472 - val_loss: 23.6782\n",
      "Epoch 270/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 18.1146 - val_loss: 21.4789\n",
      "Epoch 271/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 18.9635 - val_loss: 28.5619\n",
      "Epoch 272/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 18.3970 - val_loss: 23.2383\n",
      "Epoch 273/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 17.2832 - val_loss: 23.8793\n",
      "Epoch 274/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 17.6335 - val_loss: 22.9950\n",
      "Epoch 275/2000\n",
      "1500/1500 [==============================] - 0s 50us/sample - loss: 17.8854 - val_loss: 22.9610\n",
      "Epoch 276/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 17.9616 - val_loss: 23.6239\n",
      "Epoch 277/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 18.8688 - val_loss: 28.2205\n",
      "Epoch 278/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 18.7272 - val_loss: 24.3866\n",
      "Epoch 279/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 16.7593 - val_loss: 24.8065\n",
      "Epoch 280/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 18.3253 - val_loss: 22.4244\n",
      "Epoch 281/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 19.0243 - val_loss: 22.5956\n",
      "Epoch 282/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 19.1857 - val_loss: 24.2436\n",
      "Epoch 283/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 18.0150 - val_loss: 24.1769\n",
      "Epoch 284/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 17.9471 - val_loss: 22.6209\n",
      "Epoch 285/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 17.7449 - val_loss: 25.6742\n",
      "Epoch 286/2000\n",
      "1500/1500 [==============================] - 0s 51us/sample - loss: 17.8279 - val_loss: 25.8909\n",
      "Epoch 287/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 17.8212 - val_loss: 24.6639\n",
      "Epoch 288/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 21.4519 - val_loss: 27.9487\n",
      "Epoch 289/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 19.9342 - val_loss: 24.2128\n",
      "Epoch 290/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 19.1285 - val_loss: 24.1963\n",
      "Epoch 291/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 18.5875 - val_loss: 24.8959\n",
      "Epoch 292/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 17.0911 - val_loss: 22.9346\n",
      "Epoch 293/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 17.4345 - val_loss: 26.5768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 294/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 19.7203 - val_loss: 24.5404\n",
      "Epoch 295/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 21.2305 - val_loss: 23.1165\n",
      "Epoch 296/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 17.1972 - val_loss: 23.1342\n",
      "Epoch 297/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 17.9939 - val_loss: 22.9624\n",
      "Epoch 298/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 17.6861 - val_loss: 30.5773\n",
      "Epoch 299/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 20.2985 - val_loss: 24.2231\n",
      "Epoch 300/2000\n",
      "1500/1500 [==============================] - 0s 51us/sample - loss: 16.3355 - val_loss: 21.2653\n",
      "Epoch 301/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 17.1444 - val_loss: 23.7396\n",
      "Epoch 302/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 19.1445 - val_loss: 25.5340\n",
      "Epoch 303/2000\n",
      "1500/1500 [==============================] - 0s 52us/sample - loss: 18.6886 - val_loss: 23.1999\n",
      "Epoch 304/2000\n",
      "1500/1500 [==============================] - 0s 51us/sample - loss: 18.3337 - val_loss: 27.6690\n",
      "Epoch 305/2000\n",
      "1500/1500 [==============================] - 0s 53us/sample - loss: 19.1297 - val_loss: 24.5103\n",
      "Epoch 306/2000\n",
      "1500/1500 [==============================] - 0s 51us/sample - loss: 17.8565 - val_loss: 24.9009\n",
      "Epoch 307/2000\n",
      "1500/1500 [==============================] - 0s 55us/sample - loss: 17.2711 - val_loss: 25.3670\n",
      "Epoch 308/2000\n",
      "1500/1500 [==============================] - 0s 53us/sample - loss: 17.8298 - val_loss: 24.8492\n",
      "Epoch 309/2000\n",
      "1500/1500 [==============================] - 0s 51us/sample - loss: 17.5156 - val_loss: 23.9007\n",
      "Epoch 310/2000\n",
      "1500/1500 [==============================] - 0s 53us/sample - loss: 17.5772 - val_loss: 22.6283\n",
      "Epoch 311/2000\n",
      "1500/1500 [==============================] - 0s 51us/sample - loss: 16.5212 - val_loss: 21.8671\n",
      "Epoch 312/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 17.7886 - val_loss: 26.2858\n",
      "Epoch 313/2000\n",
      "1500/1500 [==============================] - 0s 59us/sample - loss: 18.9013 - val_loss: 24.3825\n",
      "Epoch 314/2000\n",
      "1500/1500 [==============================] - 0s 51us/sample - loss: 17.2190 - val_loss: 23.5762\n",
      "Epoch 315/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 16.9187 - val_loss: 21.7196\n",
      "Epoch 316/2000\n",
      "1500/1500 [==============================] - 0s 51us/sample - loss: 17.1370 - val_loss: 24.7616\n",
      "Epoch 317/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 17.7258 - val_loss: 21.8364\n",
      "Epoch 318/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 17.4397 - val_loss: 25.0393\n",
      "Epoch 319/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 18.2781 - val_loss: 21.9459\n",
      "Epoch 320/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 18.1781 - val_loss: 22.6314\n",
      "Epoch 321/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 19.2717 - val_loss: 24.1651\n",
      "Epoch 322/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 17.3836 - val_loss: 24.7104\n",
      "Epoch 323/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 20.9693 - val_loss: 30.8952\n",
      "Epoch 324/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 19.8924 - val_loss: 24.7079\n",
      "Epoch 325/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 17.0767 - val_loss: 22.0627\n",
      "Epoch 326/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 18.1324 - val_loss: 23.6225\n",
      "Epoch 327/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 17.4002 - val_loss: 32.9870\n",
      "Epoch 328/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 20.7334 - val_loss: 24.7306\n",
      "Epoch 329/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 18.0239 - val_loss: 23.4516\n",
      "Epoch 330/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 16.0647 - val_loss: 23.0403\n",
      "Epoch 331/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 19.0173 - val_loss: 25.1374\n",
      "Epoch 332/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 17.6800 - val_loss: 24.1281\n",
      "Epoch 333/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 17.9479 - val_loss: 27.3704\n",
      "Epoch 334/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 16.0782 - val_loss: 23.9072\n",
      "Epoch 335/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 17.9175 - val_loss: 25.4251\n",
      "Epoch 336/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 17.5273 - val_loss: 25.5259\n",
      "Epoch 337/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 17.3988 - val_loss: 24.0226\n",
      "Epoch 338/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 16.8025 - val_loss: 22.7800\n",
      "Epoch 339/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 16.5915 - val_loss: 23.2391\n",
      "Epoch 340/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 16.8855 - val_loss: 22.4485\n",
      "Epoch 341/2000\n",
      "1500/1500 [==============================] - 0s 51us/sample - loss: 17.1506 - val_loss: 23.2513\n",
      "Epoch 342/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 16.8708 - val_loss: 24.4256\n",
      "Epoch 343/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 19.3106 - val_loss: 25.3587\n",
      "Epoch 344/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 17.7945 - val_loss: 21.3800\n",
      "Epoch 345/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 18.1524 - val_loss: 23.2585\n",
      "Epoch 346/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 18.1190 - val_loss: 22.9177\n",
      "Epoch 347/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 17.9426 - val_loss: 22.7660\n",
      "Epoch 348/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 17.1148 - val_loss: 22.6236\n",
      "Epoch 349/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 17.3948 - val_loss: 24.4266\n",
      "Epoch 350/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 19.5317 - val_loss: 25.5389\n",
      "Epoch 351/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 17.7074 - val_loss: 24.0321\n",
      "Epoch 352/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 16.9523 - val_loss: 24.6527\n",
      "Epoch 353/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 18.2484 - val_loss: 23.6037\n",
      "Epoch 354/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 18.4004 - val_loss: 26.0969\n",
      "Epoch 355/2000\n",
      "1500/1500 [==============================] - 0s 50us/sample - loss: 18.3971 - val_loss: 23.5765\n",
      "Epoch 356/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 17.6039 - val_loss: 22.9492\n",
      "Epoch 357/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 16.2961 - val_loss: 22.4469\n",
      "Epoch 358/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 16.7929 - val_loss: 21.7795\n",
      "Epoch 359/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 17.2650 - val_loss: 27.7827\n",
      "Epoch 360/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 17.1189 - val_loss: 22.9966\n",
      "Epoch 361/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 16.4408 - val_loss: 21.9031\n",
      "Epoch 362/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 17.8303 - val_loss: 26.0477\n",
      "Epoch 363/2000\n",
      "1500/1500 [==============================] - 0s 51us/sample - loss: 19.0903 - val_loss: 21.5943\n",
      "Epoch 364/2000\n",
      "1500/1500 [==============================] - 0s 50us/sample - loss: 16.3574 - val_loss: 27.1315\n",
      "Epoch 365/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 17.5284 - val_loss: 25.4557\n",
      "Epoch 366/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 18.6021 - val_loss: 24.0532\n",
      "Epoch 367/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 17.2268 - val_loss: 23.0368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 368/2000\n",
      "1500/1500 [==============================] - 0s 51us/sample - loss: 17.5748 - val_loss: 23.4779\n",
      "Epoch 369/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 18.6473 - val_loss: 22.2468\n",
      "Epoch 370/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 16.6743 - val_loss: 23.9330\n",
      "Epoch 371/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 16.3975 - val_loss: 23.0996\n",
      "Epoch 372/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 16.7908 - val_loss: 21.1994\n",
      "Epoch 373/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 18.6424 - val_loss: 26.5775\n",
      "Epoch 374/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 20.4320 - val_loss: 23.4109\n",
      "Epoch 375/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 15.7961 - val_loss: 22.4798\n",
      "Epoch 376/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 16.1495 - val_loss: 22.9110\n",
      "Epoch 377/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 20.2204 - val_loss: 23.2805\n",
      "Epoch 378/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 17.3104 - val_loss: 21.9974\n",
      "Epoch 379/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 15.6122 - val_loss: 22.5221\n",
      "Epoch 380/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 17.0289 - val_loss: 22.5536\n",
      "Epoch 381/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 16.4429 - val_loss: 24.6235\n",
      "Epoch 382/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 16.6343 - val_loss: 24.6109\n",
      "Epoch 383/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 17.3228 - val_loss: 26.5313\n",
      "Epoch 384/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 17.0815 - val_loss: 25.8379\n",
      "Epoch 385/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 15.8811 - val_loss: 22.7539\n",
      "Epoch 386/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 16.8505 - val_loss: 23.5061\n",
      "Epoch 387/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 19.0912 - val_loss: 27.7278\n",
      "Epoch 388/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 16.0429 - val_loss: 22.2721\n",
      "Epoch 389/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 18.0163 - val_loss: 25.0557\n",
      "Epoch 390/2000\n",
      "1500/1500 [==============================] - 0s 50us/sample - loss: 19.7361 - val_loss: 24.9853\n",
      "Epoch 391/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 17.6579 - val_loss: 26.0915\n",
      "Epoch 392/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 18.3630 - val_loss: 29.2637\n",
      "Epoch 393/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 16.1574 - val_loss: 21.6078\n",
      "Epoch 394/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 16.4398 - val_loss: 22.5301\n",
      "Epoch 395/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 15.9697 - val_loss: 22.1094\n",
      "Epoch 396/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 17.2425 - val_loss: 22.6712\n",
      "Epoch 397/2000\n",
      "1500/1500 [==============================] - 0s 53us/sample - loss: 16.0879 - val_loss: 22.3358\n",
      "Epoch 398/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 17.9390 - val_loss: 22.9808\n",
      "Epoch 399/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 18.2537 - val_loss: 24.1331\n",
      "Epoch 400/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 17.5668 - val_loss: 23.7335\n",
      "Epoch 401/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 17.1985 - val_loss: 24.1162\n",
      "Epoch 402/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 18.0413 - val_loss: 22.4611\n",
      "Epoch 403/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 16.9300 - val_loss: 25.6150\n",
      "Epoch 404/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 18.1522 - val_loss: 27.7709\n",
      "Epoch 405/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 15.2826 - val_loss: 23.4032\n",
      "Epoch 406/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 17.9368 - val_loss: 22.8496\n",
      "Epoch 407/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 16.3950 - val_loss: 23.2126\n",
      "Epoch 408/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 15.9953 - val_loss: 23.6618\n",
      "Epoch 409/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 16.1061 - val_loss: 22.2512\n",
      "Epoch 410/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 20.0230 - val_loss: 29.2281\n",
      "Epoch 411/2000\n",
      "1500/1500 [==============================] - 0s 50us/sample - loss: 17.8784 - val_loss: 22.7389\n",
      "Epoch 412/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 16.2827 - val_loss: 22.9448\n",
      "Epoch 413/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 15.1946 - val_loss: 22.8690\n",
      "Epoch 414/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 15.9109 - val_loss: 21.9060\n",
      "Epoch 415/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 16.3224 - val_loss: 24.0728\n",
      "Epoch 416/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 15.1411 - val_loss: 24.0713\n",
      "Epoch 417/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 15.2848 - val_loss: 24.2889\n",
      "Epoch 418/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 17.1157 - val_loss: 21.9515\n",
      "Epoch 419/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 17.5674 - val_loss: 22.3981\n",
      "Epoch 420/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 17.6597 - val_loss: 25.5501\n",
      "Epoch 421/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 16.6046 - val_loss: 25.6492\n",
      "Epoch 422/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 17.0865 - val_loss: 23.7349\n",
      "Epoch 423/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 16.4564 - val_loss: 22.7249\n",
      "Epoch 424/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 16.9931 - val_loss: 24.6231\n",
      "Epoch 425/2000\n",
      "1500/1500 [==============================] - 0s 51us/sample - loss: 16.8451 - val_loss: 23.8608\n",
      "Epoch 426/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 17.2549 - val_loss: 22.0088\n",
      "Epoch 427/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 16.0914 - val_loss: 27.5178\n",
      "Epoch 428/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 18.3937 - val_loss: 29.7172\n",
      "Epoch 429/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 16.3095 - val_loss: 22.4435\n",
      "Epoch 430/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 16.1780 - val_loss: 21.9470\n",
      "Epoch 431/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 16.3462 - val_loss: 23.9989\n",
      "Epoch 432/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 18.0696 - val_loss: 22.7051\n",
      "Epoch 433/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 16.0104 - val_loss: 22.0440\n",
      "Epoch 434/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 15.6491 - val_loss: 21.9764\n",
      "Epoch 435/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 15.3767 - val_loss: 26.3534\n",
      "Epoch 436/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 16.8977 - val_loss: 27.8207\n",
      "Epoch 437/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 18.3735 - val_loss: 24.3442\n",
      "Epoch 438/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 16.6085 - val_loss: 23.0714\n",
      "Epoch 439/2000\n",
      "1500/1500 [==============================] - 0s 50us/sample - loss: 15.4249 - val_loss: 21.6666\n",
      "Epoch 440/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 16.3635 - val_loss: 24.6555\n",
      "Epoch 441/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 16.3848 - val_loss: 23.0357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 442/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 16.8021 - val_loss: 22.4003\n",
      "Epoch 443/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 16.8903 - val_loss: 25.8168\n",
      "Epoch 444/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 17.1886 - val_loss: 22.5284\n",
      "Epoch 445/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 16.0134 - val_loss: 23.0036\n",
      "Epoch 446/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 15.9911 - val_loss: 27.7869\n",
      "Epoch 447/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 18.2026 - val_loss: 23.4702\n",
      "Epoch 448/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 17.3644 - val_loss: 26.4252\n",
      "Epoch 449/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 17.3383 - val_loss: 24.4404\n",
      "Epoch 450/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 14.5099 - val_loss: 22.3121\n",
      "Epoch 451/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 16.1653 - val_loss: 23.1279\n",
      "Epoch 452/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 16.3867 - val_loss: 24.2766\n",
      "Epoch 453/2000\n",
      "1500/1500 [==============================] - 0s 52us/sample - loss: 18.0025 - val_loss: 25.6116\n",
      "Epoch 454/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 19.7658 - val_loss: 28.3855\n",
      "Epoch 455/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 15.7899 - val_loss: 22.2483\n",
      "Epoch 456/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 15.2342 - val_loss: 21.3705\n",
      "Epoch 457/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 16.2526 - val_loss: 23.5752\n",
      "Epoch 458/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 17.1365 - val_loss: 25.3782\n",
      "Epoch 459/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 17.1438 - val_loss: 23.3160\n",
      "Epoch 460/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 17.1356 - val_loss: 23.0012\n",
      "Epoch 461/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 16.5080 - val_loss: 25.9267\n",
      "Epoch 462/2000\n",
      "1500/1500 [==============================] - 0s 51us/sample - loss: 16.0968 - val_loss: 23.8021\n",
      "Epoch 463/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 15.4666 - val_loss: 21.5968\n",
      "Epoch 464/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 15.7985 - val_loss: 23.4626\n",
      "Epoch 465/2000\n",
      "1500/1500 [==============================] - 0s 50us/sample - loss: 15.2669 - val_loss: 22.8752\n",
      "Epoch 466/2000\n",
      "1500/1500 [==============================] - 0s 52us/sample - loss: 19.0261 - val_loss: 22.7742\n",
      "Epoch 467/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 17.6264 - val_loss: 24.5718\n",
      "Epoch 468/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 15.3621 - val_loss: 22.7952\n",
      "Epoch 469/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 14.8620 - val_loss: 25.5701\n",
      "Epoch 470/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 15.5663 - val_loss: 25.9498\n",
      "Epoch 471/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 18.8911 - val_loss: 22.1850\n",
      "Epoch 472/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 19.4265 - val_loss: 23.2057\n",
      "Epoch 473/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 15.9161 - val_loss: 23.6208\n",
      "Epoch 474/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 16.8139 - val_loss: 22.0032\n",
      "Epoch 475/2000\n",
      "1500/1500 [==============================] - 0s 51us/sample - loss: 15.0858 - val_loss: 23.6168\n",
      "Epoch 476/2000\n",
      "1500/1500 [==============================] - 0s 51us/sample - loss: 16.8710 - val_loss: 25.2908\n",
      "Epoch 477/2000\n",
      "1500/1500 [==============================] - 0s 51us/sample - loss: 17.1965 - val_loss: 23.6445\n",
      "Epoch 478/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 19.0981 - val_loss: 21.9233\n",
      "Epoch 479/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 15.3665 - val_loss: 23.1182\n",
      "Epoch 480/2000\n",
      "1500/1500 [==============================] - 0s 51us/sample - loss: 16.4680 - val_loss: 24.6029\n",
      "Epoch 481/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 14.4030 - val_loss: 23.7176\n",
      "Epoch 482/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 14.9319 - val_loss: 22.3115\n",
      "Epoch 483/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 14.8152 - val_loss: 26.0291\n",
      "Epoch 484/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 16.3895 - val_loss: 22.2559\n",
      "Epoch 485/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 16.6917 - val_loss: 24.5807\n",
      "Epoch 486/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 15.2878 - val_loss: 21.9753\n",
      "Epoch 487/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 15.1731 - val_loss: 22.4124\n",
      "Epoch 488/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 15.2340 - val_loss: 21.7846\n",
      "Epoch 489/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 15.0631 - val_loss: 22.8368\n",
      "Epoch 490/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 17.6160 - val_loss: 23.8075\n",
      "Epoch 491/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 15.7846 - val_loss: 21.9361\n",
      "Epoch 492/2000\n",
      "1500/1500 [==============================] - 0s 51us/sample - loss: 15.0255 - val_loss: 25.1468\n",
      "Epoch 493/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 15.3919 - val_loss: 23.8716\n",
      "Epoch 494/2000\n",
      "1500/1500 [==============================] - 0s 53us/sample - loss: 16.4063 - val_loss: 24.7700\n",
      "Epoch 495/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 15.7597 - val_loss: 20.8351\n",
      "Epoch 496/2000\n",
      "1500/1500 [==============================] - 0s 51us/sample - loss: 15.6900 - val_loss: 22.4042\n",
      "Epoch 497/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 15.2883 - val_loss: 22.2252\n",
      "Epoch 498/2000\n",
      "1500/1500 [==============================] - 0s 50us/sample - loss: 14.9817 - val_loss: 21.9764\n",
      "Epoch 499/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 16.7505 - val_loss: 25.9917\n",
      "Epoch 500/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 15.5797 - val_loss: 21.9499\n",
      "Epoch 501/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 15.8043 - val_loss: 21.7435\n",
      "Epoch 502/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 15.3943 - val_loss: 23.0825\n",
      "Epoch 503/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 15.6535 - val_loss: 23.0600\n",
      "Epoch 504/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 15.8861 - val_loss: 26.4129\n",
      "Epoch 505/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 17.1475 - val_loss: 24.3655\n",
      "Epoch 506/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 17.5680 - val_loss: 24.9011\n",
      "Epoch 507/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 15.0200 - val_loss: 21.8215\n",
      "Epoch 508/2000\n",
      "1500/1500 [==============================] - 0s 52us/sample - loss: 16.4193 - val_loss: 21.1224\n",
      "Epoch 509/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 18.5093 - val_loss: 25.8010\n",
      "Epoch 510/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 15.6504 - val_loss: 22.9662\n",
      "Epoch 511/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 15.3281 - val_loss: 23.6358\n",
      "Epoch 512/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 15.2697 - val_loss: 24.6927\n",
      "Epoch 513/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 14.7511 - val_loss: 25.4427\n",
      "Epoch 514/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 15.1456 - val_loss: 27.6383\n",
      "Epoch 515/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 16.7632 - val_loss: 23.7235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 516/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 15.5110 - val_loss: 25.2148\n",
      "Epoch 517/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 16.4696 - val_loss: 22.3029\n",
      "Epoch 518/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 14.6086 - val_loss: 21.4431\n",
      "Epoch 519/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 15.0878 - val_loss: 26.0605\n",
      "Epoch 520/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 15.5450 - val_loss: 24.2969\n",
      "Epoch 521/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 14.6296 - val_loss: 24.2904\n",
      "Epoch 522/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 14.7121 - val_loss: 22.4400\n",
      "Epoch 523/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 15.4930 - val_loss: 24.0441\n",
      "Epoch 524/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 14.8820 - val_loss: 23.6370\n",
      "Epoch 525/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 15.3652 - val_loss: 25.7772\n",
      "Epoch 526/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 16.4567 - val_loss: 23.1362\n",
      "Epoch 527/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 14.9356 - val_loss: 22.0026\n",
      "Epoch 528/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 14.4401 - val_loss: 22.4621\n",
      "Epoch 529/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 14.7684 - val_loss: 23.8794\n",
      "Epoch 530/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 15.8963 - val_loss: 23.8932\n",
      "Epoch 531/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 15.2018 - val_loss: 25.4854\n",
      "Epoch 532/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 14.6641 - val_loss: 22.0686\n",
      "Epoch 533/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 14.0258 - val_loss: 23.4199\n",
      "Epoch 534/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 13.7986 - val_loss: 22.3546\n",
      "Epoch 535/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 15.2384 - val_loss: 28.4310\n",
      "Epoch 536/2000\n",
      "1500/1500 [==============================] - 0s 53us/sample - loss: 17.5109 - val_loss: 24.4950\n",
      "Epoch 537/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 16.0884 - val_loss: 26.0586\n",
      "Epoch 538/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 15.8788 - val_loss: 24.8315\n",
      "Epoch 539/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 16.3046 - val_loss: 22.6870\n",
      "Epoch 540/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 17.0467 - val_loss: 23.1787\n",
      "Epoch 541/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 14.6254 - val_loss: 22.5382\n",
      "Epoch 542/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 14.4456 - val_loss: 23.7583\n",
      "Epoch 543/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 14.8586 - val_loss: 22.7351\n",
      "Epoch 544/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 16.5686 - val_loss: 26.6770\n",
      "Epoch 545/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 15.2212 - val_loss: 22.9975\n",
      "Epoch 546/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 16.9253 - val_loss: 23.8120\n",
      "Epoch 547/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 15.3261 - val_loss: 22.5556\n",
      "Epoch 548/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 14.2335 - val_loss: 23.9234\n",
      "Epoch 549/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 16.1686 - val_loss: 24.5104\n",
      "Epoch 550/2000\n",
      "1500/1500 [==============================] - 0s 51us/sample - loss: 14.6768 - val_loss: 26.5866\n",
      "Epoch 551/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 14.8730 - val_loss: 26.2786\n",
      "Epoch 552/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 16.6055 - val_loss: 23.2000\n",
      "Epoch 553/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 17.0685 - val_loss: 22.8095\n",
      "Epoch 554/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 14.9450 - val_loss: 22.1963\n",
      "Epoch 555/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 16.3581 - val_loss: 24.5301\n",
      "Epoch 556/2000\n",
      "1500/1500 [==============================] - 0s 50us/sample - loss: 14.9996 - val_loss: 24.1107\n",
      "Epoch 557/2000\n",
      "1500/1500 [==============================] - 0s 50us/sample - loss: 14.8948 - val_loss: 22.7478\n",
      "Epoch 558/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 19.4937 - val_loss: 32.4260\n",
      "Epoch 559/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 15.5419 - val_loss: 21.9027\n",
      "Epoch 560/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 13.9961 - val_loss: 22.2426\n",
      "Epoch 561/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 16.4300 - val_loss: 23.7894\n",
      "Epoch 562/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 16.4880 - val_loss: 24.5396\n",
      "Epoch 563/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 15.0463 - val_loss: 24.7266\n",
      "Epoch 564/2000\n",
      "1500/1500 [==============================] - 0s 52us/sample - loss: 16.9216 - val_loss: 24.6672\n",
      "Epoch 565/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 18.6968 - val_loss: 23.2311\n",
      "Epoch 566/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 15.6218 - val_loss: 23.8096\n",
      "Epoch 567/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 15.1275 - val_loss: 21.5682\n",
      "Epoch 568/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 14.5348 - val_loss: 23.1254\n",
      "Epoch 569/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 17.5313 - val_loss: 23.7278\n",
      "Epoch 570/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 14.6177 - val_loss: 24.3684\n",
      "Epoch 571/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 15.5107 - val_loss: 25.6173\n",
      "Epoch 572/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 14.9449 - val_loss: 24.0929\n",
      "Epoch 573/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 14.2832 - val_loss: 23.6296\n",
      "Epoch 574/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 14.4192 - val_loss: 22.6256\n",
      "Epoch 575/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 13.9034 - val_loss: 23.3546\n",
      "Epoch 576/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 14.3231 - val_loss: 22.7553\n",
      "Epoch 577/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 17.4550 - val_loss: 24.7674\n",
      "Epoch 578/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 16.6318 - val_loss: 22.9378\n",
      "Epoch 579/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 14.3822 - val_loss: 23.5399\n",
      "Epoch 580/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 13.7742 - val_loss: 24.1883\n",
      "Epoch 581/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 14.7730 - val_loss: 22.7650\n",
      "Epoch 582/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 15.8159 - val_loss: 21.4838\n",
      "Epoch 583/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 15.0834 - val_loss: 21.0110\n",
      "Epoch 584/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 16.0211 - val_loss: 23.8155\n",
      "Epoch 585/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 15.6765 - val_loss: 27.5310\n",
      "Epoch 586/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 15.5775 - val_loss: 22.7501\n",
      "Epoch 587/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 14.4093 - val_loss: 21.4666\n",
      "Epoch 588/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 16.4807 - val_loss: 26.9643\n",
      "Epoch 589/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 14.7718 - val_loss: 26.5274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 590/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 15.9984 - val_loss: 24.2412\n",
      "Epoch 591/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 15.0086 - val_loss: 24.2608\n",
      "Epoch 592/2000\n",
      "1500/1500 [==============================] - 0s 52us/sample - loss: 15.4028 - val_loss: 24.4320\n",
      "Epoch 593/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 17.7448 - val_loss: 27.2257\n",
      "Epoch 594/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 16.3355 - val_loss: 22.9723\n",
      "Epoch 595/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 14.8237 - val_loss: 23.4273\n",
      "Epoch 596/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 15.8318 - val_loss: 23.6763\n",
      "Epoch 597/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 15.3863 - val_loss: 22.3872\n",
      "Epoch 598/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 13.2001 - val_loss: 22.0711\n",
      "Epoch 599/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 14.4992 - val_loss: 23.1890\n",
      "Epoch 600/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 15.4518 - val_loss: 22.2786\n",
      "Epoch 601/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 13.6441 - val_loss: 22.7829\n",
      "Epoch 602/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 16.3379 - val_loss: 22.3845\n",
      "Epoch 603/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 13.4503 - val_loss: 26.7967\n",
      "Epoch 604/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 14.0951 - val_loss: 25.6848\n",
      "Epoch 605/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 15.4286 - val_loss: 23.2576\n",
      "Epoch 606/2000\n",
      "1500/1500 [==============================] - 0s 50us/sample - loss: 17.9792 - val_loss: 22.6085\n",
      "Epoch 607/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 14.8906 - val_loss: 22.4398\n",
      "Epoch 608/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 15.7356 - val_loss: 23.3767\n",
      "Epoch 609/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 16.4511 - val_loss: 28.3535\n",
      "Epoch 610/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 17.0513 - val_loss: 21.4142\n",
      "Epoch 611/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 13.7907 - val_loss: 23.8413\n",
      "Epoch 612/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 15.1208 - val_loss: 23.2707\n",
      "Epoch 613/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 14.6994 - val_loss: 26.3418\n",
      "Epoch 614/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 17.2810 - val_loss: 26.9559\n",
      "Epoch 615/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 14.6797 - val_loss: 26.1105\n",
      "Epoch 616/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 14.8399 - val_loss: 22.8543\n",
      "Epoch 617/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 14.8543 - val_loss: 21.1272\n",
      "Epoch 618/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 13.7521 - val_loss: 22.1157\n",
      "Epoch 619/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 13.8979 - val_loss: 24.7092\n",
      "Epoch 620/2000\n",
      "1500/1500 [==============================] - 0s 51us/sample - loss: 14.6953 - val_loss: 24.6255\n",
      "Epoch 621/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 13.7361 - val_loss: 21.8386\n",
      "Epoch 622/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 14.2531 - val_loss: 22.6572\n",
      "Epoch 623/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 13.7206 - val_loss: 24.1092\n",
      "Epoch 624/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 14.0529 - val_loss: 22.3695\n",
      "Epoch 625/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 14.8430 - val_loss: 22.8834\n",
      "Epoch 626/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 14.3049 - val_loss: 27.7219\n",
      "Epoch 627/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 15.0288 - val_loss: 22.0897\n",
      "Epoch 628/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 14.1617 - val_loss: 23.9045\n",
      "Epoch 629/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 13.2300 - val_loss: 21.7751\n",
      "Epoch 630/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 13.8417 - val_loss: 26.2135\n",
      "Epoch 631/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 16.2911 - val_loss: 23.9582\n",
      "Epoch 632/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 14.8498 - val_loss: 24.5028\n",
      "Epoch 633/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 15.5144 - val_loss: 23.0372\n",
      "Epoch 634/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 13.6786 - val_loss: 23.3328\n",
      "Epoch 635/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 13.3755 - val_loss: 22.7597\n",
      "Epoch 636/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 15.2221 - val_loss: 24.0548\n",
      "Epoch 637/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 14.7445 - val_loss: 22.4964\n",
      "Epoch 638/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 16.5624 - val_loss: 23.8938\n",
      "Epoch 639/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 15.4284 - val_loss: 26.0559\n",
      "Epoch 640/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 15.1030 - val_loss: 26.1054\n",
      "Epoch 641/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 14.4608 - val_loss: 27.2726\n",
      "Epoch 642/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 14.3429 - val_loss: 24.4763\n",
      "Epoch 643/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 14.8935 - val_loss: 24.0222\n",
      "Epoch 644/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 14.8518 - val_loss: 29.4416\n",
      "Epoch 645/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 19.6919 - val_loss: 25.6632\n",
      "Epoch 646/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 14.3412 - val_loss: 25.9463\n",
      "Epoch 647/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 15.1251 - val_loss: 22.6699\n",
      "Epoch 648/2000\n",
      "1500/1500 [==============================] - 0s 51us/sample - loss: 15.5697 - val_loss: 23.3554\n",
      "Epoch 649/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 16.8228 - val_loss: 26.7823\n",
      "Epoch 650/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 16.2967 - val_loss: 25.9128\n",
      "Epoch 651/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 15.5761 - val_loss: 22.2829\n",
      "Epoch 652/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 15.1925 - val_loss: 22.5282\n",
      "Epoch 653/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 14.2082 - val_loss: 22.2975\n",
      "Epoch 654/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 16.2475 - val_loss: 23.3609\n",
      "Epoch 655/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 15.3633 - val_loss: 24.7183\n",
      "Epoch 656/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 16.6180 - val_loss: 23.0072\n",
      "Epoch 657/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 14.7758 - val_loss: 26.3379\n",
      "Epoch 658/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 14.1652 - val_loss: 22.0385\n",
      "Epoch 659/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 13.4436 - val_loss: 21.1062\n",
      "Epoch 660/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 13.6592 - val_loss: 24.0696\n",
      "Epoch 661/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 14.5559 - val_loss: 21.7476\n",
      "Epoch 662/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 12.3991 - val_loss: 21.7228\n",
      "Epoch 663/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 12.9302 - val_loss: 23.6277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 664/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 15.0555 - val_loss: 28.9434\n",
      "Epoch 665/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 16.1676 - val_loss: 24.2516\n",
      "Epoch 666/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 13.6338 - val_loss: 22.5240\n",
      "Epoch 667/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 14.5737 - val_loss: 24.6955\n",
      "Epoch 668/2000\n",
      "1500/1500 [==============================] - 0s 49us/sample - loss: 16.1487 - val_loss: 23.4400\n",
      "Epoch 669/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 16.0408 - val_loss: 26.4351\n",
      "Epoch 670/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 13.0075 - val_loss: 24.1439\n",
      "Epoch 671/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 14.2967 - val_loss: 23.9642\n",
      "Epoch 672/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 15.6812 - val_loss: 23.6468\n",
      "Epoch 673/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 14.8084 - val_loss: 23.6700\n",
      "Epoch 674/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 14.5624 - val_loss: 23.0897\n",
      "Epoch 675/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 13.8151 - val_loss: 24.5828\n",
      "Epoch 676/2000\n",
      "1500/1500 [==============================] - 0s 51us/sample - loss: 17.9222 - val_loss: 25.1028\n",
      "Epoch 677/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 13.0961 - val_loss: 22.6287\n",
      "Epoch 678/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 15.4669 - val_loss: 22.7670\n",
      "Epoch 679/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 13.5522 - val_loss: 22.9573\n",
      "Epoch 680/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 16.0005 - val_loss: 23.3830\n",
      "Epoch 681/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 13.7710 - val_loss: 23.2403\n",
      "Epoch 682/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 13.5740 - val_loss: 28.2310\n",
      "Epoch 683/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 14.3936 - val_loss: 21.8572\n",
      "Epoch 684/2000\n",
      "1500/1500 [==============================] - 0s 45us/sample - loss: 14.4960 - val_loss: 22.7210\n",
      "Epoch 685/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 13.8964 - val_loss: 23.4567\n",
      "Epoch 686/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 13.8451 - val_loss: 22.2129\n",
      "Epoch 687/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 15.9603 - val_loss: 24.6463\n",
      "Epoch 688/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 15.1515 - val_loss: 24.6651\n",
      "Epoch 689/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 16.4264 - val_loss: 25.3973\n",
      "Epoch 690/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 15.0189 - val_loss: 24.4295\n",
      "Epoch 691/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 14.8301 - val_loss: 22.3544\n",
      "Epoch 692/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 14.4261 - val_loss: 27.8999\n",
      "Epoch 693/2000\n",
      "1500/1500 [==============================] - 0s 46us/sample - loss: 14.9982 - val_loss: 21.8299\n",
      "Epoch 694/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 14.0329 - val_loss: 22.2055\n",
      "Epoch 695/2000\n",
      "1500/1500 [==============================] - 0s 47us/sample - loss: 13.6285 - val_loss: 22.9562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████▌                                         | 8/16 [04:43<05:14, 39.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'batc_normalization': False, 'batch_size': 64, 'dropout': 0, 'epochs': 2000, 'first_neuron': 160, 'hidden_layers': 1, 'hidden_neuron': 50, 'kernel_initializer': 'normal', 'last_activation': 'linear', 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "Train on 1500 samples, validate on 375 samples\n",
      "Epoch 1/2000\n",
      "1500/1500 [==============================] - 0s 221us/sample - loss: 2185.7098 - val_loss: 2185.5645\n",
      "Epoch 2/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 2181.7504 - val_loss: 2177.3639\n",
      "Epoch 3/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 2166.0991 - val_loss: 2150.5276\n",
      "Epoch 4/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 2123.6156 - val_loss: 2086.9577\n",
      "Epoch 5/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 2033.9433 - val_loss: 1965.1680\n",
      "Epoch 6/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 1874.9266 - val_loss: 1762.3206\n",
      "Epoch 7/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 1623.5041 - val_loss: 1455.0291\n",
      "Epoch 8/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 1261.1557 - val_loss: 1052.2521\n",
      "Epoch 9/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 881.3341 - val_loss: 752.2322\n",
      "Epoch 10/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 695.1195 - val_loss: 637.4376\n",
      "Epoch 11/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 617.8156 - val_loss: 579.0408\n",
      "Epoch 12/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 565.3380 - val_loss: 532.5309\n",
      "Epoch 13/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 520.8225 - val_loss: 494.7317\n",
      "Epoch 14/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 478.8363 - val_loss: 453.5484\n",
      "Epoch 15/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 438.9829 - val_loss: 413.2564\n",
      "Epoch 16/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 398.2463 - val_loss: 373.6781\n",
      "Epoch 17/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 358.2280 - val_loss: 334.7832\n",
      "Epoch 18/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 318.1175 - val_loss: 295.0614\n",
      "Epoch 19/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 277.6974 - val_loss: 256.1213\n",
      "Epoch 20/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 237.9261 - val_loss: 215.5253\n",
      "Epoch 21/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 200.8678 - val_loss: 182.1327\n",
      "Epoch 22/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 169.2879 - val_loss: 155.1776\n",
      "Epoch 23/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 142.9387 - val_loss: 130.8845\n",
      "Epoch 24/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 121.2780 - val_loss: 112.6699\n",
      "Epoch 25/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 103.4068 - val_loss: 95.6252\n",
      "Epoch 26/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 89.0050 - val_loss: 82.5575\n",
      "Epoch 27/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 77.2911 - val_loss: 73.4248\n",
      "Epoch 28/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 66.3745 - val_loss: 66.0419\n",
      "Epoch 29/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 59.2166 - val_loss: 56.5094\n",
      "Epoch 30/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 51.4905 - val_loss: 51.7851\n",
      "Epoch 31/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 46.5201 - val_loss: 48.5535\n",
      "Epoch 32/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 42.4221 - val_loss: 44.2331\n",
      "Epoch 33/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 39.7434 - val_loss: 41.4035\n",
      "Epoch 34/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 37.4269 - val_loss: 40.2105\n",
      "Epoch 35/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 35.8181 - val_loss: 37.8600\n",
      "Epoch 36/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 33.7078 - val_loss: 36.0026\n",
      "Epoch 37/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 31.7229 - val_loss: 34.5444\n",
      "Epoch 38/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 31.2075 - val_loss: 33.1220\n",
      "Epoch 39/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 29.5866 - val_loss: 32.8640\n",
      "Epoch 40/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 29.4779 - val_loss: 31.7175\n",
      "Epoch 41/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 28.4269 - val_loss: 30.3586\n",
      "Epoch 42/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 27.8734 - val_loss: 29.6409\n",
      "Epoch 43/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 26.8406 - val_loss: 29.0280\n",
      "Epoch 44/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 26.4607 - val_loss: 28.9262\n",
      "Epoch 45/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 26.7209 - val_loss: 27.6496\n",
      "Epoch 46/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 25.9583 - val_loss: 27.6806\n",
      "Epoch 47/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 25.0984 - val_loss: 26.5120\n",
      "Epoch 48/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 24.7729 - val_loss: 26.5762\n",
      "Epoch 49/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 24.5819 - val_loss: 26.6553\n",
      "Epoch 50/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 24.7896 - val_loss: 25.7292\n",
      "Epoch 51/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 24.3739 - val_loss: 26.0938\n",
      "Epoch 52/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 23.7948 - val_loss: 26.6836\n",
      "Epoch 53/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 23.7960 - val_loss: 26.4390\n",
      "Epoch 54/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 23.2478 - val_loss: 25.2722\n",
      "Epoch 55/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 23.3484 - val_loss: 23.8272\n",
      "Epoch 56/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 23.0147 - val_loss: 24.2823\n",
      "Epoch 57/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 22.9258 - val_loss: 24.6443\n",
      "Epoch 58/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 22.7354 - val_loss: 24.7284\n",
      "Epoch 59/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 22.7176 - val_loss: 25.0183\n",
      "Epoch 60/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 22.8342 - val_loss: 24.6588\n",
      "Epoch 61/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 22.9098 - val_loss: 24.3293\n",
      "Epoch 62/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 23.2697 - val_loss: 25.1717\n",
      "Epoch 63/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 22.6164 - val_loss: 25.3800\n",
      "Epoch 64/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 22.2815 - val_loss: 24.4040\n",
      "Epoch 65/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 22.3593 - val_loss: 23.6410\n",
      "Epoch 66/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 22.2928 - val_loss: 24.2049\n",
      "Epoch 67/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 22.3882 - val_loss: 24.3511\n",
      "Epoch 68/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 22.3418 - val_loss: 24.4248\n",
      "Epoch 69/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 22.3829 - val_loss: 24.7654\n",
      "Epoch 70/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 21.9645 - val_loss: 23.7430\n",
      "Epoch 71/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 22.2046 - val_loss: 24.1521\n",
      "Epoch 72/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 21us/sample - loss: 22.0005 - val_loss: 24.0604\n",
      "Epoch 73/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 22.1781 - val_loss: 23.9570\n",
      "Epoch 74/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 21.7320 - val_loss: 23.5850\n",
      "Epoch 75/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 22.2868 - val_loss: 24.0013\n",
      "Epoch 76/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 21.8903 - val_loss: 23.3975\n",
      "Epoch 77/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 22.2254 - val_loss: 25.7108\n",
      "Epoch 78/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 21.8147 - val_loss: 23.5599\n",
      "Epoch 79/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 21.5990 - val_loss: 24.3896\n",
      "Epoch 80/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 21.7844 - val_loss: 24.7501\n",
      "Epoch 81/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 22.2404 - val_loss: 23.1611\n",
      "Epoch 82/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 21.7443 - val_loss: 24.5888\n",
      "Epoch 83/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 22.4928 - val_loss: 23.5835\n",
      "Epoch 84/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 21.7284 - val_loss: 24.7333\n",
      "Epoch 85/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 21.7880 - val_loss: 23.2106\n",
      "Epoch 86/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 22.3072 - val_loss: 23.4598\n",
      "Epoch 87/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 21.7005 - val_loss: 24.4039\n",
      "Epoch 88/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 22.0664 - val_loss: 23.5311\n",
      "Epoch 89/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 21.3595 - val_loss: 22.9058\n",
      "Epoch 90/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 21.4815 - val_loss: 24.6523\n",
      "Epoch 91/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 21.9415 - val_loss: 23.8442\n",
      "Epoch 92/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 21.9817 - val_loss: 23.6677\n",
      "Epoch 93/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 21.6126 - val_loss: 24.4707\n",
      "Epoch 94/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 21.2339 - val_loss: 23.3694\n",
      "Epoch 95/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 21.1775 - val_loss: 23.7832\n",
      "Epoch 96/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 21.9503 - val_loss: 22.9729\n",
      "Epoch 97/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 21.5684 - val_loss: 23.6914\n",
      "Epoch 98/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 21.4127 - val_loss: 23.4988\n",
      "Epoch 99/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 21.4838 - val_loss: 22.6774\n",
      "Epoch 100/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 21.1649 - val_loss: 23.7728\n",
      "Epoch 101/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 21.2121 - val_loss: 24.8649\n",
      "Epoch 102/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 21.5984 - val_loss: 23.1612\n",
      "Epoch 103/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 21.4289 - val_loss: 23.5003\n",
      "Epoch 104/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 21.3644 - val_loss: 23.7686\n",
      "Epoch 105/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 21.0897 - val_loss: 23.8562\n",
      "Epoch 106/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 21.3224 - val_loss: 23.9813\n",
      "Epoch 107/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 21.4410 - val_loss: 24.0538\n",
      "Epoch 108/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 21.6181 - val_loss: 23.4075\n",
      "Epoch 109/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 21.3551 - val_loss: 23.7581\n",
      "Epoch 110/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 21.0784 - val_loss: 24.2040\n",
      "Epoch 111/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 21.1946 - val_loss: 23.8411\n",
      "Epoch 112/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 21.1686 - val_loss: 23.9345\n",
      "Epoch 113/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.8157 - val_loss: 23.9092\n",
      "Epoch 114/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 21.0860 - val_loss: 23.7463\n",
      "Epoch 115/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 21.3654 - val_loss: 23.9055\n",
      "Epoch 116/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 20.7456 - val_loss: 23.4396\n",
      "Epoch 117/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 21.0623 - val_loss: 23.1955\n",
      "Epoch 118/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 21.3108 - val_loss: 24.3303\n",
      "Epoch 119/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 21.1628 - val_loss: 24.2116\n",
      "Epoch 120/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 21.3500 - val_loss: 23.5120\n",
      "Epoch 121/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 21.0705 - val_loss: 24.8978\n",
      "Epoch 122/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.7774 - val_loss: 23.4044\n",
      "Epoch 123/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.9079 - val_loss: 24.3066\n",
      "Epoch 124/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 21.0068 - val_loss: 23.6433\n",
      "Epoch 125/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 21.8580 - val_loss: 23.7216\n",
      "Epoch 126/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.3126 - val_loss: 23.0116\n",
      "Epoch 127/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 21.3356 - val_loss: 23.7586\n",
      "Epoch 128/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 21.0425 - val_loss: 23.2620\n",
      "Epoch 129/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.7388 - val_loss: 23.5971\n",
      "Epoch 130/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 21.0764 - val_loss: 22.9205\n",
      "Epoch 131/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.8137 - val_loss: 24.0509\n",
      "Epoch 132/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 21.2991 - val_loss: 23.9738\n",
      "Epoch 133/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.8183 - val_loss: 22.9835\n",
      "Epoch 134/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.6329 - val_loss: 23.6163\n",
      "Epoch 135/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 21.1216 - val_loss: 24.6905\n",
      "Epoch 136/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.9584 - val_loss: 23.5889\n",
      "Epoch 137/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.8394 - val_loss: 23.4897\n",
      "Epoch 138/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.3389 - val_loss: 24.5019\n",
      "Epoch 139/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.6889 - val_loss: 23.4060\n",
      "Epoch 140/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 21.1816 - val_loss: 24.1036\n",
      "Epoch 141/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.7081 - val_loss: 25.6275\n",
      "Epoch 142/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 21.8666 - val_loss: 22.5069\n",
      "Epoch 143/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.9098 - val_loss: 24.3332\n",
      "Epoch 144/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.6168 - val_loss: 23.7361\n",
      "Epoch 145/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 21.0173 - val_loss: 23.4101\n",
      "Epoch 146/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 20.4783 - val_loss: 25.5551\n",
      "Epoch 147/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 21.0043 - val_loss: 23.2020\n",
      "Epoch 148/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.5640 - val_loss: 23.5896\n",
      "Epoch 149/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.7628 - val_loss: 23.6089\n",
      "Epoch 150/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 21.7006 - val_loss: 24.1047\n",
      "Epoch 151/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.5722 - val_loss: 24.3773\n",
      "Epoch 152/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.5978 - val_loss: 22.9403\n",
      "Epoch 153/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.7566 - val_loss: 24.9126\n",
      "Epoch 154/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.9048 - val_loss: 23.3085\n",
      "Epoch 155/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.9269 - val_loss: 23.2166\n",
      "Epoch 156/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.3313 - val_loss: 24.4239\n",
      "Epoch 157/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 20.9534 - val_loss: 24.2172\n",
      "Epoch 158/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.6172 - val_loss: 22.7607\n",
      "Epoch 159/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.5375 - val_loss: 24.1063\n",
      "Epoch 160/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 21.6458 - val_loss: 22.8190\n",
      "Epoch 161/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 21.1462 - val_loss: 26.4270\n",
      "Epoch 162/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 21.2556 - val_loss: 23.6997\n",
      "Epoch 163/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.5470 - val_loss: 23.8972\n",
      "Epoch 164/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.3570 - val_loss: 22.8554\n",
      "Epoch 165/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.6719 - val_loss: 23.3760\n",
      "Epoch 166/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 20.5348 - val_loss: 24.2264\n",
      "Epoch 167/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.9448 - val_loss: 23.9609\n",
      "Epoch 168/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.6310 - val_loss: 23.5609\n",
      "Epoch 169/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 20.3667 - val_loss: 24.8636\n",
      "Epoch 170/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.2915 - val_loss: 23.5824\n",
      "Epoch 171/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.4267 - val_loss: 23.8734\n",
      "Epoch 172/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 21.1613 - val_loss: 23.1399\n",
      "Epoch 173/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 20.0095 - val_loss: 22.9901\n",
      "Epoch 174/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.0155 - val_loss: 23.9872\n",
      "Epoch 175/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.4791 - val_loss: 23.5624\n",
      "Epoch 176/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.5721 - val_loss: 23.9467\n",
      "Epoch 177/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.7728 - val_loss: 22.8003\n",
      "Epoch 178/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 20.7209 - val_loss: 23.3251\n",
      "Epoch 179/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.7518 - val_loss: 23.9455\n",
      "Epoch 180/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.8158 - val_loss: 22.9106\n",
      "Epoch 181/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.7222 - val_loss: 23.2359\n",
      "Epoch 182/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.3164 - val_loss: 23.5774\n",
      "Epoch 183/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.0963 - val_loss: 23.6083\n",
      "Epoch 184/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.9126 - val_loss: 23.2330\n",
      "Epoch 185/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 20.5186 - val_loss: 23.0403\n",
      "Epoch 186/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 21.0218 - val_loss: 23.6180\n",
      "Epoch 187/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.0400 - val_loss: 23.5931\n",
      "Epoch 188/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 20.7327 - val_loss: 23.1557\n",
      "Epoch 189/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.6228 - val_loss: 23.8965\n",
      "Epoch 190/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.4298 - val_loss: 23.1565\n",
      "Epoch 191/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.1855 - val_loss: 25.4447\n",
      "Epoch 192/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 21.3102 - val_loss: 23.5628\n",
      "Epoch 193/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 21.0285 - val_loss: 23.2474\n",
      "Epoch 194/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.9611 - val_loss: 23.8534\n",
      "Epoch 195/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.1535 - val_loss: 23.8531\n",
      "Epoch 196/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.3827 - val_loss: 23.0929\n",
      "Epoch 197/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.0386 - val_loss: 23.2078\n",
      "Epoch 198/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 20.4838 - val_loss: 24.1416\n",
      "Epoch 199/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.4485 - val_loss: 23.2710\n",
      "Epoch 200/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.4355 - val_loss: 23.5116\n",
      "Epoch 201/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.0485 - val_loss: 22.9236\n",
      "Epoch 202/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.1448 - val_loss: 23.3667\n",
      "Epoch 203/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.7808 - val_loss: 23.1062\n",
      "Epoch 204/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.0533 - val_loss: 24.4171\n",
      "Epoch 205/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.2906 - val_loss: 22.7695\n",
      "Epoch 206/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.1624 - val_loss: 22.9529\n",
      "Epoch 207/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.3290 - val_loss: 24.2200\n",
      "Epoch 208/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.5289 - val_loss: 23.9254\n",
      "Epoch 209/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.0396 - val_loss: 23.1530\n",
      "Epoch 210/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.9372 - val_loss: 22.6558\n",
      "Epoch 211/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 20.5452 - val_loss: 23.7690\n",
      "Epoch 212/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 20.3034 - val_loss: 23.1818\n",
      "Epoch 213/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.8178 - val_loss: 24.2071\n",
      "Epoch 214/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 20.1768 - val_loss: 23.3473\n",
      "Epoch 215/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.9443 - val_loss: 23.2697\n",
      "Epoch 216/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 20.3472 - val_loss: 23.4286\n",
      "Epoch 217/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.9465 - val_loss: 24.3589\n",
      "Epoch 218/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.3344 - val_loss: 22.9514\n",
      "Epoch 219/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.1325 - val_loss: 23.4126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 220/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.1596 - val_loss: 23.3101\n",
      "Epoch 221/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.9758 - val_loss: 24.1965\n",
      "Epoch 222/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.9059 - val_loss: 23.5953\n",
      "Epoch 223/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.9585 - val_loss: 23.8945\n",
      "Epoch 224/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.9067 - val_loss: 23.5066\n",
      "Epoch 225/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.6718 - val_loss: 22.6506\n",
      "Epoch 226/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.8877 - val_loss: 23.9794\n",
      "Epoch 227/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.1830 - val_loss: 23.0773\n",
      "Epoch 228/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 20.2232 - val_loss: 23.2035\n",
      "Epoch 229/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 20.0382 - val_loss: 23.3808\n",
      "Epoch 230/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.6944 - val_loss: 23.2191\n",
      "Epoch 231/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 20.1064 - val_loss: 23.0850\n",
      "Epoch 232/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.6907 - val_loss: 23.7952\n",
      "Epoch 233/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.6972 - val_loss: 23.4833\n",
      "Epoch 234/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.8265 - val_loss: 22.8393\n",
      "Epoch 235/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.8781 - val_loss: 24.8746\n",
      "Epoch 236/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.1578 - val_loss: 22.5102\n",
      "Epoch 237/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.5047 - val_loss: 23.4552\n",
      "Epoch 238/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.0157 - val_loss: 23.4579\n",
      "Epoch 239/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.4734 - val_loss: 22.9655\n",
      "Epoch 240/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.6788 - val_loss: 22.9826\n",
      "Epoch 241/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.8648 - val_loss: 23.2456\n",
      "Epoch 242/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.6685 - val_loss: 23.5989\n",
      "Epoch 243/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.8306 - val_loss: 24.0845\n",
      "Epoch 244/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 19.8713 - val_loss: 24.5987\n",
      "Epoch 245/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 19.6604 - val_loss: 23.4344\n",
      "Epoch 246/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.9769 - val_loss: 23.6771\n",
      "Epoch 247/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.5703 - val_loss: 24.5453\n",
      "Epoch 248/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.2755 - val_loss: 24.3605\n",
      "Epoch 249/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.7556 - val_loss: 22.9732\n",
      "Epoch 250/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 20.2718 - val_loss: 24.9030\n",
      "Epoch 251/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.7182 - val_loss: 23.6743\n",
      "Epoch 252/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.2683 - val_loss: 23.7959\n",
      "Epoch 253/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.8233 - val_loss: 23.1655\n",
      "Epoch 254/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.8204 - val_loss: 24.3527\n",
      "Epoch 255/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.9811 - val_loss: 24.1474\n",
      "Epoch 256/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.7229 - val_loss: 22.6895\n",
      "Epoch 257/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.8821 - val_loss: 24.1951\n",
      "Epoch 258/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.4111 - val_loss: 23.3052\n",
      "Epoch 259/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.3628 - val_loss: 23.6318\n",
      "Epoch 260/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.5119 - val_loss: 22.9957\n",
      "Epoch 261/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.3103 - val_loss: 23.1057\n",
      "Epoch 262/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.2446 - val_loss: 22.6888\n",
      "Epoch 263/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.8622 - val_loss: 22.8416\n",
      "Epoch 264/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.3564 - val_loss: 23.4009\n",
      "Epoch 265/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.8950 - val_loss: 22.9612\n",
      "Epoch 266/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.1959 - val_loss: 24.7962\n",
      "Epoch 267/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.0255 - val_loss: 24.4843\n",
      "Epoch 268/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.5484 - val_loss: 23.1119\n",
      "Epoch 269/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 20.1462 - val_loss: 23.3122\n",
      "Epoch 270/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 19.8682 - val_loss: 23.2276\n",
      "Epoch 271/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.8293 - val_loss: 24.0265\n",
      "Epoch 272/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.4670 - val_loss: 24.0442\n",
      "Epoch 273/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.4515 - val_loss: 22.5385\n",
      "Epoch 274/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.9338 - val_loss: 23.3114\n",
      "Epoch 275/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 19.9780 - val_loss: 23.8890\n",
      "Epoch 276/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.8604 - val_loss: 23.6726\n",
      "Epoch 277/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 19.4260 - val_loss: 23.3721\n",
      "Epoch 278/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.4042 - val_loss: 23.3664\n",
      "Epoch 279/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.6633 - val_loss: 22.7921\n",
      "Epoch 280/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.7290 - val_loss: 23.2744\n",
      "Epoch 281/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.8405 - val_loss: 23.3628\n",
      "Epoch 282/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.5333 - val_loss: 23.6911\n",
      "Epoch 283/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 19.7934 - val_loss: 24.7819\n",
      "Epoch 284/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.2673 - val_loss: 22.4858\n",
      "Epoch 285/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.4015 - val_loss: 24.1594\n",
      "Epoch 286/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 19.8098 - val_loss: 23.8708\n",
      "Epoch 287/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.9391 - val_loss: 23.8759\n",
      "Epoch 288/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.3348 - val_loss: 24.8378\n",
      "Epoch 289/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.5677 - val_loss: 24.3268\n",
      "Epoch 290/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.5993 - val_loss: 23.8414\n",
      "Epoch 291/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.0194 - val_loss: 22.8573\n",
      "Epoch 292/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.5942 - val_loss: 23.0383\n",
      "Epoch 293/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.4050 - val_loss: 22.7046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 294/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.4788 - val_loss: 23.3235\n",
      "Epoch 295/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.4030 - val_loss: 23.4585\n",
      "Epoch 296/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.2237 - val_loss: 23.1721\n",
      "Epoch 297/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.3411 - val_loss: 23.6062\n",
      "Epoch 298/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.7667 - val_loss: 24.0784\n",
      "Epoch 299/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.6527 - val_loss: 24.6140\n",
      "Epoch 300/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 20.5099 - val_loss: 23.2465\n",
      "Epoch 301/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.5792 - val_loss: 24.0228\n",
      "Epoch 302/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.5066 - val_loss: 23.7213\n",
      "Epoch 303/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 19.7433 - val_loss: 24.0601\n",
      "Epoch 304/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.4044 - val_loss: 23.2854\n",
      "Epoch 305/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.2547 - val_loss: 24.0761\n",
      "Epoch 306/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.5874 - val_loss: 22.7500\n",
      "Epoch 307/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.6766 - val_loss: 23.1381\n",
      "Epoch 308/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.7162 - val_loss: 23.2198\n",
      "Epoch 309/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.3384 - val_loss: 23.4297\n",
      "Epoch 310/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.6278 - val_loss: 23.4647\n",
      "Epoch 311/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.5509 - val_loss: 24.3954\n",
      "Epoch 312/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.6656 - val_loss: 23.2350\n",
      "Epoch 313/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.3828 - val_loss: 23.4869\n",
      "Epoch 314/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 19.2398 - val_loss: 23.4036\n",
      "Epoch 315/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.2797 - val_loss: 23.7269\n",
      "Epoch 316/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.9941 - val_loss: 23.0550\n",
      "Epoch 317/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.2974 - val_loss: 22.9271\n",
      "Epoch 318/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.2111 - val_loss: 22.9733\n",
      "Epoch 319/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.4123 - val_loss: 24.1552\n",
      "Epoch 320/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.4417 - val_loss: 24.1685\n",
      "Epoch 321/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.6250 - val_loss: 22.8162\n",
      "Epoch 322/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.2056 - val_loss: 22.7573\n",
      "Epoch 323/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.3750 - val_loss: 23.8688\n",
      "Epoch 324/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.1418 - val_loss: 22.9787\n",
      "Epoch 325/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.5527 - val_loss: 23.7324\n",
      "Epoch 326/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.6222 - val_loss: 23.1923\n",
      "Epoch 327/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.7652 - val_loss: 23.1511\n",
      "Epoch 328/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.5767 - val_loss: 24.6535\n",
      "Epoch 329/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.2023 - val_loss: 23.6349\n",
      "Epoch 330/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.4594 - val_loss: 23.4916\n",
      "Epoch 331/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.2788 - val_loss: 22.6293\n",
      "Epoch 332/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.8691 - val_loss: 22.7223\n",
      "Epoch 333/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.1182 - val_loss: 23.0256\n",
      "Epoch 334/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.4033 - val_loss: 23.8514\n",
      "Epoch 335/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.4121 - val_loss: 23.6673\n",
      "Epoch 336/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.3810 - val_loss: 23.1211\n",
      "Epoch 337/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.5401 - val_loss: 24.3868\n",
      "Epoch 338/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.5732 - val_loss: 22.7065\n",
      "Epoch 339/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.8831 - val_loss: 22.8029\n",
      "Epoch 340/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.8542 - val_loss: 22.2559\n",
      "Epoch 341/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.2918 - val_loss: 22.7127\n",
      "Epoch 342/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.4089 - val_loss: 23.4173\n",
      "Epoch 343/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.5524 - val_loss: 24.7895\n",
      "Epoch 344/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.6134 - val_loss: 22.8916\n",
      "Epoch 345/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.0966 - val_loss: 23.7512\n",
      "Epoch 346/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.6182 - val_loss: 22.6441\n",
      "Epoch 347/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.3603 - val_loss: 23.1903\n",
      "Epoch 348/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 19.5521 - val_loss: 22.6915\n",
      "Epoch 349/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.8882 - val_loss: 23.6147\n",
      "Epoch 350/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.1831 - val_loss: 22.4935\n",
      "Epoch 351/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.0800 - val_loss: 23.1443\n",
      "Epoch 352/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.0684 - val_loss: 22.5095\n",
      "Epoch 353/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.2416 - val_loss: 23.2243\n",
      "Epoch 354/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.1193 - val_loss: 23.8272\n",
      "Epoch 355/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.1814 - val_loss: 24.8026\n",
      "Epoch 356/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.3818 - val_loss: 23.3105\n",
      "Epoch 357/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.0025 - val_loss: 23.5313\n",
      "Epoch 358/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.0310 - val_loss: 23.9759\n",
      "Epoch 359/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.1924 - val_loss: 22.4478\n",
      "Epoch 360/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.0003 - val_loss: 23.5264\n",
      "Epoch 361/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.1936 - val_loss: 23.1895\n",
      "Epoch 362/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.2703 - val_loss: 23.1117\n",
      "Epoch 363/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.2417 - val_loss: 24.3317\n",
      "Epoch 364/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 19.3325 - val_loss: 23.0355\n",
      "Epoch 365/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.2187 - val_loss: 23.1257\n",
      "Epoch 366/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.5599 - val_loss: 24.5124\n",
      "Epoch 367/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.3462 - val_loss: 22.0808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 368/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.1069 - val_loss: 23.3664\n",
      "Epoch 369/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.8639 - val_loss: 24.5294\n",
      "Epoch 370/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 19.4820 - val_loss: 22.6773\n",
      "Epoch 371/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.2062 - val_loss: 23.6895\n",
      "Epoch 372/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.6215 - val_loss: 23.0579\n",
      "Epoch 373/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.1823 - val_loss: 22.9789\n",
      "Epoch 374/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.9636 - val_loss: 24.4629\n",
      "Epoch 375/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.0164 - val_loss: 26.4902\n",
      "Epoch 376/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.9813 - val_loss: 22.3793\n",
      "Epoch 377/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.3334 - val_loss: 22.6459\n",
      "Epoch 378/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.2830 - val_loss: 23.5829\n",
      "Epoch 379/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.6840 - val_loss: 23.1447\n",
      "Epoch 380/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.2512 - val_loss: 23.5637\n",
      "Epoch 381/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.1029 - val_loss: 22.5584\n",
      "Epoch 382/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.5716 - val_loss: 22.8857\n",
      "Epoch 383/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.4365 - val_loss: 24.0840\n",
      "Epoch 384/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.3360 - val_loss: 23.2723\n",
      "Epoch 385/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.9052 - val_loss: 23.3019\n",
      "Epoch 386/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.3480 - val_loss: 23.1444\n",
      "Epoch 387/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.3090 - val_loss: 22.5462\n",
      "Epoch 388/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.5098 - val_loss: 23.4518\n",
      "Epoch 389/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.9462 - val_loss: 23.2311\n",
      "Epoch 390/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.0700 - val_loss: 23.7710\n",
      "Epoch 391/2000\n",
      "1500/1500 [==============================] - 0s 19us/sample - loss: 19.1014 - val_loss: 23.0460\n",
      "Epoch 392/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.6893 - val_loss: 23.8928\n",
      "Epoch 393/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.8482 - val_loss: 22.5975\n",
      "Epoch 394/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.9040 - val_loss: 23.1067\n",
      "Epoch 395/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.8562 - val_loss: 23.0897\n",
      "Epoch 396/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.7199 - val_loss: 23.8939\n",
      "Epoch 397/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 19.7705 - val_loss: 22.8777\n",
      "Epoch 398/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 19.1468 - val_loss: 22.5919\n",
      "Epoch 399/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.8548 - val_loss: 23.1135\n",
      "Epoch 400/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.2693 - val_loss: 22.7881\n",
      "Epoch 401/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.7441 - val_loss: 23.4679\n",
      "Epoch 402/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.5543 - val_loss: 23.8456\n",
      "Epoch 403/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.9388 - val_loss: 22.6333\n",
      "Epoch 404/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.8561 - val_loss: 22.6446\n",
      "Epoch 405/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.0150 - val_loss: 22.9082\n",
      "Epoch 406/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.0974 - val_loss: 23.6012\n",
      "Epoch 407/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.9611 - val_loss: 22.5919\n",
      "Epoch 408/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.1812 - val_loss: 23.2329\n",
      "Epoch 409/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.9075 - val_loss: 23.4817\n",
      "Epoch 410/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.9140 - val_loss: 23.6754\n",
      "Epoch 411/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.9032 - val_loss: 22.8517\n",
      "Epoch 412/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.1959 - val_loss: 23.4075\n",
      "Epoch 413/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.9943 - val_loss: 22.5866\n",
      "Epoch 414/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.2247 - val_loss: 24.1320\n",
      "Epoch 415/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.9744 - val_loss: 23.0029\n",
      "Epoch 416/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.1669 - val_loss: 22.4992\n",
      "Epoch 417/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.1257 - val_loss: 22.8158\n",
      "Epoch 418/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.5642 - val_loss: 24.1239\n",
      "Epoch 419/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 19.4821 - val_loss: 22.9965\n",
      "Epoch 420/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.1857 - val_loss: 23.1961\n",
      "Epoch 421/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.9670 - val_loss: 23.3695\n",
      "Epoch 422/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.6188 - val_loss: 23.3419\n",
      "Epoch 423/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.3296 - val_loss: 23.2381\n",
      "Epoch 424/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.0520 - val_loss: 23.9201\n",
      "Epoch 425/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.6924 - val_loss: 23.4946\n",
      "Epoch 426/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.0053 - val_loss: 22.7485\n",
      "Epoch 427/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.6377 - val_loss: 22.7644\n",
      "Epoch 428/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.3492 - val_loss: 25.0349\n",
      "Epoch 429/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.3682 - val_loss: 23.1622\n",
      "Epoch 430/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.5810 - val_loss: 22.6936\n",
      "Epoch 431/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.9031 - val_loss: 23.7671\n",
      "Epoch 432/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.0286 - val_loss: 23.0470\n",
      "Epoch 433/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.5167 - val_loss: 23.0430\n",
      "Epoch 434/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.4312 - val_loss: 22.7572\n",
      "Epoch 435/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.4701 - val_loss: 24.2192\n",
      "Epoch 436/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.7209 - val_loss: 22.7427\n",
      "Epoch 437/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.7399 - val_loss: 23.2758\n",
      "Epoch 438/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.7170 - val_loss: 22.2867\n",
      "Epoch 439/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.7370 - val_loss: 23.2786\n",
      "Epoch 440/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.9602 - val_loss: 22.5529\n",
      "Epoch 441/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.3542 - val_loss: 22.7275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 442/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.8622 - val_loss: 22.5462\n",
      "Epoch 443/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.4100 - val_loss: 22.7642\n",
      "Epoch 444/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.0457 - val_loss: 23.1611\n",
      "Epoch 445/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.0628 - val_loss: 23.2238\n",
      "Epoch 446/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.5891 - val_loss: 23.3216\n",
      "Epoch 447/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.1317 - val_loss: 23.9821\n",
      "Epoch 448/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.9990 - val_loss: 23.0626\n",
      "Epoch 449/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.5179 - val_loss: 24.1323\n",
      "Epoch 450/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.3461 - val_loss: 23.3636\n",
      "Epoch 451/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 18.2101 - val_loss: 23.6977\n",
      "Epoch 452/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.9212 - val_loss: 22.4938\n",
      "Epoch 453/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 19.0131 - val_loss: 23.5539\n",
      "Epoch 454/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.6518 - val_loss: 22.6709\n",
      "Epoch 455/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.5733 - val_loss: 23.7804\n",
      "Epoch 456/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.8609 - val_loss: 22.7448\n",
      "Epoch 457/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.7173 - val_loss: 23.6279\n",
      "Epoch 458/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.0335 - val_loss: 24.3005\n",
      "Epoch 459/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.9750 - val_loss: 22.9285\n",
      "Epoch 460/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.6068 - val_loss: 23.1632\n",
      "Epoch 461/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.9489 - val_loss: 22.7442\n",
      "Epoch 462/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.4150 - val_loss: 23.4179\n",
      "Epoch 463/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.1374 - val_loss: 23.4521\n",
      "Epoch 464/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.1192 - val_loss: 23.1859\n",
      "Epoch 465/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.8180 - val_loss: 24.7149\n",
      "Epoch 466/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.7586 - val_loss: 23.8883\n",
      "Epoch 467/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.7697 - val_loss: 22.8918\n",
      "Epoch 468/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.1567 - val_loss: 22.0649\n",
      "Epoch 469/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.8524 - val_loss: 23.8175\n",
      "Epoch 470/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.5615 - val_loss: 22.3845\n",
      "Epoch 471/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.8915 - val_loss: 23.3910\n",
      "Epoch 472/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.3924 - val_loss: 22.7331\n",
      "Epoch 473/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.3903 - val_loss: 22.7604\n",
      "Epoch 474/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.5877 - val_loss: 23.3793\n",
      "Epoch 475/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.9465 - val_loss: 23.2038\n",
      "Epoch 476/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.6180 - val_loss: 22.8188\n",
      "Epoch 477/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.6427 - val_loss: 22.3856\n",
      "Epoch 478/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.3909 - val_loss: 22.2692\n",
      "Epoch 479/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.5143 - val_loss: 22.8583\n",
      "Epoch 480/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 18.6904 - val_loss: 22.8145\n",
      "Epoch 481/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 19.1323 - val_loss: 22.8329\n",
      "Epoch 482/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.4785 - val_loss: 23.2760\n",
      "Epoch 483/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.6841 - val_loss: 23.8998\n",
      "Epoch 484/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 18.4010 - val_loss: 24.2267\n",
      "Epoch 485/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.1923 - val_loss: 22.9353\n",
      "Epoch 486/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.0119 - val_loss: 22.2616\n",
      "Epoch 487/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.6400 - val_loss: 22.5092\n",
      "Epoch 488/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.0926 - val_loss: 24.3717\n",
      "Epoch 489/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.5314 - val_loss: 22.6881\n",
      "Epoch 490/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.6825 - val_loss: 23.1908\n",
      "Epoch 491/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.5373 - val_loss: 24.1184\n",
      "Epoch 492/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.8005 - val_loss: 22.8125\n",
      "Epoch 493/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.8732 - val_loss: 22.9126\n",
      "Epoch 494/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.2452 - val_loss: 22.8116\n",
      "Epoch 495/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.0774 - val_loss: 23.9227\n",
      "Epoch 496/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.7744 - val_loss: 23.5509\n",
      "Epoch 497/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.5265 - val_loss: 23.0673\n",
      "Epoch 498/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.2900 - val_loss: 22.4787\n",
      "Epoch 499/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.5401 - val_loss: 22.9480\n",
      "Epoch 500/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.7956 - val_loss: 22.7372\n",
      "Epoch 501/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 18.4854 - val_loss: 22.9140\n",
      "Epoch 502/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.2410 - val_loss: 23.1147\n",
      "Epoch 503/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.2153 - val_loss: 22.3945\n",
      "Epoch 504/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.9299 - val_loss: 22.7206\n",
      "Epoch 505/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.7546 - val_loss: 22.3943\n",
      "Epoch 506/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.1735 - val_loss: 23.3317\n",
      "Epoch 507/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 18.0558 - val_loss: 22.1366\n",
      "Epoch 508/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.7391 - val_loss: 23.8287\n",
      "Epoch 509/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.2333 - val_loss: 23.5058\n",
      "Epoch 510/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.5490 - val_loss: 24.3330\n",
      "Epoch 511/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.8119 - val_loss: 24.9986\n",
      "Epoch 512/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.6549 - val_loss: 23.1838\n",
      "Epoch 513/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.4315 - val_loss: 22.7746\n",
      "Epoch 514/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.2351 - val_loss: 22.7039\n",
      "Epoch 515/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.2074 - val_loss: 22.9226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 516/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 18.6276 - val_loss: 22.8510\n",
      "Epoch 517/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.1173 - val_loss: 24.4862\n",
      "Epoch 518/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.2199 - val_loss: 24.0860\n",
      "Epoch 519/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.6722 - val_loss: 23.3711\n",
      "Epoch 520/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 19.3828 - val_loss: 22.6427\n",
      "Epoch 521/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 18.5037 - val_loss: 23.4808\n",
      "Epoch 522/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.4955 - val_loss: 22.8362\n",
      "Epoch 523/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.3388 - val_loss: 23.2930\n",
      "Epoch 524/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.1416 - val_loss: 22.7313\n",
      "Epoch 525/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.4349 - val_loss: 23.3399\n",
      "Epoch 526/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.8449 - val_loss: 23.1867\n",
      "Epoch 527/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.2488 - val_loss: 22.3980\n",
      "Epoch 528/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.8670 - val_loss: 23.1872\n",
      "Epoch 529/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.3639 - val_loss: 22.4865\n",
      "Epoch 530/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.1822 - val_loss: 22.8577\n",
      "Epoch 531/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 18.2971 - val_loss: 22.7161\n",
      "Epoch 532/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.4535 - val_loss: 22.4023\n",
      "Epoch 533/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.4528 - val_loss: 22.9128\n",
      "Epoch 534/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.6352 - val_loss: 23.4203\n",
      "Epoch 535/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.6793 - val_loss: 23.7253\n",
      "Epoch 536/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 18.4563 - val_loss: 22.8189\n",
      "Epoch 537/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.8933 - val_loss: 23.3737\n",
      "Epoch 538/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.6897 - val_loss: 23.1483\n",
      "Epoch 539/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.5799 - val_loss: 23.7721\n",
      "Epoch 540/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.9973 - val_loss: 23.4445\n",
      "Epoch 541/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.5089 - val_loss: 23.1490\n",
      "Epoch 542/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.3444 - val_loss: 23.4789\n",
      "Epoch 543/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.3583 - val_loss: 22.7235\n",
      "Epoch 544/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.0638 - val_loss: 23.2298\n",
      "Epoch 545/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.0383 - val_loss: 22.2948\n",
      "Epoch 546/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.3229 - val_loss: 22.8089\n",
      "Epoch 547/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.5923 - val_loss: 22.7677\n",
      "Epoch 548/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.7295 - val_loss: 23.2299\n",
      "Epoch 549/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 18.4761 - val_loss: 23.3599\n",
      "Epoch 550/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.2094 - val_loss: 23.5963\n",
      "Epoch 551/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.1527 - val_loss: 22.3686\n",
      "Epoch 552/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.7708 - val_loss: 23.6927\n",
      "Epoch 553/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.2060 - val_loss: 22.8754\n",
      "Epoch 554/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.9678 - val_loss: 23.1376\n",
      "Epoch 555/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.0920 - val_loss: 23.1039\n",
      "Epoch 556/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.8640 - val_loss: 22.4125\n",
      "Epoch 557/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.7798 - val_loss: 23.9606\n",
      "Epoch 558/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.4198 - val_loss: 22.7224\n",
      "Epoch 559/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.9489 - val_loss: 22.4007\n",
      "Epoch 560/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.2188 - val_loss: 22.1369\n",
      "Epoch 561/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.5582 - val_loss: 22.7016\n",
      "Epoch 562/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.3551 - val_loss: 23.1133\n",
      "Epoch 563/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 18.2396 - val_loss: 23.4371\n",
      "Epoch 564/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 18.1708 - val_loss: 22.6481\n",
      "Epoch 565/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.1466 - val_loss: 23.0484\n",
      "Epoch 566/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 18.2377 - val_loss: 22.9014\n",
      "Epoch 567/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 18.2794 - val_loss: 22.4081\n",
      "Epoch 568/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.9497 - val_loss: 22.7842\n",
      "Epoch 569/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.8647 - val_loss: 22.6770\n",
      "Epoch 570/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.7419 - val_loss: 24.0280\n",
      "Epoch 571/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.2630 - val_loss: 23.9745\n",
      "Epoch 572/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.2924 - val_loss: 22.4890\n",
      "Epoch 573/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.2284 - val_loss: 23.1045\n",
      "Epoch 574/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.6742 - val_loss: 23.2095\n",
      "Epoch 575/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.1032 - val_loss: 23.7866\n",
      "Epoch 576/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.1913 - val_loss: 23.4059\n",
      "Epoch 577/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.6590 - val_loss: 22.5349\n",
      "Epoch 578/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.5419 - val_loss: 22.7766\n",
      "Epoch 579/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 17.9915 - val_loss: 23.1075\n",
      "Epoch 580/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.9826 - val_loss: 22.9120\n",
      "Epoch 581/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.4806 - val_loss: 22.4845\n",
      "Epoch 582/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 18.5178 - val_loss: 22.3213\n",
      "Epoch 583/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.0056 - val_loss: 23.4319\n",
      "Epoch 584/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.1387 - val_loss: 22.6079\n",
      "Epoch 585/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.3441 - val_loss: 23.5099\n",
      "Epoch 586/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.8960 - val_loss: 23.2937\n",
      "Epoch 587/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.2236 - val_loss: 23.6969\n",
      "Epoch 588/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.8804 - val_loss: 23.1674\n",
      "Epoch 589/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.0807 - val_loss: 23.5743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 590/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 18.1069 - val_loss: 22.9210\n",
      "Epoch 591/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.1258 - val_loss: 23.2458\n",
      "Epoch 592/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.3687 - val_loss: 22.9821\n",
      "Epoch 593/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.2961 - val_loss: 23.7085\n",
      "Epoch 594/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.4314 - val_loss: 24.1247\n",
      "Epoch 595/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.2543 - val_loss: 22.5916\n",
      "Epoch 596/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 18.2322 - val_loss: 23.0732\n",
      "Epoch 597/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.6184 - val_loss: 22.6501\n",
      "Epoch 598/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.2842 - val_loss: 23.1826\n",
      "Epoch 599/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.8317 - val_loss: 23.4656\n",
      "Epoch 600/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.0498 - val_loss: 24.4079\n",
      "Epoch 601/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.5181 - val_loss: 23.1760\n",
      "Epoch 602/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.3276 - val_loss: 24.0399\n",
      "Epoch 603/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.6574 - val_loss: 22.4133\n",
      "Epoch 604/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.1309 - val_loss: 23.0951\n",
      "Epoch 605/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 17.9561 - val_loss: 23.0435\n",
      "Epoch 606/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.8310 - val_loss: 23.6928\n",
      "Epoch 607/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.0548 - val_loss: 23.1727\n",
      "Epoch 608/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.0003 - val_loss: 22.9215\n",
      "Epoch 609/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.2124 - val_loss: 22.1315\n",
      "Epoch 610/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.0315 - val_loss: 22.5406\n",
      "Epoch 611/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.1292 - val_loss: 23.7772\n",
      "Epoch 612/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.5161 - val_loss: 22.7988\n",
      "Epoch 613/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 17.8056 - val_loss: 22.2343\n",
      "Epoch 614/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.7817 - val_loss: 22.4311\n",
      "Epoch 615/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 17.7493 - val_loss: 23.1324\n",
      "Epoch 616/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.2153 - val_loss: 24.0294\n",
      "Epoch 617/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.1566 - val_loss: 23.3224\n",
      "Epoch 618/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.7379 - val_loss: 23.4990\n",
      "Epoch 619/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.9114 - val_loss: 24.0333\n",
      "Epoch 620/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.8386 - val_loss: 23.5741\n",
      "Epoch 621/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.6590 - val_loss: 21.9209\n",
      "Epoch 622/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 18.4256 - val_loss: 23.6208\n",
      "Epoch 623/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 18.4609 - val_loss: 22.9267\n",
      "Epoch 624/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 18.6064 - val_loss: 22.9176\n",
      "Epoch 625/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.6901 - val_loss: 22.5288\n",
      "Epoch 626/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.8695 - val_loss: 22.5199\n",
      "Epoch 627/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.5817 - val_loss: 22.1421\n",
      "Epoch 628/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.9737 - val_loss: 22.4870\n",
      "Epoch 629/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.4300 - val_loss: 24.0595\n",
      "Epoch 630/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.8408 - val_loss: 22.5071\n",
      "Epoch 631/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.8080 - val_loss: 22.9183\n",
      "Epoch 632/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 17.9556 - val_loss: 22.6770\n",
      "Epoch 633/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.8897 - val_loss: 23.5244\n",
      "Epoch 634/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.4097 - val_loss: 24.0707\n",
      "Epoch 635/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.4729 - val_loss: 22.8457\n",
      "Epoch 636/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.6592 - val_loss: 22.3966\n",
      "Epoch 637/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.4613 - val_loss: 22.3896\n",
      "Epoch 638/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.0368 - val_loss: 22.7618\n",
      "Epoch 639/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.6876 - val_loss: 22.5583\n",
      "Epoch 640/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.6382 - val_loss: 22.1385\n",
      "Epoch 641/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.7906 - val_loss: 22.2087\n",
      "Epoch 642/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.9768 - val_loss: 22.9236\n",
      "Epoch 643/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.5040 - val_loss: 24.3938\n",
      "Epoch 644/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.8504 - val_loss: 23.4783\n",
      "Epoch 645/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.2293 - val_loss: 22.9610\n",
      "Epoch 646/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.7900 - val_loss: 23.4240\n",
      "Epoch 647/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.2800 - val_loss: 22.5935\n",
      "Epoch 648/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.5849 - val_loss: 22.6790\n",
      "Epoch 649/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.1444 - val_loss: 24.2922\n",
      "Epoch 650/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.9862 - val_loss: 23.0583\n",
      "Epoch 651/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.0401 - val_loss: 23.7776\n",
      "Epoch 652/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.0713 - val_loss: 22.7554\n",
      "Epoch 653/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.8566 - val_loss: 24.6447\n",
      "Epoch 654/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.3811 - val_loss: 23.1190\n",
      "Epoch 655/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 17.7581 - val_loss: 22.7690\n",
      "Epoch 656/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.4348 - val_loss: 22.4492\n",
      "Epoch 657/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.5205 - val_loss: 23.2069\n",
      "Epoch 658/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.9478 - val_loss: 23.2359\n",
      "Epoch 659/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.7070 - val_loss: 22.9416\n",
      "Epoch 660/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.7122 - val_loss: 22.4339\n",
      "Epoch 661/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.7566 - val_loss: 22.4056\n",
      "Epoch 662/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.9109 - val_loss: 22.5847\n",
      "Epoch 663/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.7718 - val_loss: 22.7861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 664/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.9788 - val_loss: 23.2709\n",
      "Epoch 665/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.5868 - val_loss: 23.6658\n",
      "Epoch 666/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.6051 - val_loss: 23.4434\n",
      "Epoch 667/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.6607 - val_loss: 22.6952\n",
      "Epoch 668/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.3322 - val_loss: 22.8620\n",
      "Epoch 669/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.5795 - val_loss: 22.7570\n",
      "Epoch 670/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.3117 - val_loss: 22.4585\n",
      "Epoch 671/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.6449 - val_loss: 23.2694\n",
      "Epoch 672/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.7041 - val_loss: 22.8978\n",
      "Epoch 673/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.6192 - val_loss: 22.7535\n",
      "Epoch 674/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.5178 - val_loss: 23.3469\n",
      "Epoch 675/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.4640 - val_loss: 22.5318\n",
      "Epoch 676/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.6871 - val_loss: 22.3431\n",
      "Epoch 677/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.3520 - val_loss: 22.3660\n",
      "Epoch 678/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.8868 - val_loss: 23.0915\n",
      "Epoch 679/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.1289 - val_loss: 22.7582\n",
      "Epoch 680/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.3224 - val_loss: 23.0764\n",
      "Epoch 681/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.5533 - val_loss: 22.5708\n",
      "Epoch 682/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 18.4498 - val_loss: 22.6717\n",
      "Epoch 683/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 18.9863 - val_loss: 23.8114\n",
      "Epoch 684/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.5049 - val_loss: 22.3346\n",
      "Epoch 685/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 17.5047 - val_loss: 22.5776\n",
      "Epoch 686/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.8343 - val_loss: 22.7749\n",
      "Epoch 687/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.3821 - val_loss: 22.5880\n",
      "Epoch 688/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.5707 - val_loss: 22.8968\n",
      "Epoch 689/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.5378 - val_loss: 23.7454\n",
      "Epoch 690/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.3992 - val_loss: 22.2875\n",
      "Epoch 691/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.9854 - val_loss: 23.8571\n",
      "Epoch 692/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.1909 - val_loss: 23.3740\n",
      "Epoch 693/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 18.4316 - val_loss: 22.5873\n",
      "Epoch 694/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.2945 - val_loss: 22.4479\n",
      "Epoch 695/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.0810 - val_loss: 22.1106\n",
      "Epoch 696/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.7766 - val_loss: 22.7085\n",
      "Epoch 697/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.1037 - val_loss: 23.5595\n",
      "Epoch 698/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.2008 - val_loss: 23.7313\n",
      "Epoch 699/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.6929 - val_loss: 22.7139\n",
      "Epoch 700/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.5628 - val_loss: 23.2749\n",
      "Epoch 701/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.2966 - val_loss: 22.2339\n",
      "Epoch 702/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.4329 - val_loss: 22.5566\n",
      "Epoch 703/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 17.5423 - val_loss: 22.8776\n",
      "Epoch 704/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 17.8975 - val_loss: 22.5572\n",
      "Epoch 705/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.5704 - val_loss: 23.4978\n",
      "Epoch 706/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.3621 - val_loss: 22.9910\n",
      "Epoch 707/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.4675 - val_loss: 24.2046\n",
      "Epoch 708/2000\n",
      "1500/1500 [==============================] - 0s 19us/sample - loss: 18.1047 - val_loss: 23.1391\n",
      "Epoch 709/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.1066 - val_loss: 22.8073\n",
      "Epoch 710/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.8617 - val_loss: 23.1775\n",
      "Epoch 711/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.7047 - val_loss: 22.1016\n",
      "Epoch 712/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.6335 - val_loss: 22.9513\n",
      "Epoch 713/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.6396 - val_loss: 24.2254\n",
      "Epoch 714/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.2240 - val_loss: 22.6432\n",
      "Epoch 715/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.1939 - val_loss: 22.6280\n",
      "Epoch 716/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.3088 - val_loss: 22.0435\n",
      "Epoch 717/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.4828 - val_loss: 22.3861\n",
      "Epoch 718/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 17.3814 - val_loss: 22.0862\n",
      "Epoch 719/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.3151 - val_loss: 22.8177\n",
      "Epoch 720/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.4200 - val_loss: 22.4899\n",
      "Epoch 721/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.4014 - val_loss: 23.0668\n",
      "Epoch 722/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.9313 - val_loss: 21.8679\n",
      "Epoch 723/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.8565 - val_loss: 22.2791\n",
      "Epoch 724/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 17.9404 - val_loss: 22.2491\n",
      "Epoch 725/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.4392 - val_loss: 22.0368\n",
      "Epoch 726/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 17.5489 - val_loss: 22.8319\n",
      "Epoch 727/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.5762 - val_loss: 25.5744\n",
      "Epoch 728/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.0022 - val_loss: 22.2080\n",
      "Epoch 729/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.4158 - val_loss: 22.6374\n",
      "Epoch 730/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.7309 - val_loss: 22.4748\n",
      "Epoch 731/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.8754 - val_loss: 23.3363\n",
      "Epoch 732/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 17.7136 - val_loss: 22.2117\n",
      "Epoch 733/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.4786 - val_loss: 22.1253\n",
      "Epoch 734/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.4140 - val_loss: 22.8159\n",
      "Epoch 735/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.3721 - val_loss: 22.5763\n",
      "Epoch 736/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.3465 - val_loss: 23.3051\n",
      "Epoch 737/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.9596 - val_loss: 24.1748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 738/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.0023 - val_loss: 24.2328\n",
      "Epoch 739/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.7293 - val_loss: 22.5669\n",
      "Epoch 740/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.7458 - val_loss: 23.0524\n",
      "Epoch 741/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.2909 - val_loss: 24.3019\n",
      "Epoch 742/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.4398 - val_loss: 22.3356\n",
      "Epoch 743/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.3785 - val_loss: 22.9834\n",
      "Epoch 744/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.4204 - val_loss: 23.0140\n",
      "Epoch 745/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.7196 - val_loss: 22.6840\n",
      "Epoch 746/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.1910 - val_loss: 23.2781\n",
      "Epoch 747/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.6746 - val_loss: 22.8512\n",
      "Epoch 748/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.6106 - val_loss: 22.6344\n",
      "Epoch 749/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.7430 - val_loss: 24.3252\n",
      "Epoch 750/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.7988 - val_loss: 23.0525\n",
      "Epoch 751/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.3240 - val_loss: 22.6393\n",
      "Epoch 752/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.9385 - val_loss: 22.6618\n",
      "Epoch 753/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.4138 - val_loss: 22.8211\n",
      "Epoch 754/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.0438 - val_loss: 22.8359\n",
      "Epoch 755/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.4719 - val_loss: 22.2558\n",
      "Epoch 756/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.2727 - val_loss: 23.8748\n",
      "Epoch 757/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.8792 - val_loss: 23.8308\n",
      "Epoch 758/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.7041 - val_loss: 22.6380\n",
      "Epoch 759/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.2486 - val_loss: 22.5945\n",
      "Epoch 760/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.7866 - val_loss: 23.2651\n",
      "Epoch 761/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.8093 - val_loss: 23.5104\n",
      "Epoch 762/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.3479 - val_loss: 22.9921\n",
      "Epoch 763/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.6542 - val_loss: 22.7364\n",
      "Epoch 764/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.1452 - val_loss: 22.3416\n",
      "Epoch 765/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.5461 - val_loss: 22.7396\n",
      "Epoch 766/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.5769 - val_loss: 23.0133\n",
      "Epoch 767/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.4939 - val_loss: 22.6904\n",
      "Epoch 768/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.5004 - val_loss: 22.7503\n",
      "Epoch 769/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.1619 - val_loss: 24.0157\n",
      "Epoch 770/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.7954 - val_loss: 22.6379\n",
      "Epoch 771/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.6803 - val_loss: 22.1781\n",
      "Epoch 772/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.5511 - val_loss: 22.8070\n",
      "Epoch 773/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.6170 - val_loss: 22.8566\n",
      "Epoch 774/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.9188 - val_loss: 22.6917\n",
      "Epoch 775/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.2261 - val_loss: 23.1700\n",
      "Epoch 776/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.0760 - val_loss: 23.0666\n",
      "Epoch 777/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.5174 - val_loss: 23.4306\n",
      "Epoch 778/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.1081 - val_loss: 22.5744\n",
      "Epoch 779/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.5193 - val_loss: 22.3885\n",
      "Epoch 780/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.9493 - val_loss: 23.0500\n",
      "Epoch 781/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 17.7201 - val_loss: 24.0367\n",
      "Epoch 782/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.1989 - val_loss: 22.8914\n",
      "Epoch 783/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.8903 - val_loss: 23.7289\n",
      "Epoch 784/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.3463 - val_loss: 23.0327\n",
      "Epoch 785/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.5592 - val_loss: 23.2077\n",
      "Epoch 786/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.3747 - val_loss: 22.7640\n",
      "Epoch 787/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.8778 - val_loss: 22.7641\n",
      "Epoch 788/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.2272 - val_loss: 22.5681\n",
      "Epoch 789/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.4865 - val_loss: 23.2502\n",
      "Epoch 790/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.3192 - val_loss: 22.7961\n",
      "Epoch 791/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.2105 - val_loss: 22.8928\n",
      "Epoch 792/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.5951 - val_loss: 23.8543\n",
      "Epoch 793/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.3087 - val_loss: 24.0403\n",
      "Epoch 794/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.9262 - val_loss: 23.5630\n",
      "Epoch 795/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.9604 - val_loss: 22.4569\n",
      "Epoch 796/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.5341 - val_loss: 24.0092\n",
      "Epoch 797/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.9739 - val_loss: 23.9566\n",
      "Epoch 798/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.3844 - val_loss: 22.8013\n",
      "Epoch 799/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.2779 - val_loss: 22.0911\n",
      "Epoch 800/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.5457 - val_loss: 23.5551\n",
      "Epoch 801/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.9481 - val_loss: 23.3162\n",
      "Epoch 802/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.6409 - val_loss: 22.1784\n",
      "Epoch 803/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.0436 - val_loss: 22.9780\n",
      "Epoch 804/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.4101 - val_loss: 22.2133\n",
      "Epoch 805/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.1128 - val_loss: 22.5788\n",
      "Epoch 806/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.1227 - val_loss: 22.1818\n",
      "Epoch 807/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.0325 - val_loss: 22.0287\n",
      "Epoch 808/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.2448 - val_loss: 23.1368\n",
      "Epoch 809/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.2159 - val_loss: 22.8520\n",
      "Epoch 810/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.4688 - val_loss: 22.9528\n",
      "Epoch 811/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.0936 - val_loss: 24.1272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 812/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.2947 - val_loss: 23.4980\n",
      "Epoch 813/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.2409 - val_loss: 22.4978\n",
      "Epoch 814/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.3475 - val_loss: 23.1441\n",
      "Epoch 815/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.3020 - val_loss: 21.8619\n",
      "Epoch 816/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 17.9727 - val_loss: 24.3011\n",
      "Epoch 817/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.2070 - val_loss: 23.4374\n",
      "Epoch 818/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.2551 - val_loss: 23.1587\n",
      "Epoch 819/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.1965 - val_loss: 23.5263\n",
      "Epoch 820/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.4745 - val_loss: 22.6741\n",
      "Epoch 821/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.8307 - val_loss: 23.1568\n",
      "Epoch 822/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.4514 - val_loss: 23.2323\n",
      "Epoch 823/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.2834 - val_loss: 23.1025\n",
      "Epoch 824/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.9434 - val_loss: 22.2148\n",
      "Epoch 825/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.4093 - val_loss: 23.5057\n",
      "Epoch 826/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.2963 - val_loss: 22.6462\n",
      "Epoch 827/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.1839 - val_loss: 22.6272\n",
      "Epoch 828/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.0654 - val_loss: 22.4329\n",
      "Epoch 829/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.4822 - val_loss: 24.6006\n",
      "Epoch 830/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.5716 - val_loss: 23.3325\n",
      "Epoch 831/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.3440 - val_loss: 23.6398\n",
      "Epoch 832/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.0603 - val_loss: 22.7633\n",
      "Epoch 833/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.1628 - val_loss: 22.1722\n",
      "Epoch 834/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.3761 - val_loss: 22.6551\n",
      "Epoch 835/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.0971 - val_loss: 23.0458\n",
      "Epoch 836/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.4401 - val_loss: 22.1949\n",
      "Epoch 837/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.1990 - val_loss: 22.8763\n",
      "Epoch 838/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.1922 - val_loss: 22.8154\n",
      "Epoch 839/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.2871 - val_loss: 22.6605\n",
      "Epoch 840/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.9129 - val_loss: 22.7016\n",
      "Epoch 841/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.1711 - val_loss: 23.4565\n",
      "Epoch 842/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 18.1437 - val_loss: 22.6659\n",
      "Epoch 843/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.9088 - val_loss: 22.8742\n",
      "Epoch 844/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.2983 - val_loss: 22.0287\n",
      "Epoch 845/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.0159 - val_loss: 23.0922\n",
      "Epoch 846/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.2810 - val_loss: 23.3547\n",
      "Epoch 847/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.4363 - val_loss: 23.5514\n",
      "Epoch 848/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.3394 - val_loss: 22.6071\n",
      "Epoch 849/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.4077 - val_loss: 22.5315\n",
      "Epoch 850/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.1012 - val_loss: 22.3896\n",
      "Epoch 851/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.3902 - val_loss: 22.6259\n",
      "Epoch 852/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.2051 - val_loss: 22.9779\n",
      "Epoch 853/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 16.9336 - val_loss: 22.5910\n",
      "Epoch 854/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.1844 - val_loss: 23.3949\n",
      "Epoch 855/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.4624 - val_loss: 21.8991\n",
      "Epoch 856/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.2240 - val_loss: 23.2643\n",
      "Epoch 857/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.0094 - val_loss: 22.6046\n",
      "Epoch 858/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.1391 - val_loss: 22.5517\n",
      "Epoch 859/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.9918 - val_loss: 22.2013\n",
      "Epoch 860/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 17.0570 - val_loss: 22.2250\n",
      "Epoch 861/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 16.9441 - val_loss: 23.3925\n",
      "Epoch 862/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.1340 - val_loss: 22.3283\n",
      "Epoch 863/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.0321 - val_loss: 21.9368\n",
      "Epoch 864/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.6240 - val_loss: 22.9537\n",
      "Epoch 865/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.8960 - val_loss: 22.5322\n",
      "Epoch 866/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.0031 - val_loss: 22.8166\n",
      "Epoch 867/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.2211 - val_loss: 22.5961\n",
      "Epoch 868/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.0930 - val_loss: 22.3065\n",
      "Epoch 869/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 16.7919 - val_loss: 23.1468\n",
      "Epoch 870/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.8579 - val_loss: 22.2514\n",
      "Epoch 871/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.4222 - val_loss: 23.4217\n",
      "Epoch 872/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 17.3355 - val_loss: 23.3394\n",
      "Epoch 873/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.2509 - val_loss: 22.5506\n",
      "Epoch 874/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.0485 - val_loss: 22.9532\n",
      "Epoch 875/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.1010 - val_loss: 22.8048\n",
      "Epoch 876/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.0123 - val_loss: 22.4635\n",
      "Epoch 877/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 16.9072 - val_loss: 22.6389\n",
      "Epoch 878/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.8621 - val_loss: 25.2217\n",
      "Epoch 879/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.3061 - val_loss: 23.5082\n",
      "Epoch 880/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.2042 - val_loss: 23.9571\n",
      "Epoch 881/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.2734 - val_loss: 22.9859\n",
      "Epoch 882/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.7832 - val_loss: 23.0903\n",
      "Epoch 883/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.5094 - val_loss: 24.3387\n",
      "Epoch 884/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.2432 - val_loss: 23.6200\n",
      "Epoch 885/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.5273 - val_loss: 23.0896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 886/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 17.3465 - val_loss: 22.6402\n",
      "Epoch 887/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.4047 - val_loss: 22.7257\n",
      "Epoch 888/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.8080 - val_loss: 23.4121\n",
      "Epoch 889/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.3413 - val_loss: 22.7808\n",
      "Epoch 890/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.1832 - val_loss: 22.8493\n",
      "Epoch 891/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 17.4268 - val_loss: 22.6190\n",
      "Epoch 892/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.4377 - val_loss: 22.4587\n",
      "Epoch 893/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.1782 - val_loss: 24.2289\n",
      "Epoch 894/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.8507 - val_loss: 24.6957\n",
      "Epoch 895/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.7458 - val_loss: 22.7797\n",
      "Epoch 896/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.3858 - val_loss: 23.3575\n",
      "Epoch 897/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.4208 - val_loss: 23.4610\n",
      "Epoch 898/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 17.5818 - val_loss: 22.5754\n",
      "Epoch 899/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.3725 - val_loss: 22.7901\n",
      "Epoch 900/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.2799 - val_loss: 22.5736\n",
      "Epoch 901/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 16.7372 - val_loss: 22.4224\n",
      "Epoch 902/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.5883 - val_loss: 22.4969\n",
      "Epoch 903/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.6729 - val_loss: 24.3509\n",
      "Epoch 904/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.6314 - val_loss: 22.3698\n",
      "Epoch 905/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 16.9849 - val_loss: 22.6343\n",
      "Epoch 906/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.1854 - val_loss: 24.7593\n",
      "Epoch 907/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.6370 - val_loss: 24.4105\n",
      "Epoch 908/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.3190 - val_loss: 22.6911\n",
      "Epoch 909/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.7953 - val_loss: 22.6271\n",
      "Epoch 910/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.8866 - val_loss: 22.8016\n",
      "Epoch 911/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.1578 - val_loss: 22.4839\n",
      "Epoch 912/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 16.7033 - val_loss: 23.0640\n",
      "Epoch 913/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.7439 - val_loss: 22.5711\n",
      "Epoch 914/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.2581 - val_loss: 22.3823\n",
      "Epoch 915/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.4332 - val_loss: 23.6965\n",
      "Epoch 916/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.4060 - val_loss: 23.2669\n",
      "Epoch 917/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 17.0506 - val_loss: 22.8406\n",
      "Epoch 918/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.0246 - val_loss: 22.1434\n",
      "Epoch 919/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.0825 - val_loss: 22.5445\n",
      "Epoch 920/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 16.9927 - val_loss: 24.0089\n",
      "Epoch 921/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 16.9528 - val_loss: 23.1816\n",
      "Epoch 922/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.2842 - val_loss: 22.9665\n",
      "Epoch 923/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 16.6870 - val_loss: 22.6461\n",
      "Epoch 924/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 16.5972 - val_loss: 23.3437\n",
      "Epoch 925/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 16.9921 - val_loss: 23.0428\n",
      "Epoch 926/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 16.7167 - val_loss: 22.3663\n",
      "Epoch 927/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.7917 - val_loss: 22.2360\n",
      "Epoch 928/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.9222 - val_loss: 22.7906\n",
      "Epoch 929/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.9550 - val_loss: 22.7852\n",
      "Epoch 930/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 16.7281 - val_loss: 22.8930\n",
      "Epoch 931/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 16.6044 - val_loss: 22.9762\n",
      "Epoch 932/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 17.0532 - val_loss: 22.8227\n",
      "Epoch 933/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.0977 - val_loss: 23.0913\n",
      "Epoch 934/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.6524 - val_loss: 23.2006\n",
      "Epoch 935/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.0248 - val_loss: 22.7899\n",
      "Epoch 936/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.5923 - val_loss: 23.6107\n",
      "Epoch 937/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.8690 - val_loss: 22.8689\n",
      "Epoch 938/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.0583 - val_loss: 23.9869\n",
      "Epoch 939/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.8668 - val_loss: 23.1940\n",
      "Epoch 940/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 16.7980 - val_loss: 23.2522\n",
      "Epoch 941/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.9600 - val_loss: 22.2779\n",
      "Epoch 942/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.9864 - val_loss: 23.3102\n",
      "Epoch 943/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.0656 - val_loss: 22.8614\n",
      "Epoch 944/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.4156 - val_loss: 22.9837\n",
      "Epoch 945/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.5854 - val_loss: 22.6490\n",
      "Epoch 946/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 16.9399 - val_loss: 22.5577\n",
      "Epoch 947/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 17.1231 - val_loss: 23.3778\n",
      "Epoch 948/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.9194 - val_loss: 22.8124\n",
      "Epoch 949/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.1610 - val_loss: 22.3135\n",
      "Epoch 950/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.5446 - val_loss: 22.7165\n",
      "Epoch 951/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.9166 - val_loss: 22.2292\n",
      "Epoch 952/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.0088 - val_loss: 21.8961\n",
      "Epoch 953/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.7813 - val_loss: 22.1483\n",
      "Epoch 954/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.7622 - val_loss: 22.2539\n",
      "Epoch 955/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.8249 - val_loss: 22.7666\n",
      "Epoch 956/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.0274 - val_loss: 22.3499\n",
      "Epoch 957/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 16.5609 - val_loss: 23.1679\n",
      "Epoch 958/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.9878 - val_loss: 22.6053\n",
      "Epoch 959/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.5643 - val_loss: 23.7029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 960/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.4765 - val_loss: 23.3767\n",
      "Epoch 961/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.1928 - val_loss: 22.7543\n",
      "Epoch 962/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.8421 - val_loss: 22.3455\n",
      "Epoch 963/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.9074 - val_loss: 22.2668\n",
      "Epoch 964/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.6874 - val_loss: 23.3929\n",
      "Epoch 965/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.8512 - val_loss: 23.5308\n",
      "Epoch 966/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.0669 - val_loss: 22.5131\n",
      "Epoch 967/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.7629 - val_loss: 22.4930\n",
      "Epoch 968/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.9374 - val_loss: 23.5822\n",
      "Epoch 969/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.3967 - val_loss: 24.3470\n",
      "Epoch 970/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.2115 - val_loss: 23.0691\n",
      "Epoch 971/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.8825 - val_loss: 22.5616\n",
      "Epoch 972/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 16.9526 - val_loss: 22.7714\n",
      "Epoch 973/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.7853 - val_loss: 22.8972\n",
      "Epoch 974/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.2106 - val_loss: 22.7125\n",
      "Epoch 975/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.9994 - val_loss: 22.5867\n",
      "Epoch 976/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 16.5826 - val_loss: 22.6890\n",
      "Epoch 977/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.6311 - val_loss: 21.9711\n",
      "Epoch 978/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.8127 - val_loss: 22.2557\n",
      "Epoch 979/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.7280 - val_loss: 22.4862\n",
      "Epoch 980/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.7826 - val_loss: 22.5816\n",
      "Epoch 981/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.6549 - val_loss: 23.3582\n",
      "Epoch 982/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.8829 - val_loss: 23.4180\n",
      "Epoch 983/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.2423 - val_loss: 23.2133\n",
      "Epoch 984/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.2522 - val_loss: 22.5382\n",
      "Epoch 985/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.7621 - val_loss: 23.3500\n",
      "Epoch 986/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.8585 - val_loss: 23.6915\n",
      "Epoch 987/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 16.6119 - val_loss: 22.2960\n",
      "Epoch 988/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 16.5905 - val_loss: 22.8012\n",
      "Epoch 989/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.1514 - val_loss: 23.1327\n",
      "Epoch 990/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 16.8717 - val_loss: 22.9349\n",
      "Epoch 991/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 16.8523 - val_loss: 22.6001\n",
      "Epoch 992/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.4649 - val_loss: 22.5336\n",
      "Epoch 993/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.9859 - val_loss: 23.0839\n",
      "Epoch 994/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.3900 - val_loss: 22.2372\n",
      "Epoch 995/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.5005 - val_loss: 22.2931\n",
      "Epoch 996/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 16.7498 - val_loss: 23.2136\n",
      "Epoch 997/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.1853 - val_loss: 22.3478\n",
      "Epoch 998/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.5925 - val_loss: 21.8717\n",
      "Epoch 999/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.7388 - val_loss: 21.9527\n",
      "Epoch 1000/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.1279 - val_loss: 23.4896\n",
      "Epoch 1001/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.8817 - val_loss: 23.2596\n",
      "Epoch 1002/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.7413 - val_loss: 22.8461\n",
      "Epoch 1003/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.5025 - val_loss: 23.4940\n",
      "Epoch 1004/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 16.5862 - val_loss: 23.8812\n",
      "Epoch 1005/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 16.4902 - val_loss: 22.8775\n",
      "Epoch 1006/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.5009 - val_loss: 22.4693\n",
      "Epoch 1007/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.0060 - val_loss: 23.1989\n",
      "Epoch 1008/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.1464 - val_loss: 23.2289\n",
      "Epoch 1009/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 17.3110 - val_loss: 23.4383\n",
      "Epoch 1010/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.1110 - val_loss: 24.4652\n",
      "Epoch 1011/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.9165 - val_loss: 22.8101\n",
      "Epoch 1012/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.9184 - val_loss: 22.2361\n",
      "Epoch 1013/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.7781 - val_loss: 22.8050\n",
      "Epoch 1014/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.8190 - val_loss: 23.1372\n",
      "Epoch 1015/2000\n",
      "1500/1500 [==============================] - 0s 20us/sample - loss: 17.3792 - val_loss: 23.1543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|██████████████████████████████████████████████▋                                    | 9/16 [05:16<04:22, 37.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'batc_normalization': False, 'batch_size': 64, 'dropout': 0, 'epochs': 2000, 'first_neuron': 160, 'hidden_layers': 1, 'hidden_neuron': 100, 'kernel_initializer': 'normal', 'last_activation': 'linear', 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "Train on 1500 samples, validate on 375 samples\n",
      "Epoch 1/2000\n",
      "1500/1500 [==============================] - 1s 341us/sample - loss: 2185.3575 - val_loss: 2184.4712\n",
      "Epoch 2/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 2178.7502 - val_loss: 2170.6814\n",
      "Epoch 3/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 2152.1157 - val_loss: 2124.9010\n",
      "Epoch 4/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 2079.1948 - val_loss: 2016.2160\n",
      "Epoch 5/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 1926.1629 - val_loss: 1807.2069\n",
      "Epoch 6/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 1653.3661 - val_loss: 1458.5392\n",
      "Epoch 7/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 1231.9861 - val_loss: 968.6818\n",
      "Epoch 8/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 818.2253 - val_loss: 673.1375\n",
      "Epoch 9/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 640.1679 - val_loss: 579.4426\n",
      "Epoch 10/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 569.9687 - val_loss: 525.6363\n",
      "Epoch 11/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 516.3544 - val_loss: 477.8862\n",
      "Epoch 12/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 467.2713 - val_loss: 431.2033\n",
      "Epoch 13/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 419.6642 - val_loss: 387.2822\n",
      "Epoch 14/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 372.7489 - val_loss: 342.6177\n",
      "Epoch 15/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 325.5655 - val_loss: 294.2278\n",
      "Epoch 16/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 277.1520 - val_loss: 247.5714\n",
      "Epoch 17/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 230.5400 - val_loss: 204.7592\n",
      "Epoch 18/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 188.2854 - val_loss: 167.7861\n",
      "Epoch 19/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 153.3686 - val_loss: 140.5284\n",
      "Epoch 20/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 129.1901 - val_loss: 117.0544\n",
      "Epoch 21/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 106.3038 - val_loss: 97.3561\n",
      "Epoch 22/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 89.6185 - val_loss: 82.5135\n",
      "Epoch 23/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 76.1911 - val_loss: 71.3728\n",
      "Epoch 24/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 64.9859 - val_loss: 62.5652\n",
      "Epoch 25/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 55.6818 - val_loss: 55.7028\n",
      "Epoch 26/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 50.0500 - val_loss: 50.5048\n",
      "Epoch 27/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 45.6988 - val_loss: 46.4137\n",
      "Epoch 28/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 41.0750 - val_loss: 41.7381\n",
      "Epoch 29/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 37.6513 - val_loss: 40.2714\n",
      "Epoch 30/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 35.2624 - val_loss: 37.3506\n",
      "Epoch 31/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 33.7671 - val_loss: 36.4211\n",
      "Epoch 32/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 32.5249 - val_loss: 34.0532\n",
      "Epoch 33/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 30.5302 - val_loss: 33.1382\n",
      "Epoch 34/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 29.5171 - val_loss: 32.8671\n",
      "Epoch 35/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 28.8364 - val_loss: 30.0001\n",
      "Epoch 36/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 27.4221 - val_loss: 29.0050\n",
      "Epoch 37/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 27.3017 - val_loss: 30.3943\n",
      "Epoch 38/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 25.9234 - val_loss: 27.4533\n",
      "Epoch 39/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 25.2711 - val_loss: 26.9425\n",
      "Epoch 40/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 25.5024 - val_loss: 28.5916\n",
      "Epoch 41/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 25.5129 - val_loss: 25.9651\n",
      "Epoch 42/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 25.9871 - val_loss: 26.5334\n",
      "Epoch 43/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 24.3216 - val_loss: 25.8423\n",
      "Epoch 44/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 24.0770 - val_loss: 25.8556\n",
      "Epoch 45/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 23.3923 - val_loss: 25.1719\n",
      "Epoch 46/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 23.3695 - val_loss: 25.2943\n",
      "Epoch 47/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 23.5929 - val_loss: 24.3475\n",
      "Epoch 48/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 23.8099 - val_loss: 26.7776\n",
      "Epoch 49/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 23.7955 - val_loss: 27.4722\n",
      "Epoch 50/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 23.3416 - val_loss: 25.5678\n",
      "Epoch 51/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 22.7221 - val_loss: 26.0287\n",
      "Epoch 52/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 23.2188 - val_loss: 25.3297\n",
      "Epoch 53/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 22.6477 - val_loss: 24.7063\n",
      "Epoch 54/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 22.5407 - val_loss: 24.6231\n",
      "Epoch 55/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 23.6771 - val_loss: 25.8153\n",
      "Epoch 56/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 22.7066 - val_loss: 23.7382\n",
      "Epoch 57/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 22.2560 - val_loss: 23.6553\n",
      "Epoch 58/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 22.1119 - val_loss: 24.1638\n",
      "Epoch 59/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 21.9394 - val_loss: 23.9756\n",
      "Epoch 60/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 22.8776 - val_loss: 25.6548\n",
      "Epoch 61/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 22.3295 - val_loss: 24.1318\n",
      "Epoch 62/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 22.2431 - val_loss: 24.3117\n",
      "Epoch 63/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 22.3277 - val_loss: 24.6831\n",
      "Epoch 64/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 22.5775 - val_loss: 24.2493\n",
      "Epoch 65/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 22.9097 - val_loss: 24.7360\n",
      "Epoch 66/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 22.2556 - val_loss: 23.0126\n",
      "Epoch 67/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 21.6468 - val_loss: 23.2519\n",
      "Epoch 68/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 22.0839 - val_loss: 23.9108\n",
      "Epoch 69/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 22.2580 - val_loss: 24.6815\n",
      "Epoch 70/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 22.1133 - val_loss: 23.4018\n",
      "Epoch 71/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.8958 - val_loss: 23.5529\n",
      "Epoch 72/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 22.4248 - val_loss: 25.4500\n",
      "Epoch 73/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 22.6115 - val_loss: 23.3590\n",
      "Epoch 74/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 21.5390 - val_loss: 24.5156\n",
      "Epoch 75/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.4550 - val_loss: 24.1565\n",
      "Epoch 76/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.6404 - val_loss: 24.2502\n",
      "Epoch 77/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.7300 - val_loss: 23.0047\n",
      "Epoch 78/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.7857 - val_loss: 25.1923\n",
      "Epoch 79/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 23.3383 - val_loss: 26.0351\n",
      "Epoch 80/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 22.2390 - val_loss: 23.6101\n",
      "Epoch 81/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 21.7074 - val_loss: 24.0208\n",
      "Epoch 82/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.2905 - val_loss: 23.7587\n",
      "Epoch 83/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.3030 - val_loss: 23.4669\n",
      "Epoch 84/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 21.2354 - val_loss: 22.8802\n",
      "Epoch 85/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.4875 - val_loss: 24.3112\n",
      "Epoch 86/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.7386 - val_loss: 24.0414\n",
      "Epoch 87/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.7532 - val_loss: 24.5779\n",
      "Epoch 88/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.5848 - val_loss: 24.8692\n",
      "Epoch 89/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 21.3198 - val_loss: 23.6548\n",
      "Epoch 90/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.8464 - val_loss: 24.2173\n",
      "Epoch 91/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 22.0233 - val_loss: 23.4935\n",
      "Epoch 92/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 22.3513 - val_loss: 24.9773\n",
      "Epoch 93/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.2692 - val_loss: 23.0786\n",
      "Epoch 94/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 21.1154 - val_loss: 24.3539\n",
      "Epoch 95/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.7767 - val_loss: 24.5467\n",
      "Epoch 96/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 22.1359 - val_loss: 24.4565\n",
      "Epoch 97/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 22.3596 - val_loss: 24.0142\n",
      "Epoch 98/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.1968 - val_loss: 24.7157\n",
      "Epoch 99/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.1004 - val_loss: 24.8940\n",
      "Epoch 100/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.5663 - val_loss: 23.5531\n",
      "Epoch 101/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.7712 - val_loss: 26.7308\n",
      "Epoch 102/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.6723 - val_loss: 23.2104\n",
      "Epoch 103/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.9738 - val_loss: 23.8376\n",
      "Epoch 104/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.9021 - val_loss: 23.0543\n",
      "Epoch 105/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.8199 - val_loss: 23.9392\n",
      "Epoch 106/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.5726 - val_loss: 25.9627\n",
      "Epoch 107/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.4096 - val_loss: 23.3645\n",
      "Epoch 108/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.6933 - val_loss: 23.8697\n",
      "Epoch 109/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 21.2316 - val_loss: 22.9201\n",
      "Epoch 110/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.0760 - val_loss: 24.1424\n",
      "Epoch 111/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.8381 - val_loss: 23.5239\n",
      "Epoch 112/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 21.2822 - val_loss: 23.6567\n",
      "Epoch 113/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 22.4467 - val_loss: 24.5638\n",
      "Epoch 114/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.5775 - val_loss: 24.8895\n",
      "Epoch 115/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 21.0845 - val_loss: 22.9238\n",
      "Epoch 116/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.6736 - val_loss: 24.9011\n",
      "Epoch 117/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.8848 - val_loss: 22.7816\n",
      "Epoch 118/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.2923 - val_loss: 23.2720\n",
      "Epoch 119/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.0995 - val_loss: 24.1847\n",
      "Epoch 120/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.9589 - val_loss: 23.0114\n",
      "Epoch 121/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.3839 - val_loss: 25.4137\n",
      "Epoch 122/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.5641 - val_loss: 24.0038\n",
      "Epoch 123/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.8519 - val_loss: 23.0178\n",
      "Epoch 124/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.8803 - val_loss: 23.3958\n",
      "Epoch 125/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.6949 - val_loss: 23.7014\n",
      "Epoch 126/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.0448 - val_loss: 24.5093\n",
      "Epoch 127/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.9741 - val_loss: 25.3200\n",
      "Epoch 128/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.1924 - val_loss: 23.4012\n",
      "Epoch 129/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.5176 - val_loss: 24.8709\n",
      "Epoch 130/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.5647 - val_loss: 23.9783\n",
      "Epoch 131/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.0758 - val_loss: 23.2951\n",
      "Epoch 132/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.1117 - val_loss: 23.6875\n",
      "Epoch 133/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.3018 - val_loss: 23.8937\n",
      "Epoch 134/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.5222 - val_loss: 23.3836\n",
      "Epoch 135/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.3368 - val_loss: 24.3533\n",
      "Epoch 136/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.1796 - val_loss: 22.9713\n",
      "Epoch 137/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 20.6852 - val_loss: 24.0346\n",
      "Epoch 138/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.3572 - val_loss: 23.7992\n",
      "Epoch 139/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 22.4622 - val_loss: 25.3243\n",
      "Epoch 140/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 22.1785 - val_loss: 23.7423\n",
      "Epoch 141/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.2337 - val_loss: 26.2881\n",
      "Epoch 142/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.6958 - val_loss: 23.2056\n",
      "Epoch 143/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.5880 - val_loss: 23.5396\n",
      "Epoch 144/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.7694 - val_loss: 23.0180\n",
      "Epoch 145/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.6700 - val_loss: 23.8716\n",
      "Epoch 146/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.3343 - val_loss: 23.5475\n",
      "Epoch 147/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.7440 - val_loss: 23.8599\n",
      "Epoch 148/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 20.2072 - val_loss: 24.7874\n",
      "Epoch 149/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 20.6031 - val_loss: 24.3097\n",
      "Epoch 150/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.4937 - val_loss: 23.9311\n",
      "Epoch 151/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.4919 - val_loss: 22.9274\n",
      "Epoch 152/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.6254 - val_loss: 23.5964\n",
      "Epoch 153/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.8378 - val_loss: 25.3829\n",
      "Epoch 154/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.8930 - val_loss: 25.8825\n",
      "Epoch 155/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.0078 - val_loss: 26.5110\n",
      "Epoch 156/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 21.7391 - val_loss: 23.9365\n",
      "Epoch 157/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.6984 - val_loss: 23.1795\n",
      "Epoch 158/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.9527 - val_loss: 22.3944\n",
      "Epoch 159/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.4787 - val_loss: 25.6831\n",
      "Epoch 160/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 20.7739 - val_loss: 22.6043\n",
      "Epoch 161/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.0787 - val_loss: 23.8920\n",
      "Epoch 162/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.5237 - val_loss: 23.1035\n",
      "Epoch 163/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.3969 - val_loss: 23.0142\n",
      "Epoch 164/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.4873 - val_loss: 23.1467\n",
      "Epoch 165/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.3238 - val_loss: 22.7422\n",
      "Epoch 166/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.2743 - val_loss: 23.1685\n",
      "Epoch 167/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.2325 - val_loss: 22.6738\n",
      "Epoch 168/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.4707 - val_loss: 23.3574\n",
      "Epoch 169/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.3243 - val_loss: 23.9781\n",
      "Epoch 170/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 20.1653 - val_loss: 23.6529\n",
      "Epoch 171/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 20.6405 - val_loss: 23.8336\n",
      "Epoch 172/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.9158 - val_loss: 24.5369\n",
      "Epoch 173/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.4492 - val_loss: 23.2291\n",
      "Epoch 174/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.6103 - val_loss: 22.6791\n",
      "Epoch 175/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.7515 - val_loss: 22.8982\n",
      "Epoch 176/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.7661 - val_loss: 23.4412\n",
      "Epoch 177/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.9720 - val_loss: 23.5198\n",
      "Epoch 178/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.4928 - val_loss: 23.6129\n",
      "Epoch 179/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.6304 - val_loss: 23.3288\n",
      "Epoch 180/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.1075 - val_loss: 22.8200\n",
      "Epoch 181/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 20.1300 - val_loss: 23.1410\n",
      "Epoch 182/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.3929 - val_loss: 23.5270\n",
      "Epoch 183/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.4424 - val_loss: 23.2379\n",
      "Epoch 184/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.9432 - val_loss: 24.1007\n",
      "Epoch 185/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.1349 - val_loss: 23.5961\n",
      "Epoch 186/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.0338 - val_loss: 23.3342\n",
      "Epoch 187/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.3504 - val_loss: 24.1733\n",
      "Epoch 188/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 21.2203 - val_loss: 23.4950\n",
      "Epoch 189/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 20.3602 - val_loss: 23.2795\n",
      "Epoch 190/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 20.2745 - val_loss: 23.6123\n",
      "Epoch 191/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 20.0053 - val_loss: 22.3823\n",
      "Epoch 192/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.0536 - val_loss: 23.0220\n",
      "Epoch 193/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.2373 - val_loss: 24.4894\n",
      "Epoch 194/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.4003 - val_loss: 24.1857\n",
      "Epoch 195/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 20.2080 - val_loss: 23.5132\n",
      "Epoch 196/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.1009 - val_loss: 23.9402\n",
      "Epoch 197/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.4593 - val_loss: 24.7807\n",
      "Epoch 198/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.4485 - val_loss: 23.3484\n",
      "Epoch 199/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.6614 - val_loss: 22.6784\n",
      "Epoch 200/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.9883 - val_loss: 23.0514\n",
      "Epoch 201/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.8624 - val_loss: 23.0092\n",
      "Epoch 202/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.1154 - val_loss: 23.7531\n",
      "Epoch 203/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.3119 - val_loss: 27.0625\n",
      "Epoch 204/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.0607 - val_loss: 24.4874\n",
      "Epoch 205/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 20.5024 - val_loss: 22.9979\n",
      "Epoch 206/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.1463 - val_loss: 23.5511\n",
      "Epoch 207/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 20.5467 - val_loss: 25.0556\n",
      "Epoch 208/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.3289 - val_loss: 22.7513\n",
      "Epoch 209/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 21.0379 - val_loss: 22.8691\n",
      "Epoch 210/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.0866 - val_loss: 23.6494\n",
      "Epoch 211/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.9097 - val_loss: 23.5319\n",
      "Epoch 212/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.2814 - val_loss: 24.5462\n",
      "Epoch 213/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.8287 - val_loss: 22.5584\n",
      "Epoch 214/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.3736 - val_loss: 23.0104\n",
      "Epoch 215/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.4543 - val_loss: 22.9025\n",
      "Epoch 216/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.3953 - val_loss: 24.8192\n",
      "Epoch 217/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.7757 - val_loss: 23.4463\n",
      "Epoch 218/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.0419 - val_loss: 23.0954\n",
      "Epoch 219/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.9514 - val_loss: 23.3057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 220/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.9713 - val_loss: 23.7083\n",
      "Epoch 221/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.7487 - val_loss: 23.4563\n",
      "Epoch 222/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.7045 - val_loss: 24.2257\n",
      "Epoch 223/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.8689 - val_loss: 23.2320\n",
      "Epoch 224/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 20.8792 - val_loss: 23.3696\n",
      "Epoch 225/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.9771 - val_loss: 23.6145\n",
      "Epoch 226/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.2074 - val_loss: 22.8747\n",
      "Epoch 227/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.0054 - val_loss: 22.5176\n",
      "Epoch 228/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.8705 - val_loss: 23.3760\n",
      "Epoch 229/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.7784 - val_loss: 22.9326\n",
      "Epoch 230/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.7553 - val_loss: 22.8509\n",
      "Epoch 231/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.5625 - val_loss: 25.0485\n",
      "Epoch 232/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.7439 - val_loss: 22.4804\n",
      "Epoch 233/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.1190 - val_loss: 24.0391\n",
      "Epoch 234/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.4428 - val_loss: 23.4012\n",
      "Epoch 235/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.7772 - val_loss: 24.0114\n",
      "Epoch 236/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.9085 - val_loss: 23.2713\n",
      "Epoch 237/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.8795 - val_loss: 23.3860\n",
      "Epoch 238/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.4778 - val_loss: 23.1274\n",
      "Epoch 239/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.5171 - val_loss: 24.2812\n",
      "Epoch 240/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.8353 - val_loss: 22.8747\n",
      "Epoch 241/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.8643 - val_loss: 23.2510\n",
      "Epoch 242/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.3467 - val_loss: 22.9601\n",
      "Epoch 243/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.8998 - val_loss: 25.7000\n",
      "Epoch 244/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.7655 - val_loss: 25.4741\n",
      "Epoch 245/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.1241 - val_loss: 22.7774\n",
      "Epoch 246/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.0989 - val_loss: 23.0966\n",
      "Epoch 247/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.4165 - val_loss: 24.2389\n",
      "Epoch 248/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.2878 - val_loss: 24.7984\n",
      "Epoch 249/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.6462 - val_loss: 23.6566\n",
      "Epoch 250/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.0381 - val_loss: 23.3329\n",
      "Epoch 251/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.1354 - val_loss: 24.8412\n",
      "Epoch 252/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.8451 - val_loss: 23.0129\n",
      "Epoch 253/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.6345 - val_loss: 24.2762\n",
      "Epoch 254/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.1244 - val_loss: 23.8264\n",
      "Epoch 255/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.5634 - val_loss: 23.9029\n",
      "Epoch 256/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 20.1932 - val_loss: 23.0495\n",
      "Epoch 257/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.5930 - val_loss: 23.5358\n",
      "Epoch 258/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.7339 - val_loss: 23.3277\n",
      "Epoch 259/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.7755 - val_loss: 23.0685\n",
      "Epoch 260/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.0916 - val_loss: 23.1926\n",
      "Epoch 261/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.5743 - val_loss: 23.0750\n",
      "Epoch 262/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.3299 - val_loss: 22.4959\n",
      "Epoch 263/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.3693 - val_loss: 23.7494\n",
      "Epoch 264/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.8336 - val_loss: 23.8145\n",
      "Epoch 265/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.4314 - val_loss: 23.5724\n",
      "Epoch 266/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.8537 - val_loss: 22.6880\n",
      "Epoch 267/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.4475 - val_loss: 23.1920\n",
      "Epoch 268/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.3889 - val_loss: 24.2089\n",
      "Epoch 269/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.9360 - val_loss: 24.0934\n",
      "Epoch 270/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.4347 - val_loss: 24.7204\n",
      "Epoch 271/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.5635 - val_loss: 23.0258\n",
      "Epoch 272/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.5573 - val_loss: 25.1016\n",
      "Epoch 273/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.3279 - val_loss: 24.9106\n",
      "Epoch 274/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 20.5644 - val_loss: 24.1921\n",
      "Epoch 275/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.5980 - val_loss: 24.5114\n",
      "Epoch 276/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.7103 - val_loss: 26.3167\n",
      "Epoch 277/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.3467 - val_loss: 23.2712\n",
      "Epoch 278/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.3970 - val_loss: 23.4403\n",
      "Epoch 279/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.6733 - val_loss: 23.8910\n",
      "Epoch 280/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.6136 - val_loss: 23.4070\n",
      "Epoch 281/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.5093 - val_loss: 24.3678\n",
      "Epoch 282/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.4888 - val_loss: 23.3192\n",
      "Epoch 283/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 20.1625 - val_loss: 23.1373\n",
      "Epoch 284/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.5695 - val_loss: 23.5383\n",
      "Epoch 285/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.7068 - val_loss: 22.8925\n",
      "Epoch 286/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.9586 - val_loss: 24.2206\n",
      "Epoch 287/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.2285 - val_loss: 24.1952\n",
      "Epoch 288/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.5967 - val_loss: 22.6480\n",
      "Epoch 289/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.0220 - val_loss: 22.4558\n",
      "Epoch 290/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.3600 - val_loss: 24.1989\n",
      "Epoch 291/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.5698 - val_loss: 24.7318\n",
      "Epoch 292/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.4436 - val_loss: 22.1765\n",
      "Epoch 293/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.2755 - val_loss: 23.9116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 294/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.6437 - val_loss: 23.3937\n",
      "Epoch 295/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.3669 - val_loss: 22.5880\n",
      "Epoch 296/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.3784 - val_loss: 23.0546\n",
      "Epoch 297/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.8300 - val_loss: 24.2740\n",
      "Epoch 298/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.9444 - val_loss: 23.3125\n",
      "Epoch 299/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.4340 - val_loss: 23.9826\n",
      "Epoch 300/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.5093 - val_loss: 24.0502\n",
      "Epoch 301/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.4694 - val_loss: 22.7480\n",
      "Epoch 302/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.7858 - val_loss: 23.0920\n",
      "Epoch 303/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.1016 - val_loss: 22.8225\n",
      "Epoch 304/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.6831 - val_loss: 23.8374\n",
      "Epoch 305/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.3768 - val_loss: 25.4255\n",
      "Epoch 306/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.2216 - val_loss: 22.6392\n",
      "Epoch 307/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.5444 - val_loss: 23.2124\n",
      "Epoch 308/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.6647 - val_loss: 24.3901\n",
      "Epoch 309/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.7007 - val_loss: 23.9593\n",
      "Epoch 310/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.2037 - val_loss: 23.1051\n",
      "Epoch 311/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.6155 - val_loss: 23.7236\n",
      "Epoch 312/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.0254 - val_loss: 23.1457\n",
      "Epoch 313/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.6251 - val_loss: 23.4751\n",
      "Epoch 314/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.5416 - val_loss: 23.1769\n",
      "Epoch 315/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.1654 - val_loss: 23.8843\n",
      "Epoch 316/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.1815 - val_loss: 23.6049\n",
      "Epoch 317/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.2781 - val_loss: 23.1986\n",
      "Epoch 318/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.3415 - val_loss: 23.4945\n",
      "Epoch 319/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.4047 - val_loss: 22.9657\n",
      "Epoch 320/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.5678 - val_loss: 22.7230\n",
      "Epoch 321/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.8716 - val_loss: 24.5807\n",
      "Epoch 322/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.5365 - val_loss: 24.6417\n",
      "Epoch 323/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.4369 - val_loss: 24.1275\n",
      "Epoch 324/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.1791 - val_loss: 23.4098\n",
      "Epoch 325/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.0976 - val_loss: 25.2085\n",
      "Epoch 326/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.0632 - val_loss: 23.6730\n",
      "Epoch 327/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.6681 - val_loss: 23.5137\n",
      "Epoch 328/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.1179 - val_loss: 22.7150\n",
      "Epoch 329/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.8729 - val_loss: 23.8327\n",
      "Epoch 330/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.1254 - val_loss: 25.2161\n",
      "Epoch 331/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.8865 - val_loss: 23.0063\n",
      "Epoch 332/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.2733 - val_loss: 23.5611\n",
      "Epoch 333/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.2359 - val_loss: 23.8005\n",
      "Epoch 334/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.1553 - val_loss: 22.8595\n",
      "Epoch 335/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.1749 - val_loss: 25.2076\n",
      "Epoch 336/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.6988 - val_loss: 23.1135\n",
      "Epoch 337/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.8011 - val_loss: 23.8815\n",
      "Epoch 338/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.1641 - val_loss: 23.0354\n",
      "Epoch 339/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.4667 - val_loss: 22.7052\n",
      "Epoch 340/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.7726 - val_loss: 24.1952\n",
      "Epoch 341/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.7877 - val_loss: 24.0468\n",
      "Epoch 342/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.1472 - val_loss: 22.8551\n",
      "Epoch 343/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.0147 - val_loss: 23.5850\n",
      "Epoch 344/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.3358 - val_loss: 23.2231\n",
      "Epoch 345/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.0276 - val_loss: 22.3733\n",
      "Epoch 346/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.3057 - val_loss: 22.7257\n",
      "Epoch 347/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.5418 - val_loss: 25.5789\n",
      "Epoch 348/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.0158 - val_loss: 23.3846\n",
      "Epoch 349/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.7647 - val_loss: 23.3962\n",
      "Epoch 350/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.2991 - val_loss: 23.0016\n",
      "Epoch 351/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.4268 - val_loss: 23.8952\n",
      "Epoch 352/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.0071 - val_loss: 22.9008\n",
      "Epoch 353/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.1404 - val_loss: 23.4938\n",
      "Epoch 354/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.8534 - val_loss: 23.1999\n",
      "Epoch 355/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.6788 - val_loss: 23.8989\n",
      "Epoch 356/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.6689 - val_loss: 23.2144\n",
      "Epoch 357/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.0510 - val_loss: 23.4969\n",
      "Epoch 358/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.8759 - val_loss: 22.8689\n",
      "Epoch 359/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.9237 - val_loss: 22.2878\n",
      "Epoch 360/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.5565 - val_loss: 22.7647\n",
      "Epoch 361/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.4544 - val_loss: 24.1824\n",
      "Epoch 362/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.1490 - val_loss: 25.3342\n",
      "Epoch 363/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.3960 - val_loss: 23.9286\n",
      "Epoch 364/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.6372 - val_loss: 23.1926\n",
      "Epoch 365/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.2249 - val_loss: 24.0966\n",
      "Epoch 366/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.0224 - val_loss: 23.5537\n",
      "Epoch 367/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.0010 - val_loss: 23.0163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 368/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.3452 - val_loss: 22.5463\n",
      "Epoch 369/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.2932 - val_loss: 25.5960\n",
      "Epoch 370/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.4100 - val_loss: 24.2252\n",
      "Epoch 371/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.8782 - val_loss: 23.6201\n",
      "Epoch 372/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.2828 - val_loss: 23.3627\n",
      "Epoch 373/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.1696 - val_loss: 23.0892\n",
      "Epoch 374/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.5577 - val_loss: 24.6069\n",
      "Epoch 375/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.4764 - val_loss: 23.6937\n",
      "Epoch 376/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.7728 - val_loss: 24.2603\n",
      "Epoch 377/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.0425 - val_loss: 23.1742\n",
      "Epoch 378/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.3682 - val_loss: 24.8200\n",
      "Epoch 379/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.3591 - val_loss: 24.4905\n",
      "Epoch 380/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.1835 - val_loss: 22.6785\n",
      "Epoch 381/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.0236 - val_loss: 22.4215\n",
      "Epoch 382/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.9307 - val_loss: 23.2515\n",
      "Epoch 383/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.9025 - val_loss: 23.7192\n",
      "Epoch 384/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.8839 - val_loss: 22.5670\n",
      "Epoch 385/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.0825 - val_loss: 22.1574\n",
      "Epoch 386/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.7396 - val_loss: 23.4742\n",
      "Epoch 387/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.3385 - val_loss: 22.2106\n",
      "Epoch 388/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.4392 - val_loss: 24.6977\n",
      "Epoch 389/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.7236 - val_loss: 24.9088\n",
      "Epoch 390/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.2159 - val_loss: 22.6454\n",
      "Epoch 391/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.5485 - val_loss: 23.3951\n",
      "Epoch 392/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.7452 - val_loss: 23.7719\n",
      "Epoch 393/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.1185 - val_loss: 24.2984\n",
      "Epoch 394/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.7261 - val_loss: 22.8093\n",
      "Epoch 395/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.6230 - val_loss: 23.3891\n",
      "Epoch 396/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.7123 - val_loss: 23.0457\n",
      "Epoch 397/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.8182 - val_loss: 22.6080\n",
      "Epoch 398/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.5915 - val_loss: 23.9731\n",
      "Epoch 399/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.8553 - val_loss: 22.9777\n",
      "Epoch 400/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.8287 - val_loss: 22.6811\n",
      "Epoch 401/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.1725 - val_loss: 24.8079\n",
      "Epoch 402/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.1399 - val_loss: 24.3463\n",
      "Epoch 403/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.1554 - val_loss: 24.5440\n",
      "Epoch 404/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.7222 - val_loss: 23.5537\n",
      "Epoch 405/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.6497 - val_loss: 22.7146\n",
      "Epoch 406/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.7693 - val_loss: 21.9959\n",
      "Epoch 407/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.8779 - val_loss: 23.5678\n",
      "Epoch 408/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.3336 - val_loss: 22.5987\n",
      "Epoch 409/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.4528 - val_loss: 23.3797\n",
      "Epoch 410/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.1139 - val_loss: 23.0255\n",
      "Epoch 411/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.8918 - val_loss: 23.0230\n",
      "Epoch 412/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.8160 - val_loss: 22.1158\n",
      "Epoch 413/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.6481 - val_loss: 22.4370\n",
      "Epoch 414/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.5831 - val_loss: 22.5004\n",
      "Epoch 415/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.6791 - val_loss: 22.6314\n",
      "Epoch 416/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.3307 - val_loss: 23.7981\n",
      "Epoch 417/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.8353 - val_loss: 23.2697\n",
      "Epoch 418/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.3427 - val_loss: 22.7891\n",
      "Epoch 419/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.7287 - val_loss: 22.4519\n",
      "Epoch 420/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.8801 - val_loss: 22.8865\n",
      "Epoch 421/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.9088 - val_loss: 23.4338\n",
      "Epoch 422/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.9145 - val_loss: 23.3563\n",
      "Epoch 423/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.3254 - val_loss: 22.8692\n",
      "Epoch 424/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.0712 - val_loss: 22.5783\n",
      "Epoch 425/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.9688 - val_loss: 22.5446\n",
      "Epoch 426/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.5017 - val_loss: 23.2287\n",
      "Epoch 427/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.6627 - val_loss: 22.9606\n",
      "Epoch 428/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.2711 - val_loss: 24.8458\n",
      "Epoch 429/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.9320 - val_loss: 23.9706\n",
      "Epoch 430/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.9948 - val_loss: 23.8872\n",
      "Epoch 431/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.0109 - val_loss: 24.0641\n",
      "Epoch 432/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.9127 - val_loss: 23.2776\n",
      "Epoch 433/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.2127 - val_loss: 24.1840\n",
      "Epoch 434/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.1558 - val_loss: 24.7211\n",
      "Epoch 435/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.7083 - val_loss: 22.4140\n",
      "Epoch 436/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.2871 - val_loss: 22.5737\n",
      "Epoch 437/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.2165 - val_loss: 23.2676\n",
      "Epoch 438/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.6927 - val_loss: 22.7329\n",
      "Epoch 439/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.9435 - val_loss: 21.8272\n",
      "Epoch 440/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.8456 - val_loss: 22.3527\n",
      "Epoch 441/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.6453 - val_loss: 23.1222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 442/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.3752 - val_loss: 22.5876\n",
      "Epoch 443/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.0174 - val_loss: 22.1444\n",
      "Epoch 444/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.4365 - val_loss: 24.1405\n",
      "Epoch 445/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.1833 - val_loss: 23.7915\n",
      "Epoch 446/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.7973 - val_loss: 22.2657\n",
      "Epoch 447/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.1866 - val_loss: 22.6800\n",
      "Epoch 448/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.3579 - val_loss: 22.8161\n",
      "Epoch 449/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.5964 - val_loss: 22.4302\n",
      "Epoch 450/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.1045 - val_loss: 23.0573\n",
      "Epoch 451/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.3106 - val_loss: 22.1920\n",
      "Epoch 452/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.5684 - val_loss: 22.8130\n",
      "Epoch 453/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.5732 - val_loss: 22.9673\n",
      "Epoch 454/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.7952 - val_loss: 24.5308\n",
      "Epoch 455/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.1167 - val_loss: 23.0292\n",
      "Epoch 456/2000\n",
      "1500/1500 [==============================] - 0s 33us/sample - loss: 18.7356 - val_loss: 22.2155\n",
      "Epoch 457/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.0113 - val_loss: 21.9705\n",
      "Epoch 458/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.3725 - val_loss: 23.3498\n",
      "Epoch 459/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.5968 - val_loss: 22.8265\n",
      "Epoch 460/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.8247 - val_loss: 22.2614\n",
      "Epoch 461/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.2724 - val_loss: 22.7129\n",
      "Epoch 462/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.1995 - val_loss: 22.5484\n",
      "Epoch 463/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.2463 - val_loss: 22.2453\n",
      "Epoch 464/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.2569 - val_loss: 22.5479\n",
      "Epoch 465/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.2794 - val_loss: 23.3852\n",
      "Epoch 466/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.4706 - val_loss: 22.6841\n",
      "Epoch 467/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.6105 - val_loss: 22.6121\n",
      "Epoch 468/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.0643 - val_loss: 22.0456\n",
      "Epoch 469/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.2788 - val_loss: 22.8781\n",
      "Epoch 470/2000\n",
      "1500/1500 [==============================] - 0s 30us/sample - loss: 18.1600 - val_loss: 23.0071\n",
      "Epoch 471/2000\n",
      "1500/1500 [==============================] - 0s 31us/sample - loss: 18.4900 - val_loss: 22.4478\n",
      "Epoch 472/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.4046 - val_loss: 23.9295\n",
      "Epoch 473/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.1119 - val_loss: 22.6252\n",
      "Epoch 474/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.4175 - val_loss: 23.4610\n",
      "Epoch 475/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.9422 - val_loss: 23.3076\n",
      "Epoch 476/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.5688 - val_loss: 22.6104\n",
      "Epoch 477/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.9284 - val_loss: 22.3441\n",
      "Epoch 478/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.0510 - val_loss: 23.0685\n",
      "Epoch 479/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.6364 - val_loss: 22.2819\n",
      "Epoch 480/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.3304 - val_loss: 22.4229\n",
      "Epoch 481/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.3445 - val_loss: 23.3767\n",
      "Epoch 482/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.1046 - val_loss: 23.8228\n",
      "Epoch 483/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.7940 - val_loss: 23.8089\n",
      "Epoch 484/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.7847 - val_loss: 24.6993\n",
      "Epoch 485/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.9033 - val_loss: 23.3156\n",
      "Epoch 486/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.4213 - val_loss: 23.6733\n",
      "Epoch 487/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.7095 - val_loss: 22.8342\n",
      "Epoch 488/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.2911 - val_loss: 23.7004\n",
      "Epoch 489/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.0335 - val_loss: 23.5262\n",
      "Epoch 490/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.9405 - val_loss: 23.8533\n",
      "Epoch 491/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.8916 - val_loss: 23.7496\n",
      "Epoch 492/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.2252 - val_loss: 22.5226\n",
      "Epoch 493/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.3951 - val_loss: 21.9258\n",
      "Epoch 494/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.3228 - val_loss: 22.6344\n",
      "Epoch 495/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.6299 - val_loss: 24.0044\n",
      "Epoch 496/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.4389 - val_loss: 22.8794\n",
      "Epoch 497/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.3234 - val_loss: 23.7348\n",
      "Epoch 498/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.5628 - val_loss: 22.6286\n",
      "Epoch 499/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.5541 - val_loss: 22.0631\n",
      "Epoch 500/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.9946 - val_loss: 23.5114\n",
      "Epoch 501/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.2779 - val_loss: 23.3012\n",
      "Epoch 502/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.6598 - val_loss: 22.3494\n",
      "Epoch 503/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.4413 - val_loss: 23.5701\n",
      "Epoch 504/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.9690 - val_loss: 24.0137\n",
      "Epoch 505/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.0879 - val_loss: 23.7005\n",
      "Epoch 506/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.1535 - val_loss: 23.6787\n",
      "Epoch 507/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.2151 - val_loss: 23.7405\n",
      "Epoch 508/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.3571 - val_loss: 22.1051\n",
      "Epoch 509/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.6149 - val_loss: 22.1676\n",
      "Epoch 510/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.3416 - val_loss: 23.3889\n",
      "Epoch 511/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.9791 - val_loss: 22.4309\n",
      "Epoch 512/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.2278 - val_loss: 23.2773\n",
      "Epoch 513/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.3735 - val_loss: 24.8421\n",
      "Epoch 514/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 18.7021 - val_loss: 23.2447\n",
      "Epoch 515/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.7043 - val_loss: 23.2200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 516/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.5096 - val_loss: 23.7023\n",
      "Epoch 517/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.6223 - val_loss: 25.3351\n",
      "Epoch 518/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.4295 - val_loss: 23.5641\n",
      "Epoch 519/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.6074 - val_loss: 22.8065\n",
      "Epoch 520/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.8348 - val_loss: 24.1258\n",
      "Epoch 521/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.1482 - val_loss: 22.0364\n",
      "Epoch 522/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.4340 - val_loss: 22.8861\n",
      "Epoch 523/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.3503 - val_loss: 23.8692\n",
      "Epoch 524/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.8095 - val_loss: 23.0644\n",
      "Epoch 525/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.3755 - val_loss: 23.4452\n",
      "Epoch 526/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.3946 - val_loss: 22.3522\n",
      "Epoch 527/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.1356 - val_loss: 23.6013\n",
      "Epoch 528/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.6942 - val_loss: 23.4803\n",
      "Epoch 529/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.3540 - val_loss: 23.5955\n",
      "Epoch 530/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.7613 - val_loss: 23.4811\n",
      "Epoch 531/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.8941 - val_loss: 23.4412\n",
      "Epoch 532/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.4251 - val_loss: 23.1183\n",
      "Epoch 533/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.0054 - val_loss: 22.7731\n",
      "Epoch 534/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.0890 - val_loss: 23.7630\n",
      "Epoch 535/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.8742 - val_loss: 23.2848\n",
      "Epoch 536/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.1231 - val_loss: 23.6167\n",
      "Epoch 537/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.5627 - val_loss: 23.6662\n",
      "Epoch 538/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.5940 - val_loss: 25.4163\n",
      "Epoch 539/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.7914 - val_loss: 23.1151\n",
      "Epoch 540/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.1810 - val_loss: 22.7881\n",
      "Epoch 541/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.2349 - val_loss: 22.9253\n",
      "Epoch 542/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.2495 - val_loss: 24.2650\n",
      "Epoch 543/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.4915 - val_loss: 22.9212\n",
      "Epoch 544/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.8167 - val_loss: 22.9325\n",
      "Epoch 545/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.8319 - val_loss: 22.8023\n",
      "Epoch 546/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.4832 - val_loss: 23.3488\n",
      "Epoch 547/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.9008 - val_loss: 24.5190\n",
      "Epoch 548/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.4093 - val_loss: 22.8379\n",
      "Epoch 549/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.2921 - val_loss: 23.1180\n",
      "Epoch 550/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.9096 - val_loss: 23.7020\n",
      "Epoch 551/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.1719 - val_loss: 22.6088\n",
      "Epoch 552/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.9578 - val_loss: 22.5617\n",
      "Epoch 553/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.2460 - val_loss: 22.3504\n",
      "Epoch 554/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.6943 - val_loss: 22.4424\n",
      "Epoch 555/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.4652 - val_loss: 23.7807\n",
      "Epoch 556/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.0309 - val_loss: 22.6951\n",
      "Epoch 557/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.8370 - val_loss: 25.3518\n",
      "Epoch 558/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.5006 - val_loss: 23.1590\n",
      "Epoch 559/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.3211 - val_loss: 22.7731\n",
      "Epoch 560/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.6858 - val_loss: 22.6098\n",
      "Epoch 561/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.3301 - val_loss: 22.9355\n",
      "Epoch 562/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.9405 - val_loss: 23.5500\n",
      "Epoch 563/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.2586 - val_loss: 24.9587\n",
      "Epoch 564/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.7758 - val_loss: 23.3452\n",
      "Epoch 565/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.5436 - val_loss: 23.1458\n",
      "Epoch 566/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.6279 - val_loss: 23.1373\n",
      "Epoch 567/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.8236 - val_loss: 22.7671\n",
      "Epoch 568/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.5975 - val_loss: 23.2801\n",
      "Epoch 569/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.0512 - val_loss: 22.2634\n",
      "Epoch 570/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.6592 - val_loss: 23.0413\n",
      "Epoch 571/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.6340 - val_loss: 23.2102\n",
      "Epoch 572/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.7908 - val_loss: 23.3920\n",
      "Epoch 573/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.6104 - val_loss: 23.4115\n",
      "Epoch 574/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.8509 - val_loss: 22.8035\n",
      "Epoch 575/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.8556 - val_loss: 22.8742\n",
      "Epoch 576/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.0558 - val_loss: 23.3163\n",
      "Epoch 577/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.9307 - val_loss: 23.7416\n",
      "Epoch 578/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.1112 - val_loss: 23.6530\n",
      "Epoch 579/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.5811 - val_loss: 22.4170\n",
      "Epoch 580/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.7011 - val_loss: 24.7922\n",
      "Epoch 581/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.1366 - val_loss: 21.9266\n",
      "Epoch 582/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.7236 - val_loss: 22.4283\n",
      "Epoch 583/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.6464 - val_loss: 23.5066\n",
      "Epoch 584/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.7631 - val_loss: 22.0270\n",
      "Epoch 585/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.7082 - val_loss: 22.4081\n",
      "Epoch 586/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.8981 - val_loss: 24.2451\n",
      "Epoch 587/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.9654 - val_loss: 22.9603\n",
      "Epoch 588/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.9083 - val_loss: 23.4742\n",
      "Epoch 589/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.9202 - val_loss: 23.5155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 590/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.9588 - val_loss: 23.2372\n",
      "Epoch 591/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.3801 - val_loss: 23.6256\n",
      "Epoch 592/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.4119 - val_loss: 22.0443\n",
      "Epoch 593/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.8340 - val_loss: 23.7073\n",
      "Epoch 594/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.8646 - val_loss: 22.9098\n",
      "Epoch 595/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.5079 - val_loss: 23.7236\n",
      "Epoch 596/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.9802 - val_loss: 23.8788\n",
      "Epoch 597/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.6704 - val_loss: 23.2761\n",
      "Epoch 598/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.6861 - val_loss: 22.4102\n",
      "Epoch 599/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.3452 - val_loss: 23.2888\n",
      "Epoch 600/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.4546 - val_loss: 23.5658\n",
      "Epoch 601/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.6390 - val_loss: 22.3189\n",
      "Epoch 602/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.2245 - val_loss: 22.3409\n",
      "Epoch 603/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.6976 - val_loss: 23.6111\n",
      "Epoch 604/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.4283 - val_loss: 22.4513\n",
      "Epoch 605/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.0423 - val_loss: 23.2644\n",
      "Epoch 606/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.6375 - val_loss: 23.5012\n",
      "Epoch 607/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.2243 - val_loss: 22.6439\n",
      "Epoch 608/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.0124 - val_loss: 23.1381\n",
      "Epoch 609/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.7263 - val_loss: 23.6126\n",
      "Epoch 610/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.5377 - val_loss: 23.4461\n",
      "Epoch 611/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.4986 - val_loss: 24.6088\n",
      "Epoch 612/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.5531 - val_loss: 23.2266\n",
      "Epoch 613/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.8274 - val_loss: 22.3921\n",
      "Epoch 614/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.6471 - val_loss: 23.0454\n",
      "Epoch 615/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.5648 - val_loss: 23.4218\n",
      "Epoch 616/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.3194 - val_loss: 23.6700\n",
      "Epoch 617/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.1851 - val_loss: 22.8593\n",
      "Epoch 618/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.7766 - val_loss: 22.2783\n",
      "Epoch 619/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.1381 - val_loss: 22.7770\n",
      "Epoch 620/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.4658 - val_loss: 24.5262\n",
      "Epoch 621/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.0649 - val_loss: 22.0071\n",
      "Epoch 622/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.8979 - val_loss: 21.9661\n",
      "Epoch 623/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.8022 - val_loss: 23.4430\n",
      "Epoch 624/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.1156 - val_loss: 23.2480\n",
      "Epoch 625/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.9041 - val_loss: 23.4253\n",
      "Epoch 626/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.0011 - val_loss: 22.5770\n",
      "Epoch 627/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.4271 - val_loss: 22.9512\n",
      "Epoch 628/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.4887 - val_loss: 21.9598\n",
      "Epoch 629/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.3407 - val_loss: 22.0518\n",
      "Epoch 630/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.5948 - val_loss: 22.4952\n",
      "Epoch 631/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.6967 - val_loss: 22.5310\n",
      "Epoch 632/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.4987 - val_loss: 22.3167\n",
      "Epoch 633/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.5036 - val_loss: 24.1841\n",
      "Epoch 634/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.9249 - val_loss: 22.7313\n",
      "Epoch 635/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.5572 - val_loss: 22.6307\n",
      "Epoch 636/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.5515 - val_loss: 22.0702\n",
      "Epoch 637/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.4282 - val_loss: 22.6960\n",
      "Epoch 638/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.7301 - val_loss: 21.9752\n",
      "Epoch 639/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.9418 - val_loss: 23.3300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|███████████████████████████████████████████████████▎                              | 10/16 [05:40<03:18, 33.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'batc_normalization': False, 'batch_size': 64, 'dropout': 0, 'epochs': 2000, 'first_neuron': 160, 'hidden_layers': 2, 'hidden_neuron': 50, 'kernel_initializer': 'normal', 'last_activation': 'linear', 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "Train on 1500 samples, validate on 375 samples\n",
      "Epoch 1/2000\n",
      "1500/1500 [==============================] - 0s 243us/sample - loss: 2185.8702 - val_loss: 2185.7091\n",
      "Epoch 2/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 2179.3398 - val_loss: 2167.4844\n",
      "Epoch 3/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 2129.0086 - val_loss: 2057.3575\n",
      "Epoch 4/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 1903.4412 - val_loss: 1645.0500\n",
      "Epoch 5/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 1225.1766 - val_loss: 740.2101\n",
      "Epoch 6/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 636.6411 - val_loss: 534.2038\n",
      "Epoch 7/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 523.6642 - val_loss: 459.9879\n",
      "Epoch 8/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 441.5727 - val_loss: 382.1220\n",
      "Epoch 9/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 362.9524 - val_loss: 308.0168\n",
      "Epoch 10/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 286.5201 - val_loss: 240.8695\n",
      "Epoch 11/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 219.0061 - val_loss: 188.2410\n",
      "Epoch 12/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 165.9977 - val_loss: 142.2556\n",
      "Epoch 13/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 128.4074 - val_loss: 112.8418\n",
      "Epoch 14/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 101.1165 - val_loss: 91.8397\n",
      "Epoch 15/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 83.6779 - val_loss: 75.6215\n",
      "Epoch 16/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 67.9821 - val_loss: 65.0256\n",
      "Epoch 17/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 56.3247 - val_loss: 55.2291\n",
      "Epoch 18/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 47.5981 - val_loss: 48.7854\n",
      "Epoch 19/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 42.3803 - val_loss: 42.6027\n",
      "Epoch 20/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 38.3974 - val_loss: 38.7831\n",
      "Epoch 21/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 35.0025 - val_loss: 35.6324\n",
      "Epoch 22/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 32.5339 - val_loss: 32.9250\n",
      "Epoch 23/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 30.7121 - val_loss: 31.2920\n",
      "Epoch 24/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 30.6946 - val_loss: 31.6605\n",
      "Epoch 25/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 28.5134 - val_loss: 29.2156\n",
      "Epoch 26/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 27.4876 - val_loss: 27.2761\n",
      "Epoch 27/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 25.8103 - val_loss: 26.9533\n",
      "Epoch 28/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 26.4649 - val_loss: 29.9979\n",
      "Epoch 29/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 26.6540 - val_loss: 26.0135\n",
      "Epoch 30/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 26.2097 - val_loss: 25.5498\n",
      "Epoch 31/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 25.4340 - val_loss: 26.0213\n",
      "Epoch 32/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 23.4526 - val_loss: 24.7182\n",
      "Epoch 33/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 23.5627 - val_loss: 28.2496\n",
      "Epoch 34/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 23.9185 - val_loss: 24.7557\n",
      "Epoch 35/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 22.9473 - val_loss: 24.7386\n",
      "Epoch 36/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 23.0913 - val_loss: 24.3771\n",
      "Epoch 37/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 23.2427 - val_loss: 25.1689\n",
      "Epoch 38/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 23.4066 - val_loss: 25.4467\n",
      "Epoch 39/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 25.3966 - val_loss: 24.4168\n",
      "Epoch 40/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 24.5891 - val_loss: 24.2526\n",
      "Epoch 41/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 24.3810 - val_loss: 23.9230\n",
      "Epoch 42/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 23.2112 - val_loss: 23.8954\n",
      "Epoch 43/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 23.3577 - val_loss: 25.0543\n",
      "Epoch 44/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 22.9518 - val_loss: 27.3801\n",
      "Epoch 45/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 23.6222 - val_loss: 24.8301\n",
      "Epoch 46/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 23.2511 - val_loss: 25.6537\n",
      "Epoch 47/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 22.2306 - val_loss: 25.0408\n",
      "Epoch 48/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 22.1743 - val_loss: 24.3729\n",
      "Epoch 49/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 25.0262 - val_loss: 25.9737\n",
      "Epoch 50/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 25.5471 - val_loss: 25.8032\n",
      "Epoch 51/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 25.4927 - val_loss: 26.7613\n",
      "Epoch 52/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 23.7240 - val_loss: 25.9648\n",
      "Epoch 53/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 21.8876 - val_loss: 23.6060\n",
      "Epoch 54/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 22.0643 - val_loss: 27.2447\n",
      "Epoch 55/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.9775 - val_loss: 24.8787\n",
      "Epoch 56/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 23.4718 - val_loss: 26.1871\n",
      "Epoch 57/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 23.2361 - val_loss: 24.4588\n",
      "Epoch 58/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 22.9535 - val_loss: 23.9099\n",
      "Epoch 59/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 22.5983 - val_loss: 23.6292\n",
      "Epoch 60/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 21.8719 - val_loss: 24.5103\n",
      "Epoch 61/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 22.0527 - val_loss: 25.0263\n",
      "Epoch 62/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 22.2800 - val_loss: 26.2030\n",
      "Epoch 63/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 22.0722 - val_loss: 26.8682\n",
      "Epoch 64/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 22.4133 - val_loss: 30.1872\n",
      "Epoch 65/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 22.3904 - val_loss: 24.3537\n",
      "Epoch 66/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.8739 - val_loss: 24.2407\n",
      "Epoch 67/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 22.2832 - val_loss: 30.3666\n",
      "Epoch 68/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 23.4042 - val_loss: 24.8649\n",
      "Epoch 69/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 22.8556 - val_loss: 24.9790\n",
      "Epoch 70/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 22.5245 - val_loss: 27.8095\n",
      "Epoch 71/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 22.0860 - val_loss: 24.3400\n",
      "Epoch 72/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 21.8068 - val_loss: 22.5148\n",
      "Epoch 73/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.6462 - val_loss: 24.5775\n",
      "Epoch 74/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.9020 - val_loss: 24.2488\n",
      "Epoch 75/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.5610 - val_loss: 26.1728\n",
      "Epoch 76/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 21.7097 - val_loss: 25.5399\n",
      "Epoch 77/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.9938 - val_loss: 24.4802\n",
      "Epoch 78/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.9507 - val_loss: 24.1328\n",
      "Epoch 79/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.2942 - val_loss: 23.8890\n",
      "Epoch 80/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.7881 - val_loss: 23.1996\n",
      "Epoch 81/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.4026 - val_loss: 23.3740\n",
      "Epoch 82/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.0314 - val_loss: 25.0816\n",
      "Epoch 83/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.9911 - val_loss: 23.7150\n",
      "Epoch 84/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.1932 - val_loss: 23.6305\n",
      "Epoch 85/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.6922 - val_loss: 24.1279\n",
      "Epoch 86/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 21.7400 - val_loss: 27.4315\n",
      "Epoch 87/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.3042 - val_loss: 25.9567\n",
      "Epoch 88/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 20.7757 - val_loss: 23.6686\n",
      "Epoch 89/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 22.9187 - val_loss: 24.4148\n",
      "Epoch 90/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.7005 - val_loss: 25.6506\n",
      "Epoch 91/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 21.2854 - val_loss: 28.2764\n",
      "Epoch 92/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 23.0027 - val_loss: 25.0171\n",
      "Epoch 93/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.6478 - val_loss: 23.8860\n",
      "Epoch 94/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 22.1153 - val_loss: 24.3568\n",
      "Epoch 95/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.6791 - val_loss: 24.2203\n",
      "Epoch 96/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 21.9692 - val_loss: 24.3337\n",
      "Epoch 97/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.2308 - val_loss: 26.1221\n",
      "Epoch 98/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.2504 - val_loss: 24.5642\n",
      "Epoch 99/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.7221 - val_loss: 23.7618\n",
      "Epoch 100/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.0753 - val_loss: 24.9432\n",
      "Epoch 101/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 20.4532 - val_loss: 23.5793\n",
      "Epoch 102/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.5763 - val_loss: 23.4261\n",
      "Epoch 103/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 21.9413 - val_loss: 23.8546\n",
      "Epoch 104/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.5797 - val_loss: 23.4816\n",
      "Epoch 105/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.2529 - val_loss: 24.0187\n",
      "Epoch 106/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.8396 - val_loss: 24.1844\n",
      "Epoch 107/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.9779 - val_loss: 24.0015\n",
      "Epoch 108/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 21.0773 - val_loss: 22.7422\n",
      "Epoch 109/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.2740 - val_loss: 23.3792\n",
      "Epoch 110/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.5806 - val_loss: 23.6249\n",
      "Epoch 111/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.4051 - val_loss: 23.9051\n",
      "Epoch 112/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 20.8334 - val_loss: 23.0288\n",
      "Epoch 113/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.0986 - val_loss: 24.0285\n",
      "Epoch 114/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.8916 - val_loss: 23.7456\n",
      "Epoch 115/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.8987 - val_loss: 23.6544\n",
      "Epoch 116/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.0308 - val_loss: 24.8837\n",
      "Epoch 117/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.8316 - val_loss: 24.3697\n",
      "Epoch 118/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.8672 - val_loss: 24.2144\n",
      "Epoch 119/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 22.1925 - val_loss: 23.1099\n",
      "Epoch 120/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.9500 - val_loss: 24.4296\n",
      "Epoch 121/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.2224 - val_loss: 26.4453\n",
      "Epoch 122/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 22.2575 - val_loss: 23.0769\n",
      "Epoch 123/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 20.7614 - val_loss: 22.8908\n",
      "Epoch 124/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.3983 - val_loss: 23.0880\n",
      "Epoch 125/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.5561 - val_loss: 24.6219\n",
      "Epoch 126/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 20.8649 - val_loss: 23.3437\n",
      "Epoch 127/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.1559 - val_loss: 24.6819\n",
      "Epoch 128/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.1884 - val_loss: 25.6912\n",
      "Epoch 129/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.6123 - val_loss: 22.6602\n",
      "Epoch 130/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.4496 - val_loss: 23.0039\n",
      "Epoch 131/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 20.2048 - val_loss: 24.1820\n",
      "Epoch 132/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.7853 - val_loss: 23.8987\n",
      "Epoch 133/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 20.3857 - val_loss: 22.9732\n",
      "Epoch 134/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.4844 - val_loss: 24.5441\n",
      "Epoch 135/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.9927 - val_loss: 23.4985\n",
      "Epoch 136/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.1486 - val_loss: 22.2356\n",
      "Epoch 137/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 22.3792 - val_loss: 23.6368\n",
      "Epoch 138/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.2860 - val_loss: 24.0703\n",
      "Epoch 139/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.0893 - val_loss: 24.7729\n",
      "Epoch 140/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.3556 - val_loss: 23.2571\n",
      "Epoch 141/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 20.3483 - val_loss: 23.5643\n",
      "Epoch 142/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.6138 - val_loss: 24.3915\n",
      "Epoch 143/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.9235 - val_loss: 23.3612\n",
      "Epoch 144/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.4254 - val_loss: 23.6994\n",
      "Epoch 145/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.7647 - val_loss: 23.8117\n",
      "Epoch 146/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.6798 - val_loss: 25.0548\n",
      "Epoch 147/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.7046 - val_loss: 24.2154\n",
      "Epoch 148/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.9502 - val_loss: 23.9671\n",
      "Epoch 149/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 20.1460 - val_loss: 24.5018\n",
      "Epoch 150/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.3972 - val_loss: 25.6933\n",
      "Epoch 151/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 20.1090 - val_loss: 24.2667\n",
      "Epoch 152/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 20.8826 - val_loss: 23.5995\n",
      "Epoch 153/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.2221 - val_loss: 23.2340\n",
      "Epoch 154/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.7815 - val_loss: 24.9334\n",
      "Epoch 155/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.2373 - val_loss: 23.3040\n",
      "Epoch 156/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.1062 - val_loss: 26.4464\n",
      "Epoch 157/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.8709 - val_loss: 23.6418\n",
      "Epoch 158/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.2632 - val_loss: 25.2306\n",
      "Epoch 159/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.8053 - val_loss: 24.0004\n",
      "Epoch 160/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.3068 - val_loss: 25.0833\n",
      "Epoch 161/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 21.1206 - val_loss: 25.6780\n",
      "Epoch 162/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.1214 - val_loss: 25.8251\n",
      "Epoch 163/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.9621 - val_loss: 24.7942\n",
      "Epoch 164/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 19.7602 - val_loss: 23.2928\n",
      "Epoch 165/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 20.3099 - val_loss: 24.7929\n",
      "Epoch 166/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 20.1012 - val_loss: 24.1276\n",
      "Epoch 167/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.5855 - val_loss: 23.7453\n",
      "Epoch 168/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 20.2437 - val_loss: 22.6933\n",
      "Epoch 169/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.1559 - val_loss: 23.0488\n",
      "Epoch 170/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.4392 - val_loss: 24.6006\n",
      "Epoch 171/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.2614 - val_loss: 23.3514\n",
      "Epoch 172/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.9865 - val_loss: 22.7730\n",
      "Epoch 173/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.9500 - val_loss: 22.9820\n",
      "Epoch 174/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.1834 - val_loss: 22.7689\n",
      "Epoch 175/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.2651 - val_loss: 25.6363\n",
      "Epoch 176/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.8236 - val_loss: 23.9355\n",
      "Epoch 177/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.1602 - val_loss: 23.7554\n",
      "Epoch 178/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.6370 - val_loss: 24.7678\n",
      "Epoch 179/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.5726 - val_loss: 24.4231\n",
      "Epoch 180/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.3950 - val_loss: 23.9869\n",
      "Epoch 181/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.2448 - val_loss: 23.7282\n",
      "Epoch 182/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.4007 - val_loss: 23.7192\n",
      "Epoch 183/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.6480 - val_loss: 23.7742\n",
      "Epoch 184/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.7222 - val_loss: 26.4786\n",
      "Epoch 185/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.5689 - val_loss: 24.6648\n",
      "Epoch 186/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 20.3414 - val_loss: 27.5037\n",
      "Epoch 187/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.9956 - val_loss: 23.2642\n",
      "Epoch 188/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.9397 - val_loss: 23.8451\n",
      "Epoch 189/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.1475 - val_loss: 23.4658\n",
      "Epoch 190/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.5336 - val_loss: 24.2139\n",
      "Epoch 191/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 21.2022 - val_loss: 23.7290\n",
      "Epoch 192/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.3638 - val_loss: 23.7625\n",
      "Epoch 193/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.8976 - val_loss: 23.1063\n",
      "Epoch 194/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.6075 - val_loss: 22.8216\n",
      "Epoch 195/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.8890 - val_loss: 22.9705\n",
      "Epoch 196/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.7877 - val_loss: 24.5505\n",
      "Epoch 197/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.7327 - val_loss: 22.7238\n",
      "Epoch 198/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.1070 - val_loss: 24.3889\n",
      "Epoch 199/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 20.2356 - val_loss: 24.9930\n",
      "Epoch 200/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 21.2717 - val_loss: 22.4894\n",
      "Epoch 201/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.9889 - val_loss: 23.6016\n",
      "Epoch 202/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.6564 - val_loss: 23.3810\n",
      "Epoch 203/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.8345 - val_loss: 24.6392\n",
      "Epoch 204/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.0160 - val_loss: 25.3819\n",
      "Epoch 205/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.8014 - val_loss: 22.6585\n",
      "Epoch 206/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.0300 - val_loss: 24.8288\n",
      "Epoch 207/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.5027 - val_loss: 24.7829\n",
      "Epoch 208/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.9041 - val_loss: 25.9267\n",
      "Epoch 209/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 20.1453 - val_loss: 23.8924\n",
      "Epoch 210/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.6672 - val_loss: 27.4520\n",
      "Epoch 211/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.2488 - val_loss: 24.1366\n",
      "Epoch 212/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.4673 - val_loss: 23.2289\n",
      "Epoch 213/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.3392 - val_loss: 25.5798\n",
      "Epoch 214/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.1799 - val_loss: 23.6821\n",
      "Epoch 215/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.9646 - val_loss: 26.2331\n",
      "Epoch 216/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.5002 - val_loss: 23.5086\n",
      "Epoch 217/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.7822 - val_loss: 25.5869\n",
      "Epoch 218/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.9993 - val_loss: 27.8096\n",
      "Epoch 219/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.2919 - val_loss: 24.2241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 220/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 20.2877 - val_loss: 27.8467\n",
      "Epoch 221/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.6176 - val_loss: 25.4702\n",
      "Epoch 222/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.7924 - val_loss: 23.6504\n",
      "Epoch 223/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.7277 - val_loss: 25.0861\n",
      "Epoch 224/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.5797 - val_loss: 24.2474\n",
      "Epoch 225/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.8604 - val_loss: 24.5129\n",
      "Epoch 226/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.9463 - val_loss: 22.9175\n",
      "Epoch 227/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.1385 - val_loss: 23.9715\n",
      "Epoch 228/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.8265 - val_loss: 26.3572\n",
      "Epoch 229/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.7256 - val_loss: 24.3123\n",
      "Epoch 230/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 21.3418 - val_loss: 27.7918\n",
      "Epoch 231/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.2203 - val_loss: 24.2426\n",
      "Epoch 232/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.8993 - val_loss: 24.3622\n",
      "Epoch 233/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.8132 - val_loss: 23.5732\n",
      "Epoch 234/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.9072 - val_loss: 24.4339\n",
      "Epoch 235/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.2882 - val_loss: 25.8124\n",
      "Epoch 236/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.9099 - val_loss: 26.1562\n",
      "Epoch 237/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.9675 - val_loss: 25.7052\n",
      "Epoch 238/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.9236 - val_loss: 23.2873\n",
      "Epoch 239/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.6357 - val_loss: 22.8208\n",
      "Epoch 240/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.4937 - val_loss: 24.3307\n",
      "Epoch 241/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.1873 - val_loss: 23.6586\n",
      "Epoch 242/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.5486 - val_loss: 25.0711\n",
      "Epoch 243/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.5058 - val_loss: 25.0905\n",
      "Epoch 244/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.4405 - val_loss: 25.2149\n",
      "Epoch 245/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.1699 - val_loss: 24.7632\n",
      "Epoch 246/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 22.1991 - val_loss: 24.4502\n",
      "Epoch 247/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.4739 - val_loss: 22.7524\n",
      "Epoch 248/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.0573 - val_loss: 22.8132\n",
      "Epoch 249/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 20.3089 - val_loss: 23.5502\n",
      "Epoch 250/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.1522 - val_loss: 23.6474\n",
      "Epoch 251/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.7162 - val_loss: 23.3798\n",
      "Epoch 252/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.5358 - val_loss: 24.0129\n",
      "Epoch 253/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.8380 - val_loss: 23.5936\n",
      "Epoch 254/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.2872 - val_loss: 24.3544\n",
      "Epoch 255/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.4889 - val_loss: 22.8843\n",
      "Epoch 256/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.6996 - val_loss: 24.2904\n",
      "Epoch 257/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.8543 - val_loss: 23.4318\n",
      "Epoch 258/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.2696 - val_loss: 24.8927\n",
      "Epoch 259/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.8410 - val_loss: 23.5058\n",
      "Epoch 260/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.2017 - val_loss: 22.9964\n",
      "Epoch 261/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.3314 - val_loss: 25.1108\n",
      "Epoch 262/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.7085 - val_loss: 23.4890\n",
      "Epoch 263/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.1481 - val_loss: 25.9977\n",
      "Epoch 264/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.4781 - val_loss: 26.1050\n",
      "Epoch 265/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.4152 - val_loss: 23.9819\n",
      "Epoch 266/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.3159 - val_loss: 22.4135\n",
      "Epoch 267/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.8773 - val_loss: 22.6449\n",
      "Epoch 268/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.9453 - val_loss: 23.6701\n",
      "Epoch 269/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.6146 - val_loss: 22.8072\n",
      "Epoch 270/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.2465 - val_loss: 22.1064\n",
      "Epoch 271/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.3681 - val_loss: 23.4270\n",
      "Epoch 272/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.4313 - val_loss: 24.4877\n",
      "Epoch 273/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.6475 - val_loss: 23.3495\n",
      "Epoch 274/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.1333 - val_loss: 22.3967\n",
      "Epoch 275/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.3194 - val_loss: 23.7616\n",
      "Epoch 276/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.1252 - val_loss: 24.8373\n",
      "Epoch 277/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.7547 - val_loss: 24.4672\n",
      "Epoch 278/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.7691 - val_loss: 26.2905\n",
      "Epoch 279/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.5688 - val_loss: 25.2800\n",
      "Epoch 280/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.5008 - val_loss: 25.4431\n",
      "Epoch 281/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.9726 - val_loss: 24.9207\n",
      "Epoch 282/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.0157 - val_loss: 26.4008\n",
      "Epoch 283/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.5382 - val_loss: 23.6414\n",
      "Epoch 284/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.8627 - val_loss: 23.0659\n",
      "Epoch 285/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.3717 - val_loss: 23.3858\n",
      "Epoch 286/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.0805 - val_loss: 24.7099\n",
      "Epoch 287/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.7814 - val_loss: 23.5059\n",
      "Epoch 288/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.4125 - val_loss: 24.7748\n",
      "Epoch 289/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.0505 - val_loss: 24.1762\n",
      "Epoch 290/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.9247 - val_loss: 24.5762\n",
      "Epoch 291/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.5152 - val_loss: 24.8150\n",
      "Epoch 292/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.0003 - val_loss: 23.2096\n",
      "Epoch 293/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.3833 - val_loss: 26.7055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 294/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.4058 - val_loss: 26.0713\n",
      "Epoch 295/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.0172 - val_loss: 22.8541\n",
      "Epoch 296/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.8603 - val_loss: 22.6528\n",
      "Epoch 297/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 20.1241 - val_loss: 24.8901\n",
      "Epoch 298/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.8281 - val_loss: 24.7186\n",
      "Epoch 299/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.9713 - val_loss: 23.4854\n",
      "Epoch 300/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.7844 - val_loss: 24.4195\n",
      "Epoch 301/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.7779 - val_loss: 24.0303\n",
      "Epoch 302/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.8958 - val_loss: 24.9981\n",
      "Epoch 303/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.8474 - val_loss: 22.9793\n",
      "Epoch 304/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.9128 - val_loss: 24.3120\n",
      "Epoch 305/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.4596 - val_loss: 24.5695\n",
      "Epoch 306/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.6991 - val_loss: 24.4964\n",
      "Epoch 307/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.6020 - val_loss: 23.1840\n",
      "Epoch 308/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.9014 - val_loss: 23.1864\n",
      "Epoch 309/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.6340 - val_loss: 23.2033\n",
      "Epoch 310/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.0226 - val_loss: 22.3930\n",
      "Epoch 311/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.5979 - val_loss: 23.2509\n",
      "Epoch 312/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.4099 - val_loss: 25.4427\n",
      "Epoch 313/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.6551 - val_loss: 22.7916\n",
      "Epoch 314/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.8950 - val_loss: 24.0380\n",
      "Epoch 315/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.8844 - val_loss: 24.6243\n",
      "Epoch 316/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.7218 - val_loss: 22.8038\n",
      "Epoch 317/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.6592 - val_loss: 25.4063\n",
      "Epoch 318/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.6826 - val_loss: 24.1277\n",
      "Epoch 319/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.7344 - val_loss: 24.7772\n",
      "Epoch 320/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.7998 - val_loss: 27.0260\n",
      "Epoch 321/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.6023 - val_loss: 23.2680\n",
      "Epoch 322/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.9301 - val_loss: 22.6338\n",
      "Epoch 323/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.1103 - val_loss: 23.1173\n",
      "Epoch 324/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.5318 - val_loss: 23.2731\n",
      "Epoch 325/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.8583 - val_loss: 24.3141\n",
      "Epoch 326/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.7865 - val_loss: 27.3855\n",
      "Epoch 327/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.1830 - val_loss: 23.3795\n",
      "Epoch 328/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.8384 - val_loss: 23.7774\n",
      "Epoch 329/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.2768 - val_loss: 25.3200\n",
      "Epoch 330/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.5439 - val_loss: 24.8971\n",
      "Epoch 331/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.7588 - val_loss: 23.8904\n",
      "Epoch 332/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.8829 - val_loss: 22.6607\n",
      "Epoch 333/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.7798 - val_loss: 23.4225\n",
      "Epoch 334/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.4786 - val_loss: 25.4119\n",
      "Epoch 335/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.3537 - val_loss: 22.7161\n",
      "Epoch 336/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.0629 - val_loss: 26.4052\n",
      "Epoch 337/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 20.4657 - val_loss: 29.8234\n",
      "Epoch 338/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.6471 - val_loss: 25.2205\n",
      "Epoch 339/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.3720 - val_loss: 23.3688\n",
      "Epoch 340/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.2538 - val_loss: 25.0881\n",
      "Epoch 341/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.8335 - val_loss: 25.6836\n",
      "Epoch 342/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 20.7319 - val_loss: 23.8827\n",
      "Epoch 343/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.7071 - val_loss: 22.0042\n",
      "Epoch 344/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.3034 - val_loss: 22.2433\n",
      "Epoch 345/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.8895 - val_loss: 28.5139\n",
      "Epoch 346/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.7399 - val_loss: 24.0918\n",
      "Epoch 347/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.3697 - val_loss: 24.3343\n",
      "Epoch 348/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.5417 - val_loss: 25.1846\n",
      "Epoch 349/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.0258 - val_loss: 24.2416\n",
      "Epoch 350/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.2228 - val_loss: 24.5020\n",
      "Epoch 351/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.4723 - val_loss: 24.0330\n",
      "Epoch 352/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.7360 - val_loss: 23.8338\n",
      "Epoch 353/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.5128 - val_loss: 22.9899\n",
      "Epoch 354/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.3185 - val_loss: 24.3011\n",
      "Epoch 355/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.4121 - val_loss: 23.4861\n",
      "Epoch 356/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.9518 - val_loss: 23.4916\n",
      "Epoch 357/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.5243 - val_loss: 22.3924\n",
      "Epoch 358/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.4958 - val_loss: 23.8728\n",
      "Epoch 359/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.4483 - val_loss: 24.1799\n",
      "Epoch 360/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.3736 - val_loss: 22.6318\n",
      "Epoch 361/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.3646 - val_loss: 24.7967\n",
      "Epoch 362/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.3846 - val_loss: 23.6537\n",
      "Epoch 363/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.5722 - val_loss: 22.4716\n",
      "Epoch 364/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.6096 - val_loss: 23.0902\n",
      "Epoch 365/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.7185 - val_loss: 22.5768\n",
      "Epoch 366/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.7070 - val_loss: 23.1184\n",
      "Epoch 367/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.9707 - val_loss: 24.7639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 368/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.5947 - val_loss: 23.4845\n",
      "Epoch 369/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.5706 - val_loss: 24.6185\n",
      "Epoch 370/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.2225 - val_loss: 23.7966\n",
      "Epoch 371/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.8444 - val_loss: 25.1511\n",
      "Epoch 372/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.9087 - val_loss: 27.4475\n",
      "Epoch 373/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.8323 - val_loss: 23.0837\n",
      "Epoch 374/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.4629 - val_loss: 24.0502\n",
      "Epoch 375/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.6820 - val_loss: 23.1240\n",
      "Epoch 376/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.8304 - val_loss: 22.3229\n",
      "Epoch 377/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.5440 - val_loss: 24.0783\n",
      "Epoch 378/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.1384 - val_loss: 22.6216\n",
      "Epoch 379/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.2886 - val_loss: 23.6719\n",
      "Epoch 380/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.6643 - val_loss: 24.1253\n",
      "Epoch 381/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.3050 - val_loss: 24.9041\n",
      "Epoch 382/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 18.7703 - val_loss: 24.4577\n",
      "Epoch 383/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.9401 - val_loss: 22.8115\n",
      "Epoch 384/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.3844 - val_loss: 23.8141\n",
      "Epoch 385/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.4546 - val_loss: 23.1810\n",
      "Epoch 386/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.0969 - val_loss: 23.4096\n",
      "Epoch 387/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.2270 - val_loss: 22.7244\n",
      "Epoch 388/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.8217 - val_loss: 23.2621\n",
      "Epoch 389/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.2838 - val_loss: 27.4059\n",
      "Epoch 390/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.0694 - val_loss: 23.4638\n",
      "Epoch 391/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.1477 - val_loss: 23.3799\n",
      "Epoch 392/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.3337 - val_loss: 27.2547\n",
      "Epoch 393/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.0288 - val_loss: 24.9817\n",
      "Epoch 394/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.6121 - val_loss: 22.7210\n",
      "Epoch 395/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.9755 - val_loss: 26.9808\n",
      "Epoch 396/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.6337 - val_loss: 24.5240\n",
      "Epoch 397/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.3429 - val_loss: 24.2021\n",
      "Epoch 398/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.8781 - val_loss: 23.0402\n",
      "Epoch 399/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.0632 - val_loss: 23.1920\n",
      "Epoch 400/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.8793 - val_loss: 22.2737\n",
      "Epoch 401/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.6871 - val_loss: 25.2606\n",
      "Epoch 402/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.4470 - val_loss: 25.2460\n",
      "Epoch 403/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.5063 - val_loss: 22.5917\n",
      "Epoch 404/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.4900 - val_loss: 24.5308\n",
      "Epoch 405/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.4540 - val_loss: 22.7749\n",
      "Epoch 406/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.1034 - val_loss: 22.1619\n",
      "Epoch 407/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.4542 - val_loss: 22.0762\n",
      "Epoch 408/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.4546 - val_loss: 25.7548\n",
      "Epoch 409/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.4574 - val_loss: 23.9466\n",
      "Epoch 410/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.5174 - val_loss: 24.7699\n",
      "Epoch 411/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.0477 - val_loss: 24.1770\n",
      "Epoch 412/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.2089 - val_loss: 24.4378\n",
      "Epoch 413/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.1894 - val_loss: 24.7018\n",
      "Epoch 414/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.3618 - val_loss: 25.0385\n",
      "Epoch 415/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.6252 - val_loss: 28.1637\n",
      "Epoch 416/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.0958 - val_loss: 24.0587\n",
      "Epoch 417/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.5934 - val_loss: 23.5367\n",
      "Epoch 418/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.1244 - val_loss: 22.6971\n",
      "Epoch 419/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.3617 - val_loss: 22.4044\n",
      "Epoch 420/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.4954 - val_loss: 23.0949\n",
      "Epoch 421/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.3252 - val_loss: 23.2111\n",
      "Epoch 422/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.3802 - val_loss: 23.8487\n",
      "Epoch 423/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.1219 - val_loss: 24.9226\n",
      "Epoch 424/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.2817 - val_loss: 22.7466\n",
      "Epoch 425/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.9674 - val_loss: 22.7085\n",
      "Epoch 426/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.2527 - val_loss: 22.3830\n",
      "Epoch 427/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.1404 - val_loss: 25.6233\n",
      "Epoch 428/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.3015 - val_loss: 24.7170\n",
      "Epoch 429/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.9362 - val_loss: 22.1132\n",
      "Epoch 430/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.9415 - val_loss: 22.3485\n",
      "Epoch 431/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.2004 - val_loss: 22.5148\n",
      "Epoch 432/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.6203 - val_loss: 23.2658\n",
      "Epoch 433/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.8328 - val_loss: 23.4802\n",
      "Epoch 434/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.3723 - val_loss: 22.9032\n",
      "Epoch 435/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.3771 - val_loss: 24.2833\n",
      "Epoch 436/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.0163 - val_loss: 23.1341\n",
      "Epoch 437/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.2511 - val_loss: 22.0730\n",
      "Epoch 438/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.0892 - val_loss: 23.8338\n",
      "Epoch 439/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 20.2610 - val_loss: 25.9447\n",
      "Epoch 440/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.2552 - val_loss: 23.5401\n",
      "Epoch 441/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.8276 - val_loss: 22.6667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 442/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.8110 - val_loss: 23.8792\n",
      "Epoch 443/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.6127 - val_loss: 23.3794\n",
      "Epoch 444/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.3451 - val_loss: 22.5595\n",
      "Epoch 445/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.1819 - val_loss: 25.7897\n",
      "Epoch 446/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.4744 - val_loss: 24.7736\n",
      "Epoch 447/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.3353 - val_loss: 26.1578\n",
      "Epoch 448/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.2122 - val_loss: 25.2970\n",
      "Epoch 449/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.4450 - val_loss: 22.7750\n",
      "Epoch 450/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.8065 - val_loss: 23.0990\n",
      "Epoch 451/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.5326 - val_loss: 22.5708\n",
      "Epoch 452/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.5814 - val_loss: 23.1799\n",
      "Epoch 453/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.8791 - val_loss: 23.3304\n",
      "Epoch 454/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.5511 - val_loss: 23.8779\n",
      "Epoch 455/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.3015 - val_loss: 23.7562\n",
      "Epoch 456/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.3032 - val_loss: 25.1425\n",
      "Epoch 457/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.1891 - val_loss: 23.5042\n",
      "Epoch 458/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.5797 - val_loss: 24.7439\n",
      "Epoch 459/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.6965 - val_loss: 22.6036\n",
      "Epoch 460/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.0423 - val_loss: 24.7882\n",
      "Epoch 461/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.8278 - val_loss: 24.2346\n",
      "Epoch 462/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.8460 - val_loss: 21.9150\n",
      "Epoch 463/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.8784 - val_loss: 24.1827\n",
      "Epoch 464/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.3957 - val_loss: 24.7755\n",
      "Epoch 465/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.4354 - val_loss: 28.7752\n",
      "Epoch 466/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.2873 - val_loss: 23.6384\n",
      "Epoch 467/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.9358 - val_loss: 22.3641\n",
      "Epoch 468/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.6065 - val_loss: 24.0384\n",
      "Epoch 469/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.8814 - val_loss: 23.6770\n",
      "Epoch 470/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.6594 - val_loss: 22.9460\n",
      "Epoch 471/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.5921 - val_loss: 23.3234\n",
      "Epoch 472/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.3393 - val_loss: 22.7326\n",
      "Epoch 473/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.0340 - val_loss: 22.0284\n",
      "Epoch 474/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.3220 - val_loss: 23.1911\n",
      "Epoch 475/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.5933 - val_loss: 23.3903\n",
      "Epoch 476/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.5162 - val_loss: 23.3945\n",
      "Epoch 477/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.6475 - val_loss: 23.3706\n",
      "Epoch 478/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.6055 - val_loss: 23.7344\n",
      "Epoch 479/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.9286 - val_loss: 24.5709\n",
      "Epoch 480/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.6595 - val_loss: 27.5917\n",
      "Epoch 481/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.3234 - val_loss: 24.6265\n",
      "Epoch 482/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.0254 - val_loss: 23.5028\n",
      "Epoch 483/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.2881 - val_loss: 22.7827\n",
      "Epoch 484/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.6979 - val_loss: 22.0926\n",
      "Epoch 485/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.6661 - val_loss: 26.6385\n",
      "Epoch 486/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.9151 - val_loss: 23.5580\n",
      "Epoch 487/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.8978 - val_loss: 22.4172\n",
      "Epoch 488/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.4633 - val_loss: 23.3042\n",
      "Epoch 489/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.8439 - val_loss: 24.4758\n",
      "Epoch 490/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.1358 - val_loss: 23.7913\n",
      "Epoch 491/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.7179 - val_loss: 22.5185\n",
      "Epoch 492/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.4703 - val_loss: 22.1725\n",
      "Epoch 493/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.1432 - val_loss: 23.6359\n",
      "Epoch 494/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.6198 - val_loss: 24.5102\n",
      "Epoch 495/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.9314 - val_loss: 22.8963\n",
      "Epoch 496/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.2239 - val_loss: 22.9738\n",
      "Epoch 497/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.5413 - val_loss: 24.6053\n",
      "Epoch 498/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.3458 - val_loss: 22.8270\n",
      "Epoch 499/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.6183 - val_loss: 23.0554\n",
      "Epoch 500/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.8154 - val_loss: 22.6214\n",
      "Epoch 501/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.5060 - val_loss: 22.7985\n",
      "Epoch 502/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.5295 - val_loss: 22.0914\n",
      "Epoch 503/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.8511 - val_loss: 23.2203\n",
      "Epoch 504/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.8877 - val_loss: 24.7590\n",
      "Epoch 505/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.8245 - val_loss: 23.8352\n",
      "Epoch 506/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.4043 - val_loss: 23.1142\n",
      "Epoch 507/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.4254 - val_loss: 22.9627\n",
      "Epoch 508/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.1206 - val_loss: 24.2505\n",
      "Epoch 509/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.2596 - val_loss: 23.1645\n",
      "Epoch 510/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.2384 - val_loss: 24.6939\n",
      "Epoch 511/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.3100 - val_loss: 24.2488\n",
      "Epoch 512/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.7477 - val_loss: 23.5192\n",
      "Epoch 513/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.5356 - val_loss: 23.1414\n",
      "Epoch 514/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.0598 - val_loss: 23.3372\n",
      "Epoch 515/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.2810 - val_loss: 23.0095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 516/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.8163 - val_loss: 22.0299\n",
      "Epoch 517/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.3512 - val_loss: 26.2381\n",
      "Epoch 518/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.5132 - val_loss: 25.4815\n",
      "Epoch 519/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.4601 - val_loss: 26.3183\n",
      "Epoch 520/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.2197 - val_loss: 22.8098\n",
      "Epoch 521/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.5645 - val_loss: 25.2708\n",
      "Epoch 522/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.5040 - val_loss: 26.1485\n",
      "Epoch 523/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.1458 - val_loss: 25.0702\n",
      "Epoch 524/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.5896 - val_loss: 23.6418\n",
      "Epoch 525/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.5557 - val_loss: 22.9174\n",
      "Epoch 526/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.0012 - val_loss: 23.3102\n",
      "Epoch 527/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.6335 - val_loss: 23.7176\n",
      "Epoch 528/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.3232 - val_loss: 22.9289\n",
      "Epoch 529/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.0098 - val_loss: 23.2421\n",
      "Epoch 530/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.4076 - val_loss: 21.8812\n",
      "Epoch 531/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.9201 - val_loss: 25.3997\n",
      "Epoch 532/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.4784 - val_loss: 23.4120\n",
      "Epoch 533/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 16.9641 - val_loss: 23.1802\n",
      "Epoch 534/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.4249 - val_loss: 22.9043\n",
      "Epoch 535/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.1106 - val_loss: 24.3079\n",
      "Epoch 536/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.5235 - val_loss: 22.6789\n",
      "Epoch 537/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.0072 - val_loss: 23.9915\n",
      "Epoch 538/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.5378 - val_loss: 24.0276\n",
      "Epoch 539/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.9178 - val_loss: 25.4436\n",
      "Epoch 540/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.8342 - val_loss: 23.5780\n",
      "Epoch 541/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.0750 - val_loss: 23.2014\n",
      "Epoch 542/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.5131 - val_loss: 24.2228\n",
      "Epoch 543/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.5992 - val_loss: 23.6237\n",
      "Epoch 544/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.2052 - val_loss: 26.4471\n",
      "Epoch 545/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.0299 - val_loss: 23.3375\n",
      "Epoch 546/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.2759 - val_loss: 22.3595\n",
      "Epoch 547/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.9428 - val_loss: 23.1869\n",
      "Epoch 548/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.0434 - val_loss: 22.9750\n",
      "Epoch 549/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.3099 - val_loss: 21.9512\n",
      "Epoch 550/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.0308 - val_loss: 23.0440\n",
      "Epoch 551/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.9678 - val_loss: 22.8551\n",
      "Epoch 552/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.7932 - val_loss: 24.7304\n",
      "Epoch 553/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.5124 - val_loss: 22.2377\n",
      "Epoch 554/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.6864 - val_loss: 22.2352\n",
      "Epoch 555/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.1922 - val_loss: 24.1674\n",
      "Epoch 556/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.1686 - val_loss: 22.2395\n",
      "Epoch 557/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.4278 - val_loss: 22.4099\n",
      "Epoch 558/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.1054 - val_loss: 22.4441\n",
      "Epoch 559/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.8446 - val_loss: 23.4903\n",
      "Epoch 560/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.3977 - val_loss: 22.7174\n",
      "Epoch 561/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.3354 - val_loss: 22.7076\n",
      "Epoch 562/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.1949 - val_loss: 23.0287\n",
      "Epoch 563/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.4399 - val_loss: 22.7008\n",
      "Epoch 564/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.6199 - val_loss: 22.1618\n",
      "Epoch 565/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.0030 - val_loss: 23.9985\n",
      "Epoch 566/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 16.9174 - val_loss: 24.4992\n",
      "Epoch 567/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.3283 - val_loss: 22.0365\n",
      "Epoch 568/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.2387 - val_loss: 22.6648\n",
      "Epoch 569/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.0055 - val_loss: 22.5406\n",
      "Epoch 570/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.9667 - val_loss: 24.7895\n",
      "Epoch 571/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.9585 - val_loss: 25.7455\n",
      "Epoch 572/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.7837 - val_loss: 23.5138\n",
      "Epoch 573/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.9995 - val_loss: 23.9590\n",
      "Epoch 574/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.2111 - val_loss: 22.4137\n",
      "Epoch 575/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.0906 - val_loss: 24.0507\n",
      "Epoch 576/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.8645 - val_loss: 26.1624\n",
      "Epoch 577/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.7753 - val_loss: 22.6919\n",
      "Epoch 578/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.9863 - val_loss: 25.8998\n",
      "Epoch 579/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.9688 - val_loss: 26.6235\n",
      "Epoch 580/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 22.8182 - val_loss: 26.4463\n",
      "Epoch 581/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.3097 - val_loss: 23.0037\n",
      "Epoch 582/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.7186 - val_loss: 25.0361\n",
      "Epoch 583/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.5638 - val_loss: 25.2678\n",
      "Epoch 584/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.3355 - val_loss: 23.9969\n",
      "Epoch 585/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.8874 - val_loss: 24.0695\n",
      "Epoch 586/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.8137 - val_loss: 23.4120\n",
      "Epoch 587/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.2317 - val_loss: 22.5835\n",
      "Epoch 588/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.5765 - val_loss: 22.0641\n",
      "Epoch 589/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.2589 - val_loss: 22.0819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 590/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.1514 - val_loss: 23.1564\n",
      "Epoch 591/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.9083 - val_loss: 24.3983\n",
      "Epoch 592/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.6547 - val_loss: 22.8671\n",
      "Epoch 593/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.0706 - val_loss: 22.2730\n",
      "Epoch 594/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.6938 - val_loss: 22.7124\n",
      "Epoch 595/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.8773 - val_loss: 22.2852\n",
      "Epoch 596/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.4818 - val_loss: 22.7740\n",
      "Epoch 597/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 16.8861 - val_loss: 22.5313\n",
      "Epoch 598/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 16.9245 - val_loss: 22.7402\n",
      "Epoch 599/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.0934 - val_loss: 26.5491\n",
      "Epoch 600/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.2127 - val_loss: 25.1430\n",
      "Epoch 601/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.0177 - val_loss: 21.9232\n",
      "Epoch 602/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.2385 - val_loss: 23.5841\n",
      "Epoch 603/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.2588 - val_loss: 23.5999\n",
      "Epoch 604/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.5321 - val_loss: 22.2188\n",
      "Epoch 605/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 16.8611 - val_loss: 22.1261\n",
      "Epoch 606/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.3470 - val_loss: 22.5772\n",
      "Epoch 607/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.2946 - val_loss: 24.3199\n",
      "Epoch 608/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.3599 - val_loss: 23.0710\n",
      "Epoch 609/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.5121 - val_loss: 21.5832\n",
      "Epoch 610/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.0966 - val_loss: 24.7775\n",
      "Epoch 611/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.0982 - val_loss: 24.0608\n",
      "Epoch 612/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.7262 - val_loss: 22.6175\n",
      "Epoch 613/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.4033 - val_loss: 22.4223\n",
      "Epoch 614/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.5662 - val_loss: 22.8488\n",
      "Epoch 615/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 17.2802 - val_loss: 22.6912\n",
      "Epoch 616/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.6231 - val_loss: 24.6192\n",
      "Epoch 617/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.2084 - val_loss: 22.9308\n",
      "Epoch 618/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.1390 - val_loss: 24.1368\n",
      "Epoch 619/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.3890 - val_loss: 23.7543\n",
      "Epoch 620/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 16.9047 - val_loss: 22.9783\n",
      "Epoch 621/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 16.8900 - val_loss: 24.2913\n",
      "Epoch 622/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.4189 - val_loss: 23.5023\n",
      "Epoch 623/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.5788 - val_loss: 23.1683\n",
      "Epoch 624/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.6694 - val_loss: 24.0121\n",
      "Epoch 625/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 16.7479 - val_loss: 23.6376\n",
      "Epoch 626/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.8086 - val_loss: 23.2800\n",
      "Epoch 627/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.4663 - val_loss: 22.7801\n",
      "Epoch 628/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.9397 - val_loss: 23.3541\n",
      "Epoch 629/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 16.8226 - val_loss: 23.7441\n",
      "Epoch 630/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.4877 - val_loss: 22.5476\n",
      "Epoch 631/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.7566 - val_loss: 23.6137\n",
      "Epoch 632/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.9907 - val_loss: 22.3678\n",
      "Epoch 633/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.3993 - val_loss: 23.5560\n",
      "Epoch 634/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.0644 - val_loss: 22.2130\n",
      "Epoch 635/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 16.8969 - val_loss: 22.2559\n",
      "Epoch 636/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.5941 - val_loss: 22.8077\n",
      "Epoch 637/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.3148 - val_loss: 22.2809\n",
      "Epoch 638/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.1030 - val_loss: 22.3735\n",
      "Epoch 639/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.0560 - val_loss: 21.9639\n",
      "Epoch 640/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.1537 - val_loss: 24.2101\n",
      "Epoch 641/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.2775 - val_loss: 23.3614\n",
      "Epoch 642/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.9832 - val_loss: 25.3290\n",
      "Epoch 643/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.0132 - val_loss: 22.7166\n",
      "Epoch 644/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.2291 - val_loss: 23.2574\n",
      "Epoch 645/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.0869 - val_loss: 22.8909\n",
      "Epoch 646/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.3129 - val_loss: 25.6619\n",
      "Epoch 647/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.9158 - val_loss: 22.9531\n",
      "Epoch 648/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 16.8106 - val_loss: 22.2830\n",
      "Epoch 649/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.1826 - val_loss: 23.5667\n",
      "Epoch 650/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.9889 - val_loss: 21.7832\n",
      "Epoch 651/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 16.5420 - val_loss: 23.2586\n",
      "Epoch 652/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 16.8339 - val_loss: 23.1081\n",
      "Epoch 653/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 16.1135 - val_loss: 22.7293\n",
      "Epoch 654/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.4057 - val_loss: 22.7533\n",
      "Epoch 655/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.0542 - val_loss: 23.6240\n",
      "Epoch 656/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.3454 - val_loss: 23.8986\n",
      "Epoch 657/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.5806 - val_loss: 23.3531\n",
      "Epoch 658/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.4541 - val_loss: 22.2433\n",
      "Epoch 659/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.1370 - val_loss: 21.8617\n",
      "Epoch 660/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.7209 - val_loss: 23.4903\n",
      "Epoch 661/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.3570 - val_loss: 23.2818\n",
      "Epoch 662/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.1193 - val_loss: 26.0798\n",
      "Epoch 663/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.0222 - val_loss: 24.4505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 664/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 16.6913 - val_loss: 23.1760\n",
      "Epoch 665/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.5353 - val_loss: 23.4684\n",
      "Epoch 666/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.5611 - val_loss: 25.7487\n",
      "Epoch 667/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.8208 - val_loss: 22.3568\n",
      "Epoch 668/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.2950 - val_loss: 24.1443\n",
      "Epoch 669/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.1146 - val_loss: 23.5762\n",
      "Epoch 670/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.8091 - val_loss: 23.4877\n",
      "Epoch 671/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 16.8055 - val_loss: 22.8872\n",
      "Epoch 672/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.9887 - val_loss: 23.6849\n",
      "Epoch 673/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.3116 - val_loss: 22.5428\n",
      "Epoch 674/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.3096 - val_loss: 23.2837\n",
      "Epoch 675/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 16.7766 - val_loss: 23.0023\n",
      "Epoch 676/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 16.9149 - val_loss: 22.7680\n",
      "Epoch 677/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.5556 - val_loss: 22.8577\n",
      "Epoch 678/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.4529 - val_loss: 22.7988\n",
      "Epoch 679/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.0826 - val_loss: 22.3934\n",
      "Epoch 680/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.3878 - val_loss: 23.6077\n",
      "Epoch 681/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.2698 - val_loss: 22.9836\n",
      "Epoch 682/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.1221 - val_loss: 22.9521\n",
      "Epoch 683/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.9586 - val_loss: 22.6534\n",
      "Epoch 684/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.6210 - val_loss: 23.3974\n",
      "Epoch 685/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.1855 - val_loss: 21.8530\n",
      "Epoch 686/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.2991 - val_loss: 21.8341\n",
      "Epoch 687/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.0379 - val_loss: 22.8269\n",
      "Epoch 688/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 16.8437 - val_loss: 21.9825\n",
      "Epoch 689/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 16.7468 - val_loss: 22.6059\n",
      "Epoch 690/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 16.8723 - val_loss: 23.1635\n",
      "Epoch 691/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.1031 - val_loss: 22.3373\n",
      "Epoch 692/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 16.7229 - val_loss: 21.7663\n",
      "Epoch 693/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.0126 - val_loss: 23.4369\n",
      "Epoch 694/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.0309 - val_loss: 22.7828\n",
      "Epoch 695/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 16.9965 - val_loss: 23.8405\n",
      "Epoch 696/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.5861 - val_loss: 24.4657\n",
      "Epoch 697/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.3997 - val_loss: 24.5637\n",
      "Epoch 698/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.9457 - val_loss: 24.6283\n",
      "Epoch 699/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.7371 - val_loss: 21.8161\n",
      "Epoch 700/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 16.3203 - val_loss: 22.8803\n",
      "Epoch 701/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.3295 - val_loss: 25.3734\n",
      "Epoch 702/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.7967 - val_loss: 22.5722\n",
      "Epoch 703/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 16.4019 - val_loss: 24.2203\n",
      "Epoch 704/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.2406 - val_loss: 27.7015\n",
      "Epoch 705/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.7697 - val_loss: 23.2402\n",
      "Epoch 706/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.4887 - val_loss: 22.4356\n",
      "Epoch 707/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 16.7276 - val_loss: 23.5692\n",
      "Epoch 708/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.4581 - val_loss: 24.6140\n",
      "Epoch 709/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 16.8399 - val_loss: 22.0382\n",
      "Epoch 710/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 16.4452 - val_loss: 25.0156\n",
      "Epoch 711/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.4697 - val_loss: 22.8225\n",
      "Epoch 712/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.1006 - val_loss: 24.0022\n",
      "Epoch 713/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 16.4443 - val_loss: 22.5094\n",
      "Epoch 714/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.1702 - val_loss: 24.5195\n",
      "Epoch 715/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.4766 - val_loss: 24.3874\n",
      "Epoch 716/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.9428 - val_loss: 25.3420\n",
      "Epoch 717/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.3739 - val_loss: 22.6876\n",
      "Epoch 718/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 16.7992 - val_loss: 24.5515\n",
      "Epoch 719/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 18.0871 - val_loss: 23.5270\n",
      "Epoch 720/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.4980 - val_loss: 24.3323\n",
      "Epoch 721/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.8298 - val_loss: 25.3884\n",
      "Epoch 722/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.5661 - val_loss: 24.3316\n",
      "Epoch 723/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.4032 - val_loss: 24.8655\n",
      "Epoch 724/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 16.9639 - val_loss: 22.3645\n",
      "Epoch 725/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.2709 - val_loss: 23.1421\n",
      "Epoch 726/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.1743 - val_loss: 22.9685\n",
      "Epoch 727/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.7460 - val_loss: 23.1194\n",
      "Epoch 728/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.9854 - val_loss: 23.0487\n",
      "Epoch 729/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 16.8970 - val_loss: 22.4897\n",
      "Epoch 730/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 16.6240 - val_loss: 22.5708\n",
      "Epoch 731/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 16.9135 - val_loss: 24.1874\n",
      "Epoch 732/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.0087 - val_loss: 23.1700\n",
      "Epoch 733/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 16.4061 - val_loss: 23.1116\n",
      "Epoch 734/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 16.7888 - val_loss: 22.7349\n",
      "Epoch 735/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.3844 - val_loss: 23.3097\n",
      "Epoch 736/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.6895 - val_loss: 24.0264\n",
      "Epoch 737/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 16.9085 - val_loss: 23.3416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 738/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 16.4326 - val_loss: 23.5873\n",
      "Epoch 739/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 16.8150 - val_loss: 22.6576\n",
      "Epoch 740/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.0907 - val_loss: 22.2042\n",
      "Epoch 741/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 16.9971 - val_loss: 22.2895\n",
      "Epoch 742/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 15.9957 - val_loss: 22.8723\n",
      "Epoch 743/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.7362 - val_loss: 22.6015\n",
      "Epoch 744/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 16.3957 - val_loss: 22.4998\n",
      "Epoch 745/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.0386 - val_loss: 22.2604\n",
      "Epoch 746/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.2261 - val_loss: 22.3345\n",
      "Epoch 747/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 16.7254 - val_loss: 24.5431\n",
      "Epoch 748/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.5830 - val_loss: 25.2382\n",
      "Epoch 749/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.2381 - val_loss: 22.3698\n",
      "Epoch 750/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.8752 - val_loss: 22.5609\n",
      "Epoch 751/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.8060 - val_loss: 27.4313\n",
      "Epoch 752/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.5096 - val_loss: 23.5354\n",
      "Epoch 753/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 16.4244 - val_loss: 23.7045\n",
      "Epoch 754/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.8392 - val_loss: 23.0132\n",
      "Epoch 755/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.2524 - val_loss: 23.6539\n",
      "Epoch 756/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.3452 - val_loss: 22.6059\n",
      "Epoch 757/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.3728 - val_loss: 22.0917\n",
      "Epoch 758/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.1794 - val_loss: 22.5214\n",
      "Epoch 759/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.0518 - val_loss: 22.9141\n",
      "Epoch 760/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.5649 - val_loss: 22.8827\n",
      "Epoch 761/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 16.0640 - val_loss: 23.0570\n",
      "Epoch 762/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 16.7400 - val_loss: 21.9267\n",
      "Epoch 763/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 16.3310 - val_loss: 23.5915\n",
      "Epoch 764/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 16.0198 - val_loss: 23.1337\n",
      "Epoch 765/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.4115 - val_loss: 23.1018\n",
      "Epoch 766/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 16.4279 - val_loss: 22.7578\n",
      "Epoch 767/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 15.9384 - val_loss: 23.6047\n",
      "Epoch 768/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 16.5067 - val_loss: 23.3493\n",
      "Epoch 769/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 16.4141 - val_loss: 24.5730\n",
      "Epoch 770/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.5291 - val_loss: 24.8865\n",
      "Epoch 771/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 16.8812 - val_loss: 25.7933\n",
      "Epoch 772/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.5864 - val_loss: 24.4357\n",
      "Epoch 773/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.0790 - val_loss: 24.7284\n",
      "Epoch 774/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.3534 - val_loss: 22.1396\n",
      "Epoch 775/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.0635 - val_loss: 22.5214\n",
      "Epoch 776/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.1049 - val_loss: 22.0322\n",
      "Epoch 777/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.0887 - val_loss: 22.5253\n",
      "Epoch 778/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 16.4214 - val_loss: 22.9763\n",
      "Epoch 779/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 16.1263 - val_loss: 22.6935\n",
      "Epoch 780/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 16.0373 - val_loss: 22.9940\n",
      "Epoch 781/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 16.4944 - val_loss: 23.7273\n",
      "Epoch 782/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 16.7320 - val_loss: 25.0361\n",
      "Epoch 783/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.3058 - val_loss: 23.5350\n",
      "Epoch 784/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.7682 - val_loss: 24.0737\n",
      "Epoch 785/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 16.9828 - val_loss: 24.1612\n",
      "Epoch 786/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.0429 - val_loss: 24.6892\n",
      "Epoch 787/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 16.4436 - val_loss: 22.9992\n",
      "Epoch 788/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.3791 - val_loss: 25.9068\n",
      "Epoch 789/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.0995 - val_loss: 23.4641\n",
      "Epoch 790/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.4491 - val_loss: 24.3474\n",
      "Epoch 791/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.2128 - val_loss: 24.0901\n",
      "Epoch 792/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 16.5192 - val_loss: 22.1930\n",
      "Epoch 793/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 15.7692 - val_loss: 24.7895\n",
      "Epoch 794/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 15.9011 - val_loss: 22.2424\n",
      "Epoch 795/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 16.4978 - val_loss: 22.4538\n",
      "Epoch 796/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 16.2095 - val_loss: 23.4374\n",
      "Epoch 797/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 16.3769 - val_loss: 22.7545\n",
      "Epoch 798/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 16.1861 - val_loss: 25.4577\n",
      "Epoch 799/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 17.0203 - val_loss: 22.7718\n",
      "Epoch 800/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 16.0460 - val_loss: 23.2495\n",
      "Epoch 801/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.4115 - val_loss: 24.1243\n",
      "Epoch 802/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.0846 - val_loss: 24.8810\n",
      "Epoch 803/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 16.5738 - val_loss: 22.4707\n",
      "Epoch 804/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 17.3625 - val_loss: 25.0467\n",
      "Epoch 805/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.7401 - val_loss: 22.2292\n",
      "Epoch 806/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 16.6605 - val_loss: 23.1771\n",
      "Epoch 807/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 16.1702 - val_loss: 22.1950\n",
      "Epoch 808/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 15.9762 - val_loss: 22.6145\n",
      "Epoch 809/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 16.9019 - val_loss: 22.1359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|████████████████████████████████████████████████████████▍                         | 11/16 [06:09<02:39, 31.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'batc_normalization': False, 'batch_size': 64, 'dropout': 0, 'epochs': 2000, 'first_neuron': 160, 'hidden_layers': 2, 'hidden_neuron': 100, 'kernel_initializer': 'normal', 'last_activation': 'linear', 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "Train on 1500 samples, validate on 375 samples\n",
      "Epoch 1/2000\n",
      "1500/1500 [==============================] - 0s 247us/sample - loss: 2185.2590 - val_loss: 2182.7577\n",
      "Epoch 2/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 2162.8957 - val_loss: 2117.2189\n",
      "Epoch 3/2000\n",
      "1500/1500 [==============================] - 0s 30us/sample - loss: 1973.8669 - val_loss: 1697.2586\n",
      "Epoch 4/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 1160.2912 - val_loss: 609.6920\n",
      "Epoch 5/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 613.7358 - val_loss: 506.0650\n",
      "Epoch 6/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 471.6981 - val_loss: 391.1927\n",
      "Epoch 7/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 362.5003 - val_loss: 300.8368\n",
      "Epoch 8/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 268.9922 - val_loss: 220.7971\n",
      "Epoch 9/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 195.0443 - val_loss: 157.7394\n",
      "Epoch 10/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 142.4508 - val_loss: 118.0531\n",
      "Epoch 11/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 107.1863 - val_loss: 90.5625\n",
      "Epoch 12/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 81.9402 - val_loss: 71.3633\n",
      "Epoch 13/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 64.5173 - val_loss: 60.8865\n",
      "Epoch 14/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 53.7213 - val_loss: 49.8112\n",
      "Epoch 15/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 46.4816 - val_loss: 43.1617\n",
      "Epoch 16/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 40.8220 - val_loss: 37.8741\n",
      "Epoch 17/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 35.9555 - val_loss: 36.1663\n",
      "Epoch 18/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 33.8361 - val_loss: 32.5927\n",
      "Epoch 19/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 31.1157 - val_loss: 34.9318\n",
      "Epoch 20/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 33.0376 - val_loss: 30.9823\n",
      "Epoch 21/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 26.8027 - val_loss: 26.6928\n",
      "Epoch 22/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 25.9678 - val_loss: 26.5914\n",
      "Epoch 23/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 25.7606 - val_loss: 31.7980\n",
      "Epoch 24/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 26.4408 - val_loss: 27.7696\n",
      "Epoch 25/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 26.1047 - val_loss: 26.0745\n",
      "Epoch 26/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 24.2187 - val_loss: 29.9647\n",
      "Epoch 27/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 25.0676 - val_loss: 24.7928\n",
      "Epoch 28/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 23.9020 - val_loss: 25.2944\n",
      "Epoch 29/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 24.3352 - val_loss: 25.4400\n",
      "Epoch 30/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 24.1598 - val_loss: 28.8834\n",
      "Epoch 31/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 24.0996 - val_loss: 25.0002\n",
      "Epoch 32/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 23.6221 - val_loss: 29.6276\n",
      "Epoch 33/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 25.2103 - val_loss: 25.0673\n",
      "Epoch 34/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 23.1435 - val_loss: 25.3343\n",
      "Epoch 35/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 23.1741 - val_loss: 27.4418\n",
      "Epoch 36/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 23.6921 - val_loss: 26.1012\n",
      "Epoch 37/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 24.5498 - val_loss: 28.3679\n",
      "Epoch 38/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 24.6668 - val_loss: 27.9641\n",
      "Epoch 39/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 23.6978 - val_loss: 25.3315\n",
      "Epoch 40/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 24.5300 - val_loss: 34.6928\n",
      "Epoch 41/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 24.7837 - val_loss: 27.3134\n",
      "Epoch 42/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 23.4085 - val_loss: 28.8103\n",
      "Epoch 43/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 24.8245 - val_loss: 25.3808\n",
      "Epoch 44/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 22.3536 - val_loss: 24.3157\n",
      "Epoch 45/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 22.2903 - val_loss: 24.3541\n",
      "Epoch 46/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 21.7219 - val_loss: 23.5897\n",
      "Epoch 47/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 22.7595 - val_loss: 26.6202\n",
      "Epoch 48/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 22.1479 - val_loss: 25.3492\n",
      "Epoch 49/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.9125 - val_loss: 24.7673\n",
      "Epoch 50/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.7739 - val_loss: 25.7694\n",
      "Epoch 51/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.7220 - val_loss: 28.0821\n",
      "Epoch 52/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 25.9249 - val_loss: 35.8616\n",
      "Epoch 53/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 26.7780 - val_loss: 24.2970\n",
      "Epoch 54/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 21.7614 - val_loss: 26.8783\n",
      "Epoch 55/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 22.5613 - val_loss: 23.3056\n",
      "Epoch 56/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 21.3845 - val_loss: 25.1177\n",
      "Epoch 57/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.8773 - val_loss: 26.2434\n",
      "Epoch 58/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 22.2350 - val_loss: 24.0721\n",
      "Epoch 59/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 24.2188 - val_loss: 24.9056\n",
      "Epoch 60/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 23.4114 - val_loss: 24.3882\n",
      "Epoch 61/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 23.1067 - val_loss: 23.6754\n",
      "Epoch 62/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 22.0364 - val_loss: 26.3786\n",
      "Epoch 63/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 22.1141 - val_loss: 23.6315\n",
      "Epoch 64/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 22.4104 - val_loss: 25.8179\n",
      "Epoch 65/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 21.5304 - val_loss: 25.2687\n",
      "Epoch 66/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 22.0952 - val_loss: 25.6799\n",
      "Epoch 67/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 22.3836 - val_loss: 24.0520\n",
      "Epoch 68/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.2103 - val_loss: 24.2590\n",
      "Epoch 69/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.4511 - val_loss: 30.9334\n",
      "Epoch 70/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 23.0683 - val_loss: 24.0032\n",
      "Epoch 71/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.6519 - val_loss: 24.5786\n",
      "Epoch 72/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 22.4669 - val_loss: 26.8172\n",
      "Epoch 73/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.3156 - val_loss: 24.4271\n",
      "Epoch 74/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.3971 - val_loss: 25.0858\n",
      "Epoch 75/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 23.3090 - val_loss: 24.3529\n",
      "Epoch 76/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.4791 - val_loss: 22.8126\n",
      "Epoch 77/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.0156 - val_loss: 27.4697\n",
      "Epoch 78/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 21.7032 - val_loss: 23.5267\n",
      "Epoch 79/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 21.8340 - val_loss: 25.3989\n",
      "Epoch 80/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 21.8620 - val_loss: 30.7032\n",
      "Epoch 81/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 25.8382 - val_loss: 26.8948\n",
      "Epoch 82/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 23.2776 - val_loss: 26.2789\n",
      "Epoch 83/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 21.7359 - val_loss: 25.2591\n",
      "Epoch 84/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.6755 - val_loss: 23.8489\n",
      "Epoch 85/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.7166 - val_loss: 25.5171\n",
      "Epoch 86/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.5822 - val_loss: 32.6610\n",
      "Epoch 87/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 23.9664 - val_loss: 31.4248\n",
      "Epoch 88/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 22.4283 - val_loss: 25.9553\n",
      "Epoch 89/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.1503 - val_loss: 27.4997\n",
      "Epoch 90/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 22.1044 - val_loss: 24.9217\n",
      "Epoch 91/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 22.5095 - val_loss: 26.9878\n",
      "Epoch 92/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.6853 - val_loss: 23.1907\n",
      "Epoch 93/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.5721 - val_loss: 26.4624\n",
      "Epoch 94/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.8814 - val_loss: 26.4295\n",
      "Epoch 95/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.7798 - val_loss: 23.6152\n",
      "Epoch 96/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 22.3701 - val_loss: 29.7541\n",
      "Epoch 97/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.8627 - val_loss: 27.5238\n",
      "Epoch 98/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 21.6115 - val_loss: 23.5943\n",
      "Epoch 99/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.9751 - val_loss: 23.8411\n",
      "Epoch 100/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.7857 - val_loss: 24.0181\n",
      "Epoch 101/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.1408 - val_loss: 24.7280\n",
      "Epoch 102/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 21.6285 - val_loss: 24.3441\n",
      "Epoch 103/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 22.1853 - val_loss: 24.1862\n",
      "Epoch 104/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 22.2397 - val_loss: 25.3442\n",
      "Epoch 105/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 22.7238 - val_loss: 23.0723\n",
      "Epoch 106/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 21.4267 - val_loss: 26.3180\n",
      "Epoch 107/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.2809 - val_loss: 24.9187\n",
      "Epoch 108/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.2650 - val_loss: 24.1830\n",
      "Epoch 109/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.4492 - val_loss: 25.3640\n",
      "Epoch 110/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 22.1325 - val_loss: 25.5130\n",
      "Epoch 111/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.5118 - val_loss: 25.8275\n",
      "Epoch 112/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.8448 - val_loss: 24.7821\n",
      "Epoch 113/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.5554 - val_loss: 23.7863\n",
      "Epoch 114/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.2873 - val_loss: 23.9954\n",
      "Epoch 115/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 22.6206 - val_loss: 27.4828\n",
      "Epoch 116/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 20.6280 - val_loss: 24.3651\n",
      "Epoch 117/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 20.5507 - val_loss: 23.5803\n",
      "Epoch 118/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 20.5771 - val_loss: 24.6733\n",
      "Epoch 119/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 20.2406 - val_loss: 24.7794\n",
      "Epoch 120/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.9660 - val_loss: 24.2210\n",
      "Epoch 121/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 20.4935 - val_loss: 24.3563\n",
      "Epoch 122/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 21.0433 - val_loss: 26.5775\n",
      "Epoch 123/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 22.0065 - val_loss: 24.5769\n",
      "Epoch 124/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.7432 - val_loss: 22.8550\n",
      "Epoch 125/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.6495 - val_loss: 24.9066\n",
      "Epoch 126/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 22.2632 - val_loss: 29.4829\n",
      "Epoch 127/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 21.8541 - val_loss: 23.7331\n",
      "Epoch 128/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.5234 - val_loss: 24.4996\n",
      "Epoch 129/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.0327 - val_loss: 24.0536\n",
      "Epoch 130/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 20.3255 - val_loss: 25.1922\n",
      "Epoch 131/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.3089 - val_loss: 25.9579\n",
      "Epoch 132/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 22.0002 - val_loss: 24.3397\n",
      "Epoch 133/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 22.3264 - val_loss: 23.0385\n",
      "Epoch 134/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.8875 - val_loss: 23.8938\n",
      "Epoch 135/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.0539 - val_loss: 24.2317\n",
      "Epoch 136/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.3510 - val_loss: 23.2164\n",
      "Epoch 137/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 21.2380 - val_loss: 26.0025\n",
      "Epoch 138/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.0766 - val_loss: 22.9703\n",
      "Epoch 139/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.7048 - val_loss: 23.7387\n",
      "Epoch 140/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.0494 - val_loss: 25.3094\n",
      "Epoch 141/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.5362 - val_loss: 25.4024\n",
      "Epoch 142/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.7652 - val_loss: 24.2262\n",
      "Epoch 143/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.2290 - val_loss: 24.1706\n",
      "Epoch 144/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.5542 - val_loss: 25.2115\n",
      "Epoch 145/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.2188 - val_loss: 24.6600\n",
      "Epoch 146/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.0239 - val_loss: 23.7203\n",
      "Epoch 147/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.7163 - val_loss: 24.0843\n",
      "Epoch 148/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.2549 - val_loss: 24.7933\n",
      "Epoch 149/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.3750 - val_loss: 24.1805\n",
      "Epoch 150/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 21.1010 - val_loss: 25.8756\n",
      "Epoch 151/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.5247 - val_loss: 25.5881\n",
      "Epoch 152/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.9669 - val_loss: 22.7076\n",
      "Epoch 153/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.4315 - val_loss: 23.2213\n",
      "Epoch 154/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 20.8141 - val_loss: 24.0103\n",
      "Epoch 155/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 20.9809 - val_loss: 23.9550\n",
      "Epoch 156/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.2266 - val_loss: 26.9761\n",
      "Epoch 157/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.4142 - val_loss: 24.5827\n",
      "Epoch 158/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.1830 - val_loss: 25.0655\n",
      "Epoch 159/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.8745 - val_loss: 24.1821\n",
      "Epoch 160/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.9512 - val_loss: 23.6487\n",
      "Epoch 161/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.2999 - val_loss: 26.3005\n",
      "Epoch 162/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.9717 - val_loss: 24.8223\n",
      "Epoch 163/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 22.9016 - val_loss: 27.3463\n",
      "Epoch 164/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.3381 - val_loss: 24.5925\n",
      "Epoch 165/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.8637 - val_loss: 24.7086\n",
      "Epoch 166/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.8152 - val_loss: 25.0246\n",
      "Epoch 167/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.2608 - val_loss: 25.1815\n",
      "Epoch 168/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.6253 - val_loss: 24.1932\n",
      "Epoch 169/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.7687 - val_loss: 24.7692\n",
      "Epoch 170/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.6307 - val_loss: 24.0508\n",
      "Epoch 171/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.3358 - val_loss: 24.9030\n",
      "Epoch 172/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.3015 - val_loss: 23.2737\n",
      "Epoch 173/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.6451 - val_loss: 24.9428\n",
      "Epoch 174/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.7377 - val_loss: 26.4517\n",
      "Epoch 175/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.0097 - val_loss: 24.4087\n",
      "Epoch 176/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.1070 - val_loss: 26.2100\n",
      "Epoch 177/2000\n",
      "1500/1500 [==============================] - 0s 31us/sample - loss: 22.2554 - val_loss: 23.4980\n",
      "Epoch 178/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 20.0096 - val_loss: 25.1684\n",
      "Epoch 179/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.2318 - val_loss: 23.5349\n",
      "Epoch 180/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 19.6526 - val_loss: 24.0360\n",
      "Epoch 181/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.0753 - val_loss: 24.8692\n",
      "Epoch 182/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.5025 - val_loss: 23.2954\n",
      "Epoch 183/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.1735 - val_loss: 24.7253\n",
      "Epoch 184/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.4807 - val_loss: 24.6294\n",
      "Epoch 185/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 20.8408 - val_loss: 26.0145\n",
      "Epoch 186/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 22.6930 - val_loss: 24.3744\n",
      "Epoch 187/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 20.2706 - val_loss: 25.2001\n",
      "Epoch 188/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.9294 - val_loss: 24.5734\n",
      "Epoch 189/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.5286 - val_loss: 24.6700\n",
      "Epoch 190/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.7823 - val_loss: 24.6502\n",
      "Epoch 191/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 19.5394 - val_loss: 25.6380\n",
      "Epoch 192/2000\n",
      "1500/1500 [==============================] - 0s 40us/sample - loss: 19.0994 - val_loss: 22.5495\n",
      "Epoch 193/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 19.4194 - val_loss: 23.9677\n",
      "Epoch 194/2000\n",
      "1500/1500 [==============================] - 0s 31us/sample - loss: 20.5226 - val_loss: 28.4270\n",
      "Epoch 195/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 22.0106 - val_loss: 23.3176\n",
      "Epoch 196/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.4560 - val_loss: 24.8879\n",
      "Epoch 197/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.9854 - val_loss: 24.4135\n",
      "Epoch 198/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 20.3658 - val_loss: 25.7621\n",
      "Epoch 199/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 21.2659 - val_loss: 23.5721\n",
      "Epoch 200/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.3363 - val_loss: 24.4991\n",
      "Epoch 201/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.0114 - val_loss: 23.5445\n",
      "Epoch 202/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.7963 - val_loss: 24.0375\n",
      "Epoch 203/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 20.1079 - val_loss: 24.5873\n",
      "Epoch 204/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 19.3607 - val_loss: 23.4233\n",
      "Epoch 205/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 20.0551 - val_loss: 22.7555\n",
      "Epoch 206/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.2656 - val_loss: 23.8017\n",
      "Epoch 207/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.9005 - val_loss: 25.6415\n",
      "Epoch 208/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.7376 - val_loss: 23.4784\n",
      "Epoch 209/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.6049 - val_loss: 24.1408\n",
      "Epoch 210/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.1630 - val_loss: 24.7298\n",
      "Epoch 211/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 21.8179 - val_loss: 26.0310\n",
      "Epoch 212/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 20.5545 - val_loss: 26.3570\n",
      "Epoch 213/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.0386 - val_loss: 23.3990\n",
      "Epoch 214/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.1990 - val_loss: 25.4501\n",
      "Epoch 215/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.3354 - val_loss: 24.0333\n",
      "Epoch 216/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.8082 - val_loss: 25.0690\n",
      "Epoch 217/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.4983 - val_loss: 24.5815\n",
      "Epoch 218/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.0117 - val_loss: 28.2014\n",
      "Epoch 219/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.1875 - val_loss: 24.7670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 220/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.7468 - val_loss: 25.6653\n",
      "Epoch 221/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.7044 - val_loss: 22.6694\n",
      "Epoch 222/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.6082 - val_loss: 23.0753\n",
      "Epoch 223/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.6860 - val_loss: 26.2776\n",
      "Epoch 224/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.6992 - val_loss: 25.6203\n",
      "Epoch 225/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.5310 - val_loss: 26.8754\n",
      "Epoch 226/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.2815 - val_loss: 27.1131\n",
      "Epoch 227/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.1409 - val_loss: 25.3791\n",
      "Epoch 228/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.9325 - val_loss: 25.8163\n",
      "Epoch 229/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 20.9621 - val_loss: 23.3856\n",
      "Epoch 230/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.3546 - val_loss: 26.5678\n",
      "Epoch 231/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.4785 - val_loss: 22.9218\n",
      "Epoch 232/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 20.4115 - val_loss: 23.7667\n",
      "Epoch 233/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.0566 - val_loss: 22.6592\n",
      "Epoch 234/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.1860 - val_loss: 26.2227\n",
      "Epoch 235/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.4677 - val_loss: 26.6399\n",
      "Epoch 236/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.1230 - val_loss: 25.2820\n",
      "Epoch 237/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.9102 - val_loss: 23.3040\n",
      "Epoch 238/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.9627 - val_loss: 24.6986\n",
      "Epoch 239/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.0873 - val_loss: 23.1306\n",
      "Epoch 240/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 20.6268 - val_loss: 23.6225\n",
      "Epoch 241/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.0173 - val_loss: 24.9075\n",
      "Epoch 242/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.0765 - val_loss: 28.6699\n",
      "Epoch 243/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.5803 - val_loss: 26.0314\n",
      "Epoch 244/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.0198 - val_loss: 24.0438\n",
      "Epoch 245/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.1860 - val_loss: 25.9309\n",
      "Epoch 246/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.7424 - val_loss: 25.4691\n",
      "Epoch 247/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.1166 - val_loss: 28.4451\n",
      "Epoch 248/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.2096 - val_loss: 24.3350\n",
      "Epoch 249/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.0062 - val_loss: 22.5411\n",
      "Epoch 250/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.1456 - val_loss: 26.6191\n",
      "Epoch 251/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.5696 - val_loss: 22.4450\n",
      "Epoch 252/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.9852 - val_loss: 25.8560\n",
      "Epoch 253/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.9979 - val_loss: 24.4663\n",
      "Epoch 254/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 19.0205 - val_loss: 24.8797\n",
      "Epoch 255/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.1322 - val_loss: 23.7278\n",
      "Epoch 256/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.3767 - val_loss: 25.1299\n",
      "Epoch 257/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.8991 - val_loss: 24.9779\n",
      "Epoch 258/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.6625 - val_loss: 24.0829\n",
      "Epoch 259/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.3402 - val_loss: 23.9827\n",
      "Epoch 260/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.5024 - val_loss: 24.4520\n",
      "Epoch 261/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.2692 - val_loss: 23.9385\n",
      "Epoch 262/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.8922 - val_loss: 26.9373\n",
      "Epoch 263/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.2226 - val_loss: 23.4491\n",
      "Epoch 264/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.4147 - val_loss: 23.3741\n",
      "Epoch 265/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.6897 - val_loss: 23.3054\n",
      "Epoch 266/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.0077 - val_loss: 25.1727\n",
      "Epoch 267/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.1319 - val_loss: 25.5167\n",
      "Epoch 268/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.4057 - val_loss: 25.7517\n",
      "Epoch 269/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.8159 - val_loss: 24.7729\n",
      "Epoch 270/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.2276 - val_loss: 27.1869\n",
      "Epoch 271/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.4120 - val_loss: 23.2321\n",
      "Epoch 272/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.4052 - val_loss: 23.6048\n",
      "Epoch 273/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.6331 - val_loss: 23.6739\n",
      "Epoch 274/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.5724 - val_loss: 23.5937\n",
      "Epoch 275/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.2906 - val_loss: 22.9540\n",
      "Epoch 276/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.7629 - val_loss: 24.5970\n",
      "Epoch 277/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 21.5537 - val_loss: 25.8317\n",
      "Epoch 278/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 21.9675 - val_loss: 25.0330\n",
      "Epoch 279/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 20.3108 - val_loss: 23.0295\n",
      "Epoch 280/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.9899 - val_loss: 24.8178\n",
      "Epoch 281/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.1973 - val_loss: 23.4460\n",
      "Epoch 282/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.6158 - val_loss: 22.9440\n",
      "Epoch 283/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.8382 - val_loss: 23.2259\n",
      "Epoch 284/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.5867 - val_loss: 23.2839\n",
      "Epoch 285/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.7490 - val_loss: 26.4267\n",
      "Epoch 286/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.6029 - val_loss: 24.2411\n",
      "Epoch 287/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.8557 - val_loss: 23.3531\n",
      "Epoch 288/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.6726 - val_loss: 24.3312\n",
      "Epoch 289/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.3623 - val_loss: 26.2285\n",
      "Epoch 290/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.6617 - val_loss: 24.5223\n",
      "Epoch 291/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.9359 - val_loss: 24.1903\n",
      "Epoch 292/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.9285 - val_loss: 23.4602\n",
      "Epoch 293/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.7349 - val_loss: 23.9143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 294/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.2330 - val_loss: 23.5847\n",
      "Epoch 295/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.7545 - val_loss: 23.1475\n",
      "Epoch 296/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.7581 - val_loss: 22.8690\n",
      "Epoch 297/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.5383 - val_loss: 23.4225\n",
      "Epoch 298/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.4185 - val_loss: 23.2505\n",
      "Epoch 299/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.6975 - val_loss: 24.7388\n",
      "Epoch 300/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.0218 - val_loss: 25.4421\n",
      "Epoch 301/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.4078 - val_loss: 24.2107\n",
      "Epoch 302/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.0881 - val_loss: 23.6842\n",
      "Epoch 303/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.0814 - val_loss: 24.9355\n",
      "Epoch 304/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.9887 - val_loss: 25.1342\n",
      "Epoch 305/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 18.0139 - val_loss: 24.5711\n",
      "Epoch 306/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.8783 - val_loss: 23.2300\n",
      "Epoch 307/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.8470 - val_loss: 23.7305\n",
      "Epoch 308/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.8597 - val_loss: 27.0812\n",
      "Epoch 309/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.3970 - val_loss: 24.4549\n",
      "Epoch 310/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.0373 - val_loss: 22.9469\n",
      "Epoch 311/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.8962 - val_loss: 24.2037\n",
      "Epoch 312/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.7525 - val_loss: 22.5101\n",
      "Epoch 313/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.4872 - val_loss: 23.6985\n",
      "Epoch 314/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.1160 - val_loss: 24.2448\n",
      "Epoch 315/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.1423 - val_loss: 23.4479\n",
      "Epoch 316/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.2643 - val_loss: 27.1711\n",
      "Epoch 317/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.8519 - val_loss: 24.2211\n",
      "Epoch 318/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.9985 - val_loss: 23.1625\n",
      "Epoch 319/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.0704 - val_loss: 24.9036\n",
      "Epoch 320/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.9305 - val_loss: 22.2505\n",
      "Epoch 321/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.8401 - val_loss: 22.5562\n",
      "Epoch 322/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.7927 - val_loss: 25.0892\n",
      "Epoch 323/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.6015 - val_loss: 26.2737\n",
      "Epoch 324/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.5378 - val_loss: 24.4747\n",
      "Epoch 325/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.3415 - val_loss: 23.4778\n",
      "Epoch 326/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.7630 - val_loss: 26.3608\n",
      "Epoch 327/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.4923 - val_loss: 22.7925\n",
      "Epoch 328/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.2499 - val_loss: 22.6739\n",
      "Epoch 329/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.0981 - val_loss: 24.3175\n",
      "Epoch 330/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.1143 - val_loss: 25.0098\n",
      "Epoch 331/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.7788 - val_loss: 26.2810\n",
      "Epoch 332/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.8076 - val_loss: 24.2789\n",
      "Epoch 333/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.2461 - val_loss: 23.3067\n",
      "Epoch 334/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.8511 - val_loss: 24.6644\n",
      "Epoch 335/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.5524 - val_loss: 23.0078\n",
      "Epoch 336/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.0031 - val_loss: 25.7383\n",
      "Epoch 337/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.1076 - val_loss: 23.9011\n",
      "Epoch 338/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.7005 - val_loss: 23.4178\n",
      "Epoch 339/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.7076 - val_loss: 25.3047\n",
      "Epoch 340/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.0761 - val_loss: 22.9047\n",
      "Epoch 341/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.5944 - val_loss: 23.5973\n",
      "Epoch 342/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.9218 - val_loss: 25.8602\n",
      "Epoch 343/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.9450 - val_loss: 23.7813\n",
      "Epoch 344/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.4635 - val_loss: 25.2946\n",
      "Epoch 345/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.9863 - val_loss: 23.3845\n",
      "Epoch 346/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.3424 - val_loss: 24.7289\n",
      "Epoch 347/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.0734 - val_loss: 24.1294\n",
      "Epoch 348/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.2425 - val_loss: 23.6078\n",
      "Epoch 349/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.7773 - val_loss: 23.2576\n",
      "Epoch 350/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.2659 - val_loss: 22.3732\n",
      "Epoch 351/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.9214 - val_loss: 22.6431\n",
      "Epoch 352/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.6074 - val_loss: 24.0372\n",
      "Epoch 353/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.3640 - val_loss: 23.5285\n",
      "Epoch 354/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.5126 - val_loss: 23.3305\n",
      "Epoch 355/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.4229 - val_loss: 23.2161\n",
      "Epoch 356/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 18.0673 - val_loss: 27.8180\n",
      "Epoch 357/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.4630 - val_loss: 22.7842\n",
      "Epoch 358/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.0733 - val_loss: 24.0481\n",
      "Epoch 359/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.0869 - val_loss: 26.0525\n",
      "Epoch 360/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.9705 - val_loss: 22.5855\n",
      "Epoch 361/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.2546 - val_loss: 23.0078\n",
      "Epoch 362/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.8112 - val_loss: 24.6758\n",
      "Epoch 363/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 21.0797 - val_loss: 26.2052\n",
      "Epoch 364/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.1470 - val_loss: 23.0758\n",
      "Epoch 365/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.9058 - val_loss: 26.9586\n",
      "Epoch 366/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.0701 - val_loss: 25.6938\n",
      "Epoch 367/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.5961 - val_loss: 24.5683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 368/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.4516 - val_loss: 23.0141\n",
      "Epoch 369/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.0763 - val_loss: 23.4866\n",
      "Epoch 370/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.4018 - val_loss: 23.2611\n",
      "Epoch 371/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 17.8176 - val_loss: 23.1519\n",
      "Epoch 372/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.4882 - val_loss: 23.7118\n",
      "Epoch 373/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 18.2488 - val_loss: 24.5880\n",
      "Epoch 374/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.0487 - val_loss: 23.3869\n",
      "Epoch 375/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.9258 - val_loss: 25.9174\n",
      "Epoch 376/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.7390 - val_loss: 24.0693\n",
      "Epoch 377/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.4882 - val_loss: 25.7972\n",
      "Epoch 378/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.8707 - val_loss: 25.8538\n",
      "Epoch 379/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.3203 - val_loss: 24.2451\n",
      "Epoch 380/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.6139 - val_loss: 27.8599\n",
      "Epoch 381/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 18.5760 - val_loss: 26.1764\n",
      "Epoch 382/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.3767 - val_loss: 23.9763\n",
      "Epoch 383/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.7779 - val_loss: 27.2849\n",
      "Epoch 384/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.8519 - val_loss: 22.8803\n",
      "Epoch 385/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.7040 - val_loss: 23.3511\n",
      "Epoch 386/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.3287 - val_loss: 24.7757\n",
      "Epoch 387/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.3926 - val_loss: 22.3655\n",
      "Epoch 388/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.3481 - val_loss: 22.3306\n",
      "Epoch 389/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.7229 - val_loss: 22.4922\n",
      "Epoch 390/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.2033 - val_loss: 23.0390\n",
      "Epoch 391/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.0127 - val_loss: 22.8409\n",
      "Epoch 392/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.0882 - val_loss: 24.7239\n",
      "Epoch 393/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.3768 - val_loss: 23.7744\n",
      "Epoch 394/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.0569 - val_loss: 24.5558\n",
      "Epoch 395/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.4182 - val_loss: 22.4143\n",
      "Epoch 396/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.8602 - val_loss: 24.4433\n",
      "Epoch 397/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.2968 - val_loss: 24.2079\n",
      "Epoch 398/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.7836 - val_loss: 25.0999\n",
      "Epoch 399/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.3494 - val_loss: 23.7334\n",
      "Epoch 400/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.9498 - val_loss: 24.0570\n",
      "Epoch 401/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.2197 - val_loss: 23.6453\n",
      "Epoch 402/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.4047 - val_loss: 26.8060\n",
      "Epoch 403/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.3836 - val_loss: 23.1080\n",
      "Epoch 404/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.0888 - val_loss: 24.0336\n",
      "Epoch 405/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.0443 - val_loss: 23.3545\n",
      "Epoch 406/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 17.5827 - val_loss: 23.2661\n",
      "Epoch 407/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.2703 - val_loss: 22.9780\n",
      "Epoch 408/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.4090 - val_loss: 25.0710\n",
      "Epoch 409/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.9288 - val_loss: 23.9204\n",
      "Epoch 410/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.4636 - val_loss: 25.7894\n",
      "Epoch 411/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.5974 - val_loss: 24.2356\n",
      "Epoch 412/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.8212 - val_loss: 23.1264\n",
      "Epoch 413/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.0938 - val_loss: 24.3846\n",
      "Epoch 414/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.6283 - val_loss: 23.8890\n",
      "Epoch 415/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.2126 - val_loss: 23.3568\n",
      "Epoch 416/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.2711 - val_loss: 23.4114\n",
      "Epoch 417/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.5488 - val_loss: 23.6175\n",
      "Epoch 418/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.4215 - val_loss: 22.8562\n",
      "Epoch 419/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.6343 - val_loss: 23.7191\n",
      "Epoch 420/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.2101 - val_loss: 27.6322\n",
      "Epoch 421/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 18.8667 - val_loss: 23.5381\n",
      "Epoch 422/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.7581 - val_loss: 25.0601\n",
      "Epoch 423/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.8089 - val_loss: 23.9918\n",
      "Epoch 424/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.6170 - val_loss: 25.0590\n",
      "Epoch 425/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.8638 - val_loss: 27.2142\n",
      "Epoch 426/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.3682 - val_loss: 25.3363\n",
      "Epoch 427/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 16.5839 - val_loss: 24.1121\n",
      "Epoch 428/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.1486 - val_loss: 22.9152\n",
      "Epoch 429/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.9346 - val_loss: 29.3752\n",
      "Epoch 430/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.8724 - val_loss: 24.8219\n",
      "Epoch 431/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.9644 - val_loss: 22.7539\n",
      "Epoch 432/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.9672 - val_loss: 24.1002\n",
      "Epoch 433/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.4418 - val_loss: 21.8158\n",
      "Epoch 434/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.9589 - val_loss: 25.0635\n",
      "Epoch 435/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.7624 - val_loss: 22.7758\n",
      "Epoch 436/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.6874 - val_loss: 23.4177\n",
      "Epoch 437/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 18.2785 - val_loss: 22.0692\n",
      "Epoch 438/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.4291 - val_loss: 23.2958\n",
      "Epoch 439/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.2689 - val_loss: 23.4855\n",
      "Epoch 440/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.7896 - val_loss: 23.6021\n",
      "Epoch 441/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.9958 - val_loss: 22.7239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 442/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.9393 - val_loss: 25.2861\n",
      "Epoch 443/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.0658 - val_loss: 24.2988\n",
      "Epoch 444/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.6512 - val_loss: 23.5639\n",
      "Epoch 445/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.0788 - val_loss: 22.9853\n",
      "Epoch 446/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.7536 - val_loss: 23.2645\n",
      "Epoch 447/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.5731 - val_loss: 23.7618\n",
      "Epoch 448/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.9873 - val_loss: 24.2883\n",
      "Epoch 449/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.9823 - val_loss: 25.8445\n",
      "Epoch 450/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.4212 - val_loss: 24.1671\n",
      "Epoch 451/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.6336 - val_loss: 24.7829\n",
      "Epoch 452/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.6682 - val_loss: 23.2201\n",
      "Epoch 453/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.1455 - val_loss: 25.4252\n",
      "Epoch 454/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.1015 - val_loss: 21.8776\n",
      "Epoch 455/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.6644 - val_loss: 25.1459\n",
      "Epoch 456/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.0155 - val_loss: 25.1361\n",
      "Epoch 457/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 17.5757 - val_loss: 25.5974\n",
      "Epoch 458/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.1409 - val_loss: 26.6384\n",
      "Epoch 459/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.3438 - val_loss: 23.0451\n",
      "Epoch 460/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.5172 - val_loss: 25.5316\n",
      "Epoch 461/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.0947 - val_loss: 22.2194\n",
      "Epoch 462/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.5483 - val_loss: 23.0598\n",
      "Epoch 463/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.5541 - val_loss: 24.9859\n",
      "Epoch 464/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.3584 - val_loss: 23.4180\n",
      "Epoch 465/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.8002 - val_loss: 24.6685\n",
      "Epoch 466/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.2775 - val_loss: 24.3678\n",
      "Epoch 467/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.1509 - val_loss: 22.4660\n",
      "Epoch 468/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.4377 - val_loss: 23.7382\n",
      "Epoch 469/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.8272 - val_loss: 23.2578\n",
      "Epoch 470/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.0193 - val_loss: 23.0690\n",
      "Epoch 471/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.0676 - val_loss: 24.4392\n",
      "Epoch 472/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.9420 - val_loss: 24.0757\n",
      "Epoch 473/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.4185 - val_loss: 22.3050\n",
      "Epoch 474/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.8337 - val_loss: 25.2314\n",
      "Epoch 475/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.7398 - val_loss: 23.8549\n",
      "Epoch 476/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.0188 - val_loss: 24.0207\n",
      "Epoch 477/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.0181 - val_loss: 24.9063\n",
      "Epoch 478/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.2432 - val_loss: 23.3938\n",
      "Epoch 479/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.7129 - val_loss: 23.9550\n",
      "Epoch 480/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.9044 - val_loss: 25.5795\n",
      "Epoch 481/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.9560 - val_loss: 22.8860\n",
      "Epoch 482/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.3824 - val_loss: 24.6958\n",
      "Epoch 483/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.7654 - val_loss: 22.8787\n",
      "Epoch 484/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.4962 - val_loss: 24.7317\n",
      "Epoch 485/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.0303 - val_loss: 24.5961\n",
      "Epoch 486/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.7989 - val_loss: 23.9486\n",
      "Epoch 487/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.2605 - val_loss: 24.6183\n",
      "Epoch 488/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.8567 - val_loss: 23.9107\n",
      "Epoch 489/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.2266 - val_loss: 24.3577\n",
      "Epoch 490/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.6683 - val_loss: 25.3946\n",
      "Epoch 491/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.1855 - val_loss: 24.2565\n",
      "Epoch 492/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.5399 - val_loss: 24.2422\n",
      "Epoch 493/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.0710 - val_loss: 27.6308\n",
      "Epoch 494/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.2333 - val_loss: 23.4467\n",
      "Epoch 495/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.1981 - val_loss: 23.5934\n",
      "Epoch 496/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.6715 - val_loss: 23.6364\n",
      "Epoch 497/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.0644 - val_loss: 24.1411\n",
      "Epoch 498/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.8816 - val_loss: 23.7117\n",
      "Epoch 499/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.2673 - val_loss: 22.5063\n",
      "Epoch 500/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.7955 - val_loss: 23.8847\n",
      "Epoch 501/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.2934 - val_loss: 22.6709\n",
      "Epoch 502/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.3267 - val_loss: 24.7425\n",
      "Epoch 503/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 16.4863 - val_loss: 22.5069\n",
      "Epoch 504/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.1346 - val_loss: 23.5882\n",
      "Epoch 505/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.2571 - val_loss: 22.3361\n",
      "Epoch 506/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.7324 - val_loss: 23.6469\n",
      "Epoch 507/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.7531 - val_loss: 22.7806\n",
      "Epoch 508/2000\n",
      "1500/1500 [==============================] - 0s 30us/sample - loss: 17.6860 - val_loss: 24.0873\n",
      "Epoch 509/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.1934 - val_loss: 23.4545\n",
      "Epoch 510/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.2944 - val_loss: 23.1530\n",
      "Epoch 511/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.4226 - val_loss: 24.9793\n",
      "Epoch 512/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.9893 - val_loss: 22.2900\n",
      "Epoch 513/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.4214 - val_loss: 22.3460\n",
      "Epoch 514/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.5704 - val_loss: 27.0321\n",
      "Epoch 515/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.0942 - val_loss: 23.0031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 516/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 16.7124 - val_loss: 23.5847\n",
      "Epoch 517/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.8677 - val_loss: 23.3002\n",
      "Epoch 518/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.2270 - val_loss: 23.7927\n",
      "Epoch 519/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.9747 - val_loss: 22.6842\n",
      "Epoch 520/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.9890 - val_loss: 24.8840\n",
      "Epoch 521/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.4373 - val_loss: 25.4618\n",
      "Epoch 522/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.3563 - val_loss: 25.1495\n",
      "Epoch 523/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.3333 - val_loss: 26.9172\n",
      "Epoch 524/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.0874 - val_loss: 22.5292\n",
      "Epoch 525/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.6629 - val_loss: 23.1568\n",
      "Epoch 526/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.6459 - val_loss: 24.6097\n",
      "Epoch 527/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.5046 - val_loss: 22.6283\n",
      "Epoch 528/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.0771 - val_loss: 23.1236\n",
      "Epoch 529/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.2143 - val_loss: 23.2406\n",
      "Epoch 530/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.4174 - val_loss: 23.0523\n",
      "Epoch 531/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.7821 - val_loss: 24.4279\n",
      "Epoch 532/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.4866 - val_loss: 22.9090\n",
      "Epoch 533/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.7484 - val_loss: 23.7486\n",
      "Epoch 534/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.4252 - val_loss: 23.7225\n",
      "Epoch 535/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.4680 - val_loss: 22.7322\n",
      "Epoch 536/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.9923 - val_loss: 22.4091\n",
      "Epoch 537/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.0236 - val_loss: 23.2231\n",
      "Epoch 538/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.6630 - val_loss: 23.8225\n",
      "Epoch 539/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.5455 - val_loss: 24.9584\n",
      "Epoch 540/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.1376 - val_loss: 21.9013\n",
      "Epoch 541/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.7601 - val_loss: 22.9042\n",
      "Epoch 542/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 16.9987 - val_loss: 25.0022\n",
      "Epoch 543/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 17.6167 - val_loss: 22.1528\n",
      "Epoch 544/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.1154 - val_loss: 22.7926\n",
      "Epoch 545/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.4819 - val_loss: 22.7603\n",
      "Epoch 546/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.5725 - val_loss: 22.8130\n",
      "Epoch 547/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.4754 - val_loss: 21.9543\n",
      "Epoch 548/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.5096 - val_loss: 22.7543\n",
      "Epoch 549/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.7376 - val_loss: 23.0506\n",
      "Epoch 550/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.4751 - val_loss: 24.2437\n",
      "Epoch 551/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.7519 - val_loss: 25.4913\n",
      "Epoch 552/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.7135 - val_loss: 24.6584\n",
      "Epoch 553/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.8719 - val_loss: 24.0206\n",
      "Epoch 554/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.5989 - val_loss: 24.5518\n",
      "Epoch 555/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.0478 - val_loss: 27.7971\n",
      "Epoch 556/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.7566 - val_loss: 26.7234\n",
      "Epoch 557/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.8717 - val_loss: 26.3779\n",
      "Epoch 558/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.7446 - val_loss: 22.5047\n",
      "Epoch 559/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 16.9374 - val_loss: 23.0647\n",
      "Epoch 560/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.0285 - val_loss: 24.4678\n",
      "Epoch 561/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 15.9026 - val_loss: 22.5585\n",
      "Epoch 562/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.8700 - val_loss: 22.9813\n",
      "Epoch 563/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.3417 - val_loss: 23.8499\n",
      "Epoch 564/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.8169 - val_loss: 24.3273\n",
      "Epoch 565/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.4027 - val_loss: 25.6741\n",
      "Epoch 566/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.6113 - val_loss: 24.1171\n",
      "Epoch 567/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.9342 - val_loss: 22.2095\n",
      "Epoch 568/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.3136 - val_loss: 27.0507\n",
      "Epoch 569/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.0834 - val_loss: 22.3346\n",
      "Epoch 570/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.2290 - val_loss: 23.8903\n",
      "Epoch 571/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.4536 - val_loss: 21.8184\n",
      "Epoch 572/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.8072 - val_loss: 23.7496\n",
      "Epoch 573/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.0902 - val_loss: 23.6037\n",
      "Epoch 574/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.6274 - val_loss: 23.0686\n",
      "Epoch 575/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.4940 - val_loss: 23.8120\n",
      "Epoch 576/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.5085 - val_loss: 24.3742\n",
      "Epoch 577/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.5520 - val_loss: 23.3385\n",
      "Epoch 578/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.4372 - val_loss: 23.7888\n",
      "Epoch 579/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.3957 - val_loss: 22.7362\n",
      "Epoch 580/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.5526 - val_loss: 26.5120\n",
      "Epoch 581/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.0272 - val_loss: 22.3548\n",
      "Epoch 582/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.2457 - val_loss: 23.4728\n",
      "Epoch 583/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.4478 - val_loss: 23.2082\n",
      "Epoch 584/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 16.2067 - val_loss: 23.3112\n",
      "Epoch 585/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.2642 - val_loss: 24.3880\n",
      "Epoch 586/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.9920 - val_loss: 24.3940\n",
      "Epoch 587/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.1273 - val_loss: 21.9500\n",
      "Epoch 588/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.3130 - val_loss: 22.9358\n",
      "Epoch 589/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.3172 - val_loss: 24.2256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 590/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.9317 - val_loss: 28.2554\n",
      "Epoch 591/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.9331 - val_loss: 25.1632\n",
      "Epoch 592/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.6524 - val_loss: 26.1201\n",
      "Epoch 593/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.4102 - val_loss: 22.9238\n",
      "Epoch 594/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.1830 - val_loss: 24.0572\n",
      "Epoch 595/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 15.4669 - val_loss: 23.7171\n",
      "Epoch 596/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.1893 - val_loss: 24.2858\n",
      "Epoch 597/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.7835 - val_loss: 22.1775\n",
      "Epoch 598/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.9001 - val_loss: 22.7034\n",
      "Epoch 599/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.7464 - val_loss: 23.7126\n",
      "Epoch 600/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.8473 - val_loss: 23.0405\n",
      "Epoch 601/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.6937 - val_loss: 23.6911\n",
      "Epoch 602/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.7382 - val_loss: 27.7662\n",
      "Epoch 603/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.8764 - val_loss: 23.5288\n",
      "Epoch 604/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.2905 - val_loss: 23.7168\n",
      "Epoch 605/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.0146 - val_loss: 22.9607\n",
      "Epoch 606/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.7988 - val_loss: 21.8551\n",
      "Epoch 607/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.0642 - val_loss: 22.4524\n",
      "Epoch 608/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.9498 - val_loss: 22.5906\n",
      "Epoch 609/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.3357 - val_loss: 24.0448\n",
      "Epoch 610/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 16.6565 - val_loss: 24.4371\n",
      "Epoch 611/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.2710 - val_loss: 23.4257\n",
      "Epoch 612/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.5509 - val_loss: 22.9422\n",
      "Epoch 613/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.2177 - val_loss: 22.1506\n",
      "Epoch 614/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.0903 - val_loss: 23.1769\n",
      "Epoch 615/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.6072 - val_loss: 23.5674\n",
      "Epoch 616/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.1078 - val_loss: 25.6702\n",
      "Epoch 617/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.5507 - val_loss: 25.3187\n",
      "Epoch 618/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.6066 - val_loss: 22.1793\n",
      "Epoch 619/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.1916 - val_loss: 24.8669\n",
      "Epoch 620/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.2453 - val_loss: 22.5111\n",
      "Epoch 621/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.6202 - val_loss: 24.3225\n",
      "Epoch 622/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.6470 - val_loss: 22.8863\n",
      "Epoch 623/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.2603 - val_loss: 23.5743\n",
      "Epoch 624/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.4946 - val_loss: 23.6944\n",
      "Epoch 625/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.2493 - val_loss: 21.9231\n",
      "Epoch 626/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.8812 - val_loss: 23.7380\n",
      "Epoch 627/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 15.2675 - val_loss: 22.1692\n",
      "Epoch 628/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 15.7092 - val_loss: 24.5854\n",
      "Epoch 629/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.8722 - val_loss: 23.7786\n",
      "Epoch 630/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.4197 - val_loss: 24.3763\n",
      "Epoch 631/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.0165 - val_loss: 22.4833\n",
      "Epoch 632/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 15.8465 - val_loss: 26.2466\n",
      "Epoch 633/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.7174 - val_loss: 23.3054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|█████████████████████████████████████████████████████████████▌                    | 12/16 [06:34<01:59, 29.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'batc_normalization': False, 'batch_size': 64, 'dropout': 0, 'epochs': 2000, 'first_neuron': 320, 'hidden_layers': 1, 'hidden_neuron': 50, 'kernel_initializer': 'normal', 'last_activation': 'linear', 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "Train on 1500 samples, validate on 375 samples\n",
      "Epoch 1/2000\n",
      "1500/1500 [==============================] - 0s 221us/sample - loss: 2185.4808 - val_loss: 2184.9326\n",
      "Epoch 2/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 2180.1741 - val_loss: 2174.0997\n",
      "Epoch 3/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 2159.3644 - val_loss: 2138.0469\n",
      "Epoch 4/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 2101.7031 - val_loss: 2051.2373\n",
      "Epoch 5/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 1979.1202 - val_loss: 1882.8273\n",
      "Epoch 6/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 1759.4171 - val_loss: 1601.1120\n",
      "Epoch 7/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 1412.2268 - val_loss: 1177.8963\n",
      "Epoch 8/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 968.9583 - val_loss: 767.1173\n",
      "Epoch 9/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 678.9165 - val_loss: 598.1240\n",
      "Epoch 10/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 593.6828 - val_loss: 537.0115\n",
      "Epoch 11/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 542.8747 - val_loss: 497.1760\n",
      "Epoch 12/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 499.1989 - val_loss: 455.2994\n",
      "Epoch 13/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 457.0596 - val_loss: 416.4215\n",
      "Epoch 14/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 415.1019 - val_loss: 378.2986\n",
      "Epoch 15/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 371.2190 - val_loss: 337.3326\n",
      "Epoch 16/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 327.7679 - val_loss: 297.0830\n",
      "Epoch 17/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 282.5379 - val_loss: 252.3394\n",
      "Epoch 18/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 238.5954 - val_loss: 214.1479\n",
      "Epoch 19/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 198.3580 - val_loss: 177.6299\n",
      "Epoch 20/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 163.4486 - val_loss: 147.2708\n",
      "Epoch 21/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 135.5583 - val_loss: 124.5230\n",
      "Epoch 22/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 114.1719 - val_loss: 106.4839\n",
      "Epoch 23/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 96.9184 - val_loss: 91.9876\n",
      "Epoch 24/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 82.6021 - val_loss: 77.4726\n",
      "Epoch 25/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 71.2377 - val_loss: 67.8117\n",
      "Epoch 26/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 62.1427 - val_loss: 59.9586\n",
      "Epoch 27/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 55.3057 - val_loss: 54.8695\n",
      "Epoch 28/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 49.1496 - val_loss: 49.6320\n",
      "Epoch 29/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 44.2363 - val_loss: 45.5404\n",
      "Epoch 30/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 40.7457 - val_loss: 41.5360\n",
      "Epoch 31/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 38.1657 - val_loss: 40.3594\n",
      "Epoch 32/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 35.7424 - val_loss: 36.4348\n",
      "Epoch 33/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 33.7758 - val_loss: 35.3909\n",
      "Epoch 34/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 32.1764 - val_loss: 33.9199\n",
      "Epoch 35/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 30.8782 - val_loss: 32.1714\n",
      "Epoch 36/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 29.6951 - val_loss: 32.4441\n",
      "Epoch 37/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 29.1122 - val_loss: 30.7501\n",
      "Epoch 38/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 29.1783 - val_loss: 29.7233\n",
      "Epoch 39/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 27.7016 - val_loss: 28.2620\n",
      "Epoch 40/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 26.7870 - val_loss: 29.3944\n",
      "Epoch 41/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 26.4740 - val_loss: 26.4362\n",
      "Epoch 42/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 25.7131 - val_loss: 26.0017\n",
      "Epoch 43/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 25.0971 - val_loss: 27.0430\n",
      "Epoch 44/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 24.4833 - val_loss: 25.4927\n",
      "Epoch 45/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 24.5463 - val_loss: 27.0661\n",
      "Epoch 46/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 24.4367 - val_loss: 26.1422\n",
      "Epoch 47/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 23.6523 - val_loss: 25.8826\n",
      "Epoch 48/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 23.5879 - val_loss: 23.7484\n",
      "Epoch 49/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 23.4643 - val_loss: 26.7558\n",
      "Epoch 50/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 23.4778 - val_loss: 24.2747\n",
      "Epoch 51/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 22.9615 - val_loss: 24.4858\n",
      "Epoch 52/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 22.8720 - val_loss: 27.3144\n",
      "Epoch 53/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 23.2842 - val_loss: 23.5951\n",
      "Epoch 54/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 22.6378 - val_loss: 26.1063\n",
      "Epoch 55/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 22.9360 - val_loss: 23.2765\n",
      "Epoch 56/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 23.2931 - val_loss: 24.0566\n",
      "Epoch 57/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 22.3368 - val_loss: 23.5314\n",
      "Epoch 58/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 22.2729 - val_loss: 23.3439\n",
      "Epoch 59/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 22.3157 - val_loss: 24.5065\n",
      "Epoch 60/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 22.4319 - val_loss: 23.9811\n",
      "Epoch 61/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 22.3061 - val_loss: 23.0646\n",
      "Epoch 62/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 22.0917 - val_loss: 23.8279\n",
      "Epoch 63/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 22.2016 - val_loss: 23.3717\n",
      "Epoch 64/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 22.4875 - val_loss: 22.8553\n",
      "Epoch 65/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 22.3520 - val_loss: 23.8533\n",
      "Epoch 66/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 22.3944 - val_loss: 23.0803\n",
      "Epoch 67/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 22.1803 - val_loss: 23.9428\n",
      "Epoch 68/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 22.1274 - val_loss: 25.5432\n",
      "Epoch 69/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 22.3136 - val_loss: 25.1515\n",
      "Epoch 70/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 21.9352 - val_loss: 23.8522\n",
      "Epoch 71/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.7839 - val_loss: 23.7475\n",
      "Epoch 72/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 22.1448 - val_loss: 23.9265\n",
      "Epoch 73/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 21.8215 - val_loss: 23.4875\n",
      "Epoch 74/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 21.6306 - val_loss: 22.8462\n",
      "Epoch 75/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.4227 - val_loss: 23.1168\n",
      "Epoch 76/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.4812 - val_loss: 23.9889\n",
      "Epoch 77/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.4554 - val_loss: 27.5588\n",
      "Epoch 78/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 22.8054 - val_loss: 23.0579\n",
      "Epoch 79/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.9648 - val_loss: 25.1213\n",
      "Epoch 80/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 21.7950 - val_loss: 23.2248\n",
      "Epoch 81/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 22.4478 - val_loss: 24.7464\n",
      "Epoch 82/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.7870 - val_loss: 23.2115\n",
      "Epoch 83/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.5712 - val_loss: 24.6090\n",
      "Epoch 84/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 22.2436 - val_loss: 23.3322\n",
      "Epoch 85/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.2692 - val_loss: 24.2789\n",
      "Epoch 86/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.5253 - val_loss: 23.5947\n",
      "Epoch 87/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 21.4777 - val_loss: 23.4072\n",
      "Epoch 88/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.1214 - val_loss: 23.9153\n",
      "Epoch 89/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.6443 - val_loss: 23.2571\n",
      "Epoch 90/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.0826 - val_loss: 25.0075\n",
      "Epoch 91/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 21.4987 - val_loss: 23.3778\n",
      "Epoch 92/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.0263 - val_loss: 22.3628\n",
      "Epoch 93/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.1492 - val_loss: 24.4371\n",
      "Epoch 94/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.1674 - val_loss: 24.5280\n",
      "Epoch 95/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 21.5137 - val_loss: 22.8057\n",
      "Epoch 96/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 21.4163 - val_loss: 24.6701\n",
      "Epoch 97/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.4212 - val_loss: 24.5809\n",
      "Epoch 98/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.3809 - val_loss: 22.4170\n",
      "Epoch 99/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.8610 - val_loss: 25.0153\n",
      "Epoch 100/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 21.8300 - val_loss: 25.5260\n",
      "Epoch 101/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 21.7307 - val_loss: 22.8659\n",
      "Epoch 102/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.6820 - val_loss: 23.6497\n",
      "Epoch 103/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.7793 - val_loss: 25.0347\n",
      "Epoch 104/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.2802 - val_loss: 22.9913\n",
      "Epoch 105/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.6153 - val_loss: 23.7279\n",
      "Epoch 106/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.3558 - val_loss: 23.4277\n",
      "Epoch 107/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.8609 - val_loss: 23.7634\n",
      "Epoch 108/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.0147 - val_loss: 23.4283\n",
      "Epoch 109/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.1929 - val_loss: 23.6171\n",
      "Epoch 110/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.4923 - val_loss: 23.7448\n",
      "Epoch 111/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.1671 - val_loss: 22.5402\n",
      "Epoch 112/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 21.0578 - val_loss: 23.0776\n",
      "Epoch 113/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.6637 - val_loss: 23.1862\n",
      "Epoch 114/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.8315 - val_loss: 23.0175\n",
      "Epoch 115/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.7726 - val_loss: 22.6830\n",
      "Epoch 116/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.9373 - val_loss: 23.6976\n",
      "Epoch 117/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.6608 - val_loss: 23.9427\n",
      "Epoch 118/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.6569 - val_loss: 22.3525\n",
      "Epoch 119/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.3276 - val_loss: 23.0521\n",
      "Epoch 120/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.6252 - val_loss: 23.0923\n",
      "Epoch 121/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.4226 - val_loss: 22.8455\n",
      "Epoch 122/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.5522 - val_loss: 22.8681\n",
      "Epoch 123/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.9354 - val_loss: 22.7549\n",
      "Epoch 124/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.7972 - val_loss: 23.2364\n",
      "Epoch 125/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 21.3249 - val_loss: 25.8130\n",
      "Epoch 126/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.9322 - val_loss: 24.6034\n",
      "Epoch 127/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.7668 - val_loss: 23.2870\n",
      "Epoch 128/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.2909 - val_loss: 22.6261\n",
      "Epoch 129/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.5811 - val_loss: 23.8814\n",
      "Epoch 130/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.7671 - val_loss: 23.1475\n",
      "Epoch 131/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.4683 - val_loss: 23.2495\n",
      "Epoch 132/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.3530 - val_loss: 23.7759\n",
      "Epoch 133/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.7893 - val_loss: 22.9087\n",
      "Epoch 134/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.3720 - val_loss: 23.8948\n",
      "Epoch 135/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.6086 - val_loss: 23.8532\n",
      "Epoch 136/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.5121 - val_loss: 24.4320\n",
      "Epoch 137/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 21.5748 - val_loss: 23.0893\n",
      "Epoch 138/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.8545 - val_loss: 22.8651\n",
      "Epoch 139/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.4249 - val_loss: 24.3353\n",
      "Epoch 140/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.0826 - val_loss: 23.6153\n",
      "Epoch 141/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.5992 - val_loss: 23.9941\n",
      "Epoch 142/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.1423 - val_loss: 23.9926\n",
      "Epoch 143/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 21.9234 - val_loss: 23.6479\n",
      "Epoch 144/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.6744 - val_loss: 22.9312\n",
      "Epoch 145/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.3864 - val_loss: 24.1299\n",
      "Epoch 146/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.6100 - val_loss: 23.0325\n",
      "Epoch 147/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.7094 - val_loss: 22.6451\n",
      "Epoch 148/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.5470 - val_loss: 24.1242\n",
      "Epoch 149/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.1932 - val_loss: 22.8317\n",
      "Epoch 150/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.5375 - val_loss: 24.0364\n",
      "Epoch 151/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.6742 - val_loss: 23.2464\n",
      "Epoch 152/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.0480 - val_loss: 24.1727\n",
      "Epoch 153/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 21.0469 - val_loss: 23.2243\n",
      "Epoch 154/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.7819 - val_loss: 24.2602\n",
      "Epoch 155/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.9943 - val_loss: 23.7718\n",
      "Epoch 156/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.3576 - val_loss: 24.3434\n",
      "Epoch 157/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.6208 - val_loss: 22.4942\n",
      "Epoch 158/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.6445 - val_loss: 23.2707\n",
      "Epoch 159/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.0255 - val_loss: 24.1030\n",
      "Epoch 160/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.9361 - val_loss: 23.2661\n",
      "Epoch 161/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.3931 - val_loss: 22.8047\n",
      "Epoch 162/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.8667 - val_loss: 23.0293\n",
      "Epoch 163/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.7344 - val_loss: 22.8193\n",
      "Epoch 164/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.1037 - val_loss: 22.8310\n",
      "Epoch 165/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 21.0059 - val_loss: 22.9395\n",
      "Epoch 166/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.1081 - val_loss: 25.3992\n",
      "Epoch 167/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.4870 - val_loss: 22.9403\n",
      "Epoch 168/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.6291 - val_loss: 22.9745\n",
      "Epoch 169/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.1415 - val_loss: 22.8488\n",
      "Epoch 170/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.9774 - val_loss: 23.7280\n",
      "Epoch 171/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.4890 - val_loss: 23.5852\n",
      "Epoch 172/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.1382 - val_loss: 23.9355\n",
      "Epoch 173/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.0899 - val_loss: 22.8833\n",
      "Epoch 174/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.6423 - val_loss: 24.6863\n",
      "Epoch 175/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.7033 - val_loss: 23.5897\n",
      "Epoch 176/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.7252 - val_loss: 23.3782\n",
      "Epoch 177/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.4020 - val_loss: 24.3970\n",
      "Epoch 178/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.8807 - val_loss: 22.5240\n",
      "Epoch 179/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.2821 - val_loss: 22.3582\n",
      "Epoch 180/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 20.3090 - val_loss: 24.3913\n",
      "Epoch 181/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.3734 - val_loss: 22.4425\n",
      "Epoch 182/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.0342 - val_loss: 23.4325\n",
      "Epoch 183/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.0503 - val_loss: 23.5628\n",
      "Epoch 184/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.6631 - val_loss: 22.9433\n",
      "Epoch 185/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.9400 - val_loss: 23.4986\n",
      "Epoch 186/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.4711 - val_loss: 23.0819\n",
      "Epoch 187/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.5579 - val_loss: 23.2982\n",
      "Epoch 188/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.9041 - val_loss: 22.9649\n",
      "Epoch 189/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.9613 - val_loss: 23.8584\n",
      "Epoch 190/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.0719 - val_loss: 23.6303\n",
      "Epoch 191/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.0660 - val_loss: 23.2664\n",
      "Epoch 192/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.1757 - val_loss: 23.4582\n",
      "Epoch 193/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.1921 - val_loss: 22.7972\n",
      "Epoch 194/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.7450 - val_loss: 23.4900\n",
      "Epoch 195/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.0410 - val_loss: 24.0103\n",
      "Epoch 196/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 20.7809 - val_loss: 22.8677\n",
      "Epoch 197/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.1175 - val_loss: 23.2330\n",
      "Epoch 198/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.6866 - val_loss: 25.8351\n",
      "Epoch 199/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.8954 - val_loss: 23.9323\n",
      "Epoch 200/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.5017 - val_loss: 23.6741\n",
      "Epoch 201/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.8518 - val_loss: 23.0136\n",
      "Epoch 202/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.8434 - val_loss: 22.5331\n",
      "Epoch 203/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.8710 - val_loss: 23.7828\n",
      "Epoch 204/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.2658 - val_loss: 23.9119\n",
      "Epoch 205/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.8931 - val_loss: 22.2881\n",
      "Epoch 206/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.7257 - val_loss: 23.0731\n",
      "Epoch 207/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.8881 - val_loss: 23.0068\n",
      "Epoch 208/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.9074 - val_loss: 23.9848\n",
      "Epoch 209/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.9719 - val_loss: 23.0846\n",
      "Epoch 210/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.9312 - val_loss: 24.3570\n",
      "Epoch 211/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.0194 - val_loss: 22.6841\n",
      "Epoch 212/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.4171 - val_loss: 22.6890\n",
      "Epoch 213/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.4931 - val_loss: 23.0131\n",
      "Epoch 214/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.7819 - val_loss: 22.7626\n",
      "Epoch 215/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.8044 - val_loss: 22.8209\n",
      "Epoch 216/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.4537 - val_loss: 23.5068\n",
      "Epoch 217/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.8350 - val_loss: 23.0994\n",
      "Epoch 218/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.6674 - val_loss: 23.1764\n",
      "Epoch 219/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.2567 - val_loss: 24.2775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 220/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.0719 - val_loss: 22.9889\n",
      "Epoch 221/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.6573 - val_loss: 23.0742\n",
      "Epoch 222/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.5401 - val_loss: 26.2120\n",
      "Epoch 223/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 21.0243 - val_loss: 22.8763\n",
      "Epoch 224/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.1140 - val_loss: 23.2634\n",
      "Epoch 225/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.4191 - val_loss: 23.6546\n",
      "Epoch 226/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.6988 - val_loss: 22.5160\n",
      "Epoch 227/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.7455 - val_loss: 24.0072\n",
      "Epoch 228/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.6615 - val_loss: 24.8614\n",
      "Epoch 229/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.7427 - val_loss: 22.7397\n",
      "Epoch 230/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.7942 - val_loss: 23.4466\n",
      "Epoch 231/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.7860 - val_loss: 23.1486\n",
      "Epoch 232/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.3150 - val_loss: 22.5085\n",
      "Epoch 233/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.1486 - val_loss: 22.6606\n",
      "Epoch 234/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.5333 - val_loss: 22.5431\n",
      "Epoch 235/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 19.4478 - val_loss: 22.0185\n",
      "Epoch 236/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.7559 - val_loss: 23.4803\n",
      "Epoch 237/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.1085 - val_loss: 22.8469\n",
      "Epoch 238/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.8557 - val_loss: 23.9873\n",
      "Epoch 239/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.6486 - val_loss: 22.9738\n",
      "Epoch 240/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.1279 - val_loss: 23.1589\n",
      "Epoch 241/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.4799 - val_loss: 22.9611\n",
      "Epoch 242/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.8539 - val_loss: 24.1504\n",
      "Epoch 243/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.3233 - val_loss: 23.9945\n",
      "Epoch 244/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.9639 - val_loss: 23.3735\n",
      "Epoch 245/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.0438 - val_loss: 23.1209\n",
      "Epoch 246/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.4236 - val_loss: 22.7828\n",
      "Epoch 247/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.3604 - val_loss: 22.5375\n",
      "Epoch 248/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.5545 - val_loss: 24.0953\n",
      "Epoch 249/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.0904 - val_loss: 25.0949\n",
      "Epoch 250/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.6188 - val_loss: 22.8926\n",
      "Epoch 251/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.9465 - val_loss: 22.1962\n",
      "Epoch 252/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.5565 - val_loss: 22.6889\n",
      "Epoch 253/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.2273 - val_loss: 22.8968\n",
      "Epoch 254/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.4921 - val_loss: 23.7647\n",
      "Epoch 255/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.8067 - val_loss: 22.9657\n",
      "Epoch 256/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.4933 - val_loss: 24.1304\n",
      "Epoch 257/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.8346 - val_loss: 23.5011\n",
      "Epoch 258/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.5831 - val_loss: 24.2034\n",
      "Epoch 259/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.6466 - val_loss: 24.0924\n",
      "Epoch 260/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.1852 - val_loss: 22.0609\n",
      "Epoch 261/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.0290 - val_loss: 22.9405\n",
      "Epoch 262/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.7190 - val_loss: 22.5735\n",
      "Epoch 263/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.8378 - val_loss: 23.1799\n",
      "Epoch 264/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.6133 - val_loss: 23.0584\n",
      "Epoch 265/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.3370 - val_loss: 22.8227\n",
      "Epoch 266/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.7226 - val_loss: 24.7032\n",
      "Epoch 267/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.3475 - val_loss: 22.4417\n",
      "Epoch 268/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.4271 - val_loss: 22.4289\n",
      "Epoch 269/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.1731 - val_loss: 25.8053\n",
      "Epoch 270/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.7584 - val_loss: 22.6152\n",
      "Epoch 271/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.8006 - val_loss: 22.7400\n",
      "Epoch 272/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.3920 - val_loss: 22.3882\n",
      "Epoch 273/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.0000 - val_loss: 22.2754\n",
      "Epoch 274/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.1047 - val_loss: 22.9464\n",
      "Epoch 275/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.9544 - val_loss: 22.4961\n",
      "Epoch 276/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.1036 - val_loss: 22.6759\n",
      "Epoch 277/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.3221 - val_loss: 22.1437\n",
      "Epoch 278/2000\n",
      "1500/1500 [==============================] - 0s 31us/sample - loss: 19.4771 - val_loss: 23.3125\n",
      "Epoch 279/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.8603 - val_loss: 23.5974\n",
      "Epoch 280/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.1201 - val_loss: 24.2024\n",
      "Epoch 281/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.5326 - val_loss: 23.1882\n",
      "Epoch 282/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.7697 - val_loss: 23.0769\n",
      "Epoch 283/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.6923 - val_loss: 25.5730\n",
      "Epoch 284/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.5528 - val_loss: 22.3290\n",
      "Epoch 285/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.5257 - val_loss: 24.1119\n",
      "Epoch 286/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.5802 - val_loss: 23.3235\n",
      "Epoch 287/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.0300 - val_loss: 23.3221\n",
      "Epoch 288/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.1288 - val_loss: 23.2243\n",
      "Epoch 289/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.2047 - val_loss: 23.3111\n",
      "Epoch 290/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.0422 - val_loss: 23.5787\n",
      "Epoch 291/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.0327 - val_loss: 22.8956\n",
      "Epoch 292/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.4382 - val_loss: 23.6772\n",
      "Epoch 293/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.5300 - val_loss: 22.4056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 294/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.4503 - val_loss: 23.2448\n",
      "Epoch 295/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.5798 - val_loss: 22.5996\n",
      "Epoch 296/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.4878 - val_loss: 23.1965\n",
      "Epoch 297/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.1217 - val_loss: 23.0157\n",
      "Epoch 298/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.5645 - val_loss: 22.6183\n",
      "Epoch 299/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.7983 - val_loss: 23.7781\n",
      "Epoch 300/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.0468 - val_loss: 23.3879\n",
      "Epoch 301/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.6133 - val_loss: 23.8188\n",
      "Epoch 302/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.3938 - val_loss: 21.9083\n",
      "Epoch 303/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.0200 - val_loss: 23.0216\n",
      "Epoch 304/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.1579 - val_loss: 23.3599\n",
      "Epoch 305/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.0932 - val_loss: 22.8257\n",
      "Epoch 306/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.6201 - val_loss: 23.3795\n",
      "Epoch 307/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.3064 - val_loss: 23.4582\n",
      "Epoch 308/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.0963 - val_loss: 22.2492\n",
      "Epoch 309/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.8776 - val_loss: 23.9773\n",
      "Epoch 310/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.2088 - val_loss: 22.9268\n",
      "Epoch 311/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.2555 - val_loss: 22.2066\n",
      "Epoch 312/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.5562 - val_loss: 22.3032\n",
      "Epoch 313/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.2490 - val_loss: 22.7794\n",
      "Epoch 314/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.2580 - val_loss: 23.6306\n",
      "Epoch 315/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.6696 - val_loss: 23.4160\n",
      "Epoch 316/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.0059 - val_loss: 23.3271\n",
      "Epoch 317/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.4772 - val_loss: 22.0957\n",
      "Epoch 318/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.7615 - val_loss: 23.0248\n",
      "Epoch 319/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.4257 - val_loss: 22.8917\n",
      "Epoch 320/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.3574 - val_loss: 22.4902\n",
      "Epoch 321/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.7453 - val_loss: 23.6459\n",
      "Epoch 322/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.1041 - val_loss: 21.9573\n",
      "Epoch 323/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.7551 - val_loss: 22.3131\n",
      "Epoch 324/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.4558 - val_loss: 22.9745\n",
      "Epoch 325/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.1778 - val_loss: 23.5792\n",
      "Epoch 326/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.8405 - val_loss: 23.7697\n",
      "Epoch 327/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.1093 - val_loss: 24.1296\n",
      "Epoch 328/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.0036 - val_loss: 23.3898\n",
      "Epoch 329/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.6067 - val_loss: 22.2446\n",
      "Epoch 330/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.4620 - val_loss: 22.6204\n",
      "Epoch 331/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.5361 - val_loss: 23.0025\n",
      "Epoch 332/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.7641 - val_loss: 22.1912\n",
      "Epoch 333/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.6837 - val_loss: 22.7579\n",
      "Epoch 334/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.8222 - val_loss: 22.8166\n",
      "Epoch 335/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.8424 - val_loss: 22.6091\n",
      "Epoch 336/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.1731 - val_loss: 22.5270\n",
      "Epoch 337/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 19.2145 - val_loss: 22.8079\n",
      "Epoch 338/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.8051 - val_loss: 23.0774\n",
      "Epoch 339/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.8025 - val_loss: 23.4600\n",
      "Epoch 340/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.7842 - val_loss: 22.9178\n",
      "Epoch 341/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.4465 - val_loss: 23.5266\n",
      "Epoch 342/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.5820 - val_loss: 25.1906\n",
      "Epoch 343/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.8057 - val_loss: 23.8943\n",
      "Epoch 344/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.3842 - val_loss: 23.4468\n",
      "Epoch 345/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.4898 - val_loss: 24.4695\n",
      "Epoch 346/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.1481 - val_loss: 23.1573\n",
      "Epoch 347/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.7090 - val_loss: 22.1335\n",
      "Epoch 348/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.6128 - val_loss: 22.8866\n",
      "Epoch 349/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.1046 - val_loss: 23.3636\n",
      "Epoch 350/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.8970 - val_loss: 22.9791\n",
      "Epoch 351/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.2571 - val_loss: 25.9059\n",
      "Epoch 352/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.7079 - val_loss: 22.3586\n",
      "Epoch 353/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.3793 - val_loss: 24.0679\n",
      "Epoch 354/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.7742 - val_loss: 23.5183\n",
      "Epoch 355/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.0040 - val_loss: 23.4711\n",
      "Epoch 356/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.8665 - val_loss: 22.1330\n",
      "Epoch 357/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.9722 - val_loss: 22.2030\n",
      "Epoch 358/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.1113 - val_loss: 22.5340\n",
      "Epoch 359/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.3222 - val_loss: 22.9543\n",
      "Epoch 360/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.8944 - val_loss: 22.8076\n",
      "Epoch 361/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.2477 - val_loss: 22.9830\n",
      "Epoch 362/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.6229 - val_loss: 23.7118\n",
      "Epoch 363/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.9057 - val_loss: 22.4901\n",
      "Epoch 364/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.3054 - val_loss: 22.8546\n",
      "Epoch 365/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.2827 - val_loss: 22.9880\n",
      "Epoch 366/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.1129 - val_loss: 23.5566\n",
      "Epoch 367/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.0113 - val_loss: 22.2063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 368/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.3655 - val_loss: 22.2726\n",
      "Epoch 369/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.8793 - val_loss: 23.2353\n",
      "Epoch 370/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.5923 - val_loss: 23.6277\n",
      "Epoch 371/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.8760 - val_loss: 22.2546\n",
      "Epoch 372/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.5380 - val_loss: 22.7698\n",
      "Epoch 373/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.1241 - val_loss: 22.3515\n",
      "Epoch 374/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.6428 - val_loss: 22.4277\n",
      "Epoch 375/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.8867 - val_loss: 23.5869\n",
      "Epoch 376/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 18.7558 - val_loss: 23.8141\n",
      "Epoch 377/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.7765 - val_loss: 22.6985\n",
      "Epoch 378/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.6517 - val_loss: 23.5928\n",
      "Epoch 379/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.9060 - val_loss: 21.9576\n",
      "Epoch 380/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.9372 - val_loss: 22.4167\n",
      "Epoch 381/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.6250 - val_loss: 22.7505\n",
      "Epoch 382/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.9285 - val_loss: 25.1841\n",
      "Epoch 383/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.6038 - val_loss: 22.5915\n",
      "Epoch 384/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.1309 - val_loss: 23.9267\n",
      "Epoch 385/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.8364 - val_loss: 23.8983\n",
      "Epoch 386/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.9828 - val_loss: 22.6258\n",
      "Epoch 387/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.9726 - val_loss: 23.7052\n",
      "Epoch 388/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.9882 - val_loss: 23.2176\n",
      "Epoch 389/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.6478 - val_loss: 23.4447\n",
      "Epoch 390/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.5114 - val_loss: 22.4868\n",
      "Epoch 391/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.2982 - val_loss: 23.7307\n",
      "Epoch 392/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.9952 - val_loss: 22.3659\n",
      "Epoch 393/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.7525 - val_loss: 22.2920\n",
      "Epoch 394/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.9468 - val_loss: 22.4414\n",
      "Epoch 395/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.5104 - val_loss: 22.3394\n",
      "Epoch 396/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.8072 - val_loss: 22.2478\n",
      "Epoch 397/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.6968 - val_loss: 23.5996\n",
      "Epoch 398/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.7408 - val_loss: 22.4314\n",
      "Epoch 399/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.3185 - val_loss: 23.3231\n",
      "Epoch 400/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.4011 - val_loss: 22.8641\n",
      "Epoch 401/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.3081 - val_loss: 22.7476\n",
      "Epoch 402/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.5553 - val_loss: 23.0807\n",
      "Epoch 403/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.2165 - val_loss: 22.5461\n",
      "Epoch 404/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.0325 - val_loss: 23.1163\n",
      "Epoch 405/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 20.0092 - val_loss: 22.9906\n",
      "Epoch 406/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.4384 - val_loss: 22.9598\n",
      "Epoch 407/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.9606 - val_loss: 22.8675\n",
      "Epoch 408/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.8213 - val_loss: 23.5057\n",
      "Epoch 409/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.9347 - val_loss: 22.5068\n",
      "Epoch 410/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.5914 - val_loss: 22.5027\n",
      "Epoch 411/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.5372 - val_loss: 22.2217\n",
      "Epoch 412/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.7806 - val_loss: 22.4514\n",
      "Epoch 413/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.7714 - val_loss: 24.4531\n",
      "Epoch 414/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.4118 - val_loss: 22.6405\n",
      "Epoch 415/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.3648 - val_loss: 23.0219\n",
      "Epoch 416/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.8200 - val_loss: 23.7483\n",
      "Epoch 417/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.6562 - val_loss: 22.8408\n",
      "Epoch 418/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.6702 - val_loss: 24.9037\n",
      "Epoch 419/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.7601 - val_loss: 22.6738\n",
      "Epoch 420/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.5671 - val_loss: 22.6138\n",
      "Epoch 421/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.7945 - val_loss: 22.6352\n",
      "Epoch 422/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.7403 - val_loss: 23.4537\n",
      "Epoch 423/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.6652 - val_loss: 22.6056\n",
      "Epoch 424/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.6824 - val_loss: 23.7108\n",
      "Epoch 425/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.2902 - val_loss: 22.6480\n",
      "Epoch 426/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.6443 - val_loss: 23.2621\n",
      "Epoch 427/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 19.9773 - val_loss: 22.4307\n",
      "Epoch 428/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.4787 - val_loss: 22.7865\n",
      "Epoch 429/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.7621 - val_loss: 22.9778\n",
      "Epoch 430/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.9964 - val_loss: 23.8596\n",
      "Epoch 431/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.7751 - val_loss: 23.8359\n",
      "Epoch 432/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.3683 - val_loss: 22.5274\n",
      "Epoch 433/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.2091 - val_loss: 23.6266\n",
      "Epoch 434/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.9852 - val_loss: 25.2384\n",
      "Epoch 435/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.0238 - val_loss: 23.2630\n",
      "Epoch 436/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.2934 - val_loss: 22.4213\n",
      "Epoch 437/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.7357 - val_loss: 21.8028\n",
      "Epoch 438/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.5745 - val_loss: 22.2337\n",
      "Epoch 439/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.3432 - val_loss: 22.6339\n",
      "Epoch 440/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.7289 - val_loss: 22.2630\n",
      "Epoch 441/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.0432 - val_loss: 23.3029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 442/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.4524 - val_loss: 22.8062\n",
      "Epoch 443/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.9307 - val_loss: 24.0132\n",
      "Epoch 444/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.1044 - val_loss: 23.3139\n",
      "Epoch 445/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 18.7779 - val_loss: 23.3461\n",
      "Epoch 446/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.4099 - val_loss: 22.3077\n",
      "Epoch 447/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.2996 - val_loss: 22.8040\n",
      "Epoch 448/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.3776 - val_loss: 24.0036\n",
      "Epoch 449/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.6864 - val_loss: 22.2478\n",
      "Epoch 450/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.4870 - val_loss: 25.0075\n",
      "Epoch 451/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.7088 - val_loss: 24.1814\n",
      "Epoch 452/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.8493 - val_loss: 22.3424\n",
      "Epoch 453/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 18.5020 - val_loss: 23.3007\n",
      "Epoch 454/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.9648 - val_loss: 23.0803\n",
      "Epoch 455/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.2446 - val_loss: 23.3232\n",
      "Epoch 456/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.2987 - val_loss: 22.6086\n",
      "Epoch 457/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.2446 - val_loss: 23.5115\n",
      "Epoch 458/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.3174 - val_loss: 22.8067\n",
      "Epoch 459/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.2050 - val_loss: 23.0068\n",
      "Epoch 460/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.3109 - val_loss: 23.8257\n",
      "Epoch 461/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.3600 - val_loss: 22.3124\n",
      "Epoch 462/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.3488 - val_loss: 23.4185\n",
      "Epoch 463/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.4730 - val_loss: 22.8199\n",
      "Epoch 464/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.3041 - val_loss: 22.6032\n",
      "Epoch 465/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.9913 - val_loss: 23.2976\n",
      "Epoch 466/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.0422 - val_loss: 24.8216\n",
      "Epoch 467/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.7081 - val_loss: 24.0584\n",
      "Epoch 468/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.3675 - val_loss: 22.1444\n",
      "Epoch 469/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.8583 - val_loss: 22.7366\n",
      "Epoch 470/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.2680 - val_loss: 23.0066\n",
      "Epoch 471/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.8771 - val_loss: 22.6779\n",
      "Epoch 472/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.7913 - val_loss: 23.1137\n",
      "Epoch 473/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.1345 - val_loss: 23.2123\n",
      "Epoch 474/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.7493 - val_loss: 21.9580\n",
      "Epoch 475/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.9269 - val_loss: 23.1091\n",
      "Epoch 476/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.9889 - val_loss: 23.9961\n",
      "Epoch 477/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.6846 - val_loss: 23.2564\n",
      "Epoch 478/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.2931 - val_loss: 22.7571\n",
      "Epoch 479/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.3291 - val_loss: 23.3968\n",
      "Epoch 480/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.3042 - val_loss: 22.3326\n",
      "Epoch 481/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.2047 - val_loss: 23.4460\n",
      "Epoch 482/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.0497 - val_loss: 23.2646\n",
      "Epoch 483/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.1351 - val_loss: 23.2087\n",
      "Epoch 484/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.1596 - val_loss: 22.2777\n",
      "Epoch 485/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.3280 - val_loss: 23.8024\n",
      "Epoch 486/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.6534 - val_loss: 23.1612\n",
      "Epoch 487/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.2110 - val_loss: 22.7208\n",
      "Epoch 488/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.5460 - val_loss: 22.7132\n",
      "Epoch 489/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.2532 - val_loss: 22.4180\n",
      "Epoch 490/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.7090 - val_loss: 21.8501\n",
      "Epoch 491/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.7314 - val_loss: 22.5131\n",
      "Epoch 492/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.9487 - val_loss: 22.9907\n",
      "Epoch 493/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.9042 - val_loss: 23.3349\n",
      "Epoch 494/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.0916 - val_loss: 23.9050\n",
      "Epoch 495/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.8274 - val_loss: 23.4566\n",
      "Epoch 496/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.8424 - val_loss: 22.3809\n",
      "Epoch 497/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.3385 - val_loss: 22.3043\n",
      "Epoch 498/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.0068 - val_loss: 22.0015\n",
      "Epoch 499/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.3435 - val_loss: 22.2602\n",
      "Epoch 500/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.5060 - val_loss: 23.4637\n",
      "Epoch 501/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.7941 - val_loss: 24.2330\n",
      "Epoch 502/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.3332 - val_loss: 21.9416\n",
      "Epoch 503/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.1385 - val_loss: 22.9681\n",
      "Epoch 504/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.2857 - val_loss: 23.8705\n",
      "Epoch 505/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.2860 - val_loss: 22.4741\n",
      "Epoch 506/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.0038 - val_loss: 22.1415\n",
      "Epoch 507/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.5626 - val_loss: 23.2180\n",
      "Epoch 508/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.4250 - val_loss: 22.2267\n",
      "Epoch 509/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.1909 - val_loss: 22.1972\n",
      "Epoch 510/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.8813 - val_loss: 22.8442\n",
      "Epoch 511/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.7673 - val_loss: 23.3311\n",
      "Epoch 512/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.3991 - val_loss: 23.8946\n",
      "Epoch 513/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.1745 - val_loss: 23.9171\n",
      "Epoch 514/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.3509 - val_loss: 24.8387\n",
      "Epoch 515/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.1988 - val_loss: 22.6354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 516/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.2742 - val_loss: 22.7270\n",
      "Epoch 517/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.1389 - val_loss: 22.7255\n",
      "Epoch 518/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.6936 - val_loss: 22.5488\n",
      "Epoch 519/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.9481 - val_loss: 22.7597\n",
      "Epoch 520/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.1227 - val_loss: 23.3115\n",
      "Epoch 521/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.7062 - val_loss: 22.7456\n",
      "Epoch 522/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.6504 - val_loss: 22.6912\n",
      "Epoch 523/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.9388 - val_loss: 22.8736\n",
      "Epoch 524/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.5631 - val_loss: 23.1947\n",
      "Epoch 525/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.1548 - val_loss: 23.3926\n",
      "Epoch 526/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.8726 - val_loss: 23.6292\n",
      "Epoch 527/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.5760 - val_loss: 23.1098\n",
      "Epoch 528/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.4619 - val_loss: 22.5690\n",
      "Epoch 529/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.8878 - val_loss: 22.9696\n",
      "Epoch 530/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.8448 - val_loss: 21.8502\n",
      "Epoch 531/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.1874 - val_loss: 23.7773\n",
      "Epoch 532/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.5939 - val_loss: 21.6755\n",
      "Epoch 533/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.3522 - val_loss: 22.7394\n",
      "Epoch 534/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.0021 - val_loss: 24.2860\n",
      "Epoch 535/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.4775 - val_loss: 22.8515\n",
      "Epoch 536/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.8737 - val_loss: 22.4762\n",
      "Epoch 537/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.7720 - val_loss: 22.5235\n",
      "Epoch 538/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.7612 - val_loss: 23.2530\n",
      "Epoch 539/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.5643 - val_loss: 25.4182\n",
      "Epoch 540/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.7588 - val_loss: 22.3492\n",
      "Epoch 541/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.3592 - val_loss: 22.4256\n",
      "Epoch 542/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.6682 - val_loss: 22.7720\n",
      "Epoch 543/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.3095 - val_loss: 24.4185\n",
      "Epoch 544/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.2667 - val_loss: 24.5015\n",
      "Epoch 545/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.3791 - val_loss: 22.6729\n",
      "Epoch 546/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.3244 - val_loss: 22.1693\n",
      "Epoch 547/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.7687 - val_loss: 22.4846\n",
      "Epoch 548/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.0897 - val_loss: 22.9603\n",
      "Epoch 549/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.8795 - val_loss: 22.8238\n",
      "Epoch 550/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.1715 - val_loss: 23.0742\n",
      "Epoch 551/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.3452 - val_loss: 23.1101\n",
      "Epoch 552/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.2470 - val_loss: 23.6857\n",
      "Epoch 553/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.9175 - val_loss: 23.3225\n",
      "Epoch 554/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.1270 - val_loss: 22.1506\n",
      "Epoch 555/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.0845 - val_loss: 22.3636\n",
      "Epoch 556/2000\n",
      "1500/1500 [==============================] - 0s 31us/sample - loss: 17.5501 - val_loss: 22.3406\n",
      "Epoch 557/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.4196 - val_loss: 22.0697\n",
      "Epoch 558/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.4308 - val_loss: 22.0372\n",
      "Epoch 559/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.3800 - val_loss: 23.2656\n",
      "Epoch 560/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.9320 - val_loss: 22.8303\n",
      "Epoch 561/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 18.0136 - val_loss: 23.2758\n",
      "Epoch 562/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.0378 - val_loss: 25.2458\n",
      "Epoch 563/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.3955 - val_loss: 22.7751\n",
      "Epoch 564/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.7225 - val_loss: 22.3613\n",
      "Epoch 565/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.0972 - val_loss: 22.9712\n",
      "Epoch 566/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.2528 - val_loss: 23.6266\n",
      "Epoch 567/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.5420 - val_loss: 22.6549\n",
      "Epoch 568/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.2301 - val_loss: 24.8898\n",
      "Epoch 569/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.1932 - val_loss: 22.7273\n",
      "Epoch 570/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.1169 - val_loss: 23.0065\n",
      "Epoch 571/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.8012 - val_loss: 22.3767\n",
      "Epoch 572/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.8065 - val_loss: 23.9257\n",
      "Epoch 573/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.4391 - val_loss: 22.6928\n",
      "Epoch 574/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.3818 - val_loss: 22.4550\n",
      "Epoch 575/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.0132 - val_loss: 22.9343\n",
      "Epoch 576/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.7189 - val_loss: 22.5664\n",
      "Epoch 577/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.9324 - val_loss: 23.1056\n",
      "Epoch 578/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.9026 - val_loss: 24.2855\n",
      "Epoch 579/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.8837 - val_loss: 22.4637\n",
      "Epoch 580/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.1503 - val_loss: 24.5719\n",
      "Epoch 581/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.1485 - val_loss: 23.4048\n",
      "Epoch 582/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.4163 - val_loss: 22.8995\n",
      "Epoch 583/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.3250 - val_loss: 22.6614\n",
      "Epoch 584/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.8852 - val_loss: 22.8142\n",
      "Epoch 585/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.0121 - val_loss: 22.8269\n",
      "Epoch 586/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.8636 - val_loss: 23.3735\n",
      "Epoch 587/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.4100 - val_loss: 24.5513\n",
      "Epoch 588/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.1857 - val_loss: 24.2278\n",
      "Epoch 589/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.0266 - val_loss: 22.9595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 590/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.6218 - val_loss: 22.2131\n",
      "Epoch 591/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.3436 - val_loss: 21.8834\n",
      "Epoch 592/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.4794 - val_loss: 22.7242\n",
      "Epoch 593/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.3169 - val_loss: 22.2911\n",
      "Epoch 594/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.4262 - val_loss: 22.9626\n",
      "Epoch 595/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.6651 - val_loss: 23.5235\n",
      "Epoch 596/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.2319 - val_loss: 22.6769\n",
      "Epoch 597/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.9262 - val_loss: 22.4093\n",
      "Epoch 598/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.6218 - val_loss: 26.6140\n",
      "Epoch 599/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.1267 - val_loss: 22.1417\n",
      "Epoch 600/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.0395 - val_loss: 23.2319\n",
      "Epoch 601/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.3317 - val_loss: 22.2720\n",
      "Epoch 602/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.3864 - val_loss: 22.7155\n",
      "Epoch 603/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.4761 - val_loss: 22.9983\n",
      "Epoch 604/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.7501 - val_loss: 22.7570\n",
      "Epoch 605/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.6441 - val_loss: 23.5513\n",
      "Epoch 606/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.5457 - val_loss: 22.4748\n",
      "Epoch 607/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.5493 - val_loss: 22.5381\n",
      "Epoch 608/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.3294 - val_loss: 22.5974\n",
      "Epoch 609/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.3490 - val_loss: 22.0558\n",
      "Epoch 610/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.2504 - val_loss: 22.5210\n",
      "Epoch 611/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.3889 - val_loss: 22.8847\n",
      "Epoch 612/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.5638 - val_loss: 22.3507\n",
      "Epoch 613/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.3612 - val_loss: 23.2843\n",
      "Epoch 614/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.3197 - val_loss: 22.8952\n",
      "Epoch 615/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.4484 - val_loss: 22.8256\n",
      "Epoch 616/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.5967 - val_loss: 23.2190\n",
      "Epoch 617/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.5414 - val_loss: 24.4548\n",
      "Epoch 618/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.2277 - val_loss: 22.5222\n",
      "Epoch 619/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.0337 - val_loss: 22.3521\n",
      "Epoch 620/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.0907 - val_loss: 23.7558\n",
      "Epoch 621/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.2354 - val_loss: 22.7159\n",
      "Epoch 622/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.7754 - val_loss: 22.0520\n",
      "Epoch 623/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.6514 - val_loss: 23.3698\n",
      "Epoch 624/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.1842 - val_loss: 22.8717\n",
      "Epoch 625/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.4567 - val_loss: 23.7907\n",
      "Epoch 626/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.7296 - val_loss: 23.1602\n",
      "Epoch 627/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.6593 - val_loss: 24.3737\n",
      "Epoch 628/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.2187 - val_loss: 21.8612\n",
      "Epoch 629/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.0716 - val_loss: 22.6633\n",
      "Epoch 630/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.2697 - val_loss: 22.1887\n",
      "Epoch 631/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.4191 - val_loss: 22.7540\n",
      "Epoch 632/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.8516 - val_loss: 22.5843\n",
      "Epoch 633/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 16.9908 - val_loss: 22.3443\n",
      "Epoch 634/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.0707 - val_loss: 22.7982\n",
      "Epoch 635/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.2397 - val_loss: 23.6982\n",
      "Epoch 636/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.4377 - val_loss: 22.5525\n",
      "Epoch 637/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.3240 - val_loss: 22.9066\n",
      "Epoch 638/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.4760 - val_loss: 22.4963\n",
      "Epoch 639/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.5602 - val_loss: 22.1170\n",
      "Epoch 640/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.2814 - val_loss: 22.9619\n",
      "Epoch 641/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.6662 - val_loss: 22.0631\n",
      "Epoch 642/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.1615 - val_loss: 24.8313\n",
      "Epoch 643/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.5344 - val_loss: 22.4349\n",
      "Epoch 644/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.5479 - val_loss: 25.5864\n",
      "Epoch 645/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.8484 - val_loss: 22.1057\n",
      "Epoch 646/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.3220 - val_loss: 23.3011\n",
      "Epoch 647/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.7631 - val_loss: 24.3440\n",
      "Epoch 648/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.6253 - val_loss: 21.9767\n",
      "Epoch 649/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.3937 - val_loss: 22.4070\n",
      "Epoch 650/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 16.8592 - val_loss: 23.2176\n",
      "Epoch 651/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.0944 - val_loss: 23.6445\n",
      "Epoch 652/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.2401 - val_loss: 22.1352\n",
      "Epoch 653/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.0354 - val_loss: 23.1831\n",
      "Epoch 654/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.4699 - val_loss: 22.5366\n",
      "Epoch 655/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.3141 - val_loss: 22.8414\n",
      "Epoch 656/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.2033 - val_loss: 22.7448\n",
      "Epoch 657/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.5525 - val_loss: 22.9262\n",
      "Epoch 658/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.3485 - val_loss: 23.0171\n",
      "Epoch 659/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.5767 - val_loss: 25.3842\n",
      "Epoch 660/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.9042 - val_loss: 22.9184\n",
      "Epoch 661/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.3274 - val_loss: 22.5581\n",
      "Epoch 662/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.2202 - val_loss: 22.1955\n",
      "Epoch 663/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.1904 - val_loss: 23.3954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 664/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.8235 - val_loss: 22.0864\n",
      "Epoch 665/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.7548 - val_loss: 22.1973\n",
      "Epoch 666/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.0982 - val_loss: 22.6393\n",
      "Epoch 667/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.1132 - val_loss: 23.4702\n",
      "Epoch 668/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.0822 - val_loss: 25.0386\n",
      "Epoch 669/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.5941 - val_loss: 23.3986\n",
      "Epoch 670/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 17.0920 - val_loss: 22.4725\n",
      "Epoch 671/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 16.8967 - val_loss: 22.3210\n",
      "Epoch 672/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.1207 - val_loss: 21.9614\n",
      "Epoch 673/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 16.7088 - val_loss: 22.3498\n",
      "Epoch 674/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 16.7929 - val_loss: 22.4258\n",
      "Epoch 675/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 16.9002 - val_loss: 22.4620\n",
      "Epoch 676/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.1203 - val_loss: 21.8064\n",
      "Epoch 677/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.7921 - val_loss: 22.6658\n",
      "Epoch 678/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.2821 - val_loss: 23.4183\n",
      "Epoch 679/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.3968 - val_loss: 22.3679\n",
      "Epoch 680/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.1123 - val_loss: 23.6007\n",
      "Epoch 681/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.0604 - val_loss: 22.1804\n",
      "Epoch 682/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.3310 - val_loss: 22.3161\n",
      "Epoch 683/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.5377 - val_loss: 23.2915\n",
      "Epoch 684/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.4025 - val_loss: 23.4390\n",
      "Epoch 685/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 16.9055 - val_loss: 23.1962\n",
      "Epoch 686/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.2337 - val_loss: 22.9997\n",
      "Epoch 687/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 16.6639 - val_loss: 22.5538\n",
      "Epoch 688/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.8235 - val_loss: 23.4137\n",
      "Epoch 689/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.4316 - val_loss: 21.9341\n",
      "Epoch 690/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 16.9550 - val_loss: 23.1471\n",
      "Epoch 691/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 16.8086 - val_loss: 22.2611\n",
      "Epoch 692/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 16.8612 - val_loss: 22.7672\n",
      "Epoch 693/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.3117 - val_loss: 23.9588\n",
      "Epoch 694/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.1010 - val_loss: 22.5598\n",
      "Epoch 695/2000\n",
      "1500/1500 [==============================] - 0s 21us/sample - loss: 17.1895 - val_loss: 23.4330\n",
      "Epoch 696/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.2505 - val_loss: 22.9618\n",
      "Epoch 697/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.7124 - val_loss: 23.4763\n",
      "Epoch 698/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.2187 - val_loss: 22.2290\n",
      "Epoch 699/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 18.0426 - val_loss: 23.3533\n",
      "Epoch 700/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.6971 - val_loss: 22.7230\n",
      "Epoch 701/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.1204 - val_loss: 23.7076\n",
      "Epoch 702/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.0014 - val_loss: 22.2542\n",
      "Epoch 703/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 16.9929 - val_loss: 22.9919\n",
      "Epoch 704/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 16.9107 - val_loss: 22.8249\n",
      "Epoch 705/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.3865 - val_loss: 23.5736\n",
      "Epoch 706/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.6035 - val_loss: 23.9399\n",
      "Epoch 707/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 16.9219 - val_loss: 22.2878\n",
      "Epoch 708/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 16.9277 - val_loss: 23.1436\n",
      "Epoch 709/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 16.6559 - val_loss: 22.7032\n",
      "Epoch 710/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.1695 - val_loss: 22.4897\n",
      "Epoch 711/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.2919 - val_loss: 23.3378\n",
      "Epoch 712/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.0465 - val_loss: 22.2644\n",
      "Epoch 713/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.1007 - val_loss: 22.4151\n",
      "Epoch 714/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 16.8675 - val_loss: 22.7194\n",
      "Epoch 715/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 16.8642 - val_loss: 22.4410\n",
      "Epoch 716/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 16.6074 - val_loss: 22.7028\n",
      "Epoch 717/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 16.7412 - val_loss: 22.3758\n",
      "Epoch 718/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.0327 - val_loss: 22.1889\n",
      "Epoch 719/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.2219 - val_loss: 23.3080\n",
      "Epoch 720/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.2802 - val_loss: 24.4508\n",
      "Epoch 721/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.5264 - val_loss: 24.4279\n",
      "Epoch 722/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.9110 - val_loss: 23.2697\n",
      "Epoch 723/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.3450 - val_loss: 22.8267\n",
      "Epoch 724/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 17.4723 - val_loss: 23.3077\n",
      "Epoch 725/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 17.5262 - val_loss: 22.9094\n",
      "Epoch 726/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.5003 - val_loss: 22.2145\n",
      "Epoch 727/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 16.9175 - val_loss: 22.5196\n",
      "Epoch 728/2000\n",
      "1500/1500 [==============================] - 0s 22us/sample - loss: 16.6606 - val_loss: 22.4871\n",
      "Epoch 729/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.1359 - val_loss: 23.5103\n",
      "Epoch 730/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 16.9049 - val_loss: 22.7520\n",
      "Epoch 731/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.1408 - val_loss: 22.1863\n",
      "Epoch 732/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 16.5602 - val_loss: 22.2792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 81%|██████████████████████████████████████████████████████████████████▋               | 13/16 [07:02<01:27, 29.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'batc_normalization': False, 'batch_size': 64, 'dropout': 0, 'epochs': 2000, 'first_neuron': 320, 'hidden_layers': 1, 'hidden_neuron': 100, 'kernel_initializer': 'normal', 'last_activation': 'linear', 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "Train on 1500 samples, validate on 375 samples\n",
      "Epoch 1/2000\n",
      "1500/1500 [==============================] - 0s 224us/sample - loss: 2184.4004 - val_loss: 2181.6233\n",
      "Epoch 2/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 2171.0076 - val_loss: 2154.1733\n",
      "Epoch 3/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 2118.4810 - val_loss: 2064.1225\n",
      "Epoch 4/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 1975.9433 - val_loss: 1852.6413\n",
      "Epoch 5/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 1680.2287 - val_loss: 1449.2811\n",
      "Epoch 6/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 1171.0663 - val_loss: 862.7110\n",
      "Epoch 7/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 714.6975 - val_loss: 598.0641\n",
      "Epoch 8/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 591.0629 - val_loss: 529.0940\n",
      "Epoch 9/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 527.6883 - val_loss: 474.4076\n",
      "Epoch 10/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 468.9616 - val_loss: 420.5649\n",
      "Epoch 11/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 410.6736 - val_loss: 366.7221\n",
      "Epoch 12/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 353.9112 - val_loss: 315.4108\n",
      "Epoch 13/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 295.6993 - val_loss: 259.0010\n",
      "Epoch 14/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 239.5876 - val_loss: 209.3811\n",
      "Epoch 15/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 192.0845 - val_loss: 167.3559\n",
      "Epoch 16/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 151.8451 - val_loss: 134.2781\n",
      "Epoch 17/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 123.4022 - val_loss: 111.5202\n",
      "Epoch 18/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 100.4285 - val_loss: 91.6248\n",
      "Epoch 19/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 82.8237 - val_loss: 76.6122\n",
      "Epoch 20/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 69.7657 - val_loss: 67.2943\n",
      "Epoch 21/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 59.5606 - val_loss: 55.9839\n",
      "Epoch 22/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 51.0651 - val_loss: 50.7878\n",
      "Epoch 23/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 45.0974 - val_loss: 45.8688\n",
      "Epoch 24/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 41.6637 - val_loss: 44.2986\n",
      "Epoch 25/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 37.4745 - val_loss: 39.0306\n",
      "Epoch 26/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 35.0828 - val_loss: 36.3910\n",
      "Epoch 27/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 34.8227 - val_loss: 36.5684\n",
      "Epoch 28/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 31.5444 - val_loss: 32.6334\n",
      "Epoch 29/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 29.9949 - val_loss: 32.2976\n",
      "Epoch 30/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 29.1401 - val_loss: 33.3007\n",
      "Epoch 31/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 28.0158 - val_loss: 29.6464\n",
      "Epoch 32/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 27.1507 - val_loss: 28.0524\n",
      "Epoch 33/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 26.6058 - val_loss: 28.4553\n",
      "Epoch 34/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 26.3853 - val_loss: 27.0535\n",
      "Epoch 35/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 25.5446 - val_loss: 25.8961\n",
      "Epoch 36/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 24.5999 - val_loss: 26.9345\n",
      "Epoch 37/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 24.6327 - val_loss: 25.5896\n",
      "Epoch 38/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 23.5978 - val_loss: 27.3941\n",
      "Epoch 39/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 23.8865 - val_loss: 24.5235\n",
      "Epoch 40/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 23.1534 - val_loss: 24.7059\n",
      "Epoch 41/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 23.1586 - val_loss: 25.2711\n",
      "Epoch 42/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 23.3098 - val_loss: 24.8129\n",
      "Epoch 43/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 24.4179 - val_loss: 24.3398\n",
      "Epoch 44/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 23.6266 - val_loss: 25.8149\n",
      "Epoch 45/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 23.1216 - val_loss: 24.2723\n",
      "Epoch 46/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 23.2040 - val_loss: 25.4988\n",
      "Epoch 47/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 22.9055 - val_loss: 23.7754\n",
      "Epoch 48/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 22.6583 - val_loss: 24.9710\n",
      "Epoch 49/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 22.3080 - val_loss: 23.4477\n",
      "Epoch 50/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 22.8155 - val_loss: 25.1402\n",
      "Epoch 51/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 22.8051 - val_loss: 24.6466\n",
      "Epoch 52/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 23.3234 - val_loss: 25.2123\n",
      "Epoch 53/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 22.9813 - val_loss: 26.3886\n",
      "Epoch 54/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 22.2574 - val_loss: 24.2295\n",
      "Epoch 55/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 22.2627 - val_loss: 25.0413\n",
      "Epoch 56/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 22.4205 - val_loss: 25.0612\n",
      "Epoch 57/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 21.7530 - val_loss: 24.7198\n",
      "Epoch 58/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 21.9200 - val_loss: 25.8250\n",
      "Epoch 59/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 21.7132 - val_loss: 25.4834\n",
      "Epoch 60/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 24.4774 - val_loss: 28.8323\n",
      "Epoch 61/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 22.5689 - val_loss: 24.1761\n",
      "Epoch 62/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 22.1162 - val_loss: 24.8186\n",
      "Epoch 63/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 21.8145 - val_loss: 23.5360\n",
      "Epoch 64/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 21.7707 - val_loss: 25.1252\n",
      "Epoch 65/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 21.8373 - val_loss: 24.2344\n",
      "Epoch 66/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.7630 - val_loss: 24.1513\n",
      "Epoch 67/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 22.1133 - val_loss: 24.6229\n",
      "Epoch 68/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 22.2453 - val_loss: 23.4676\n",
      "Epoch 69/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 22.6507 - val_loss: 25.3083\n",
      "Epoch 70/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 22.3340 - val_loss: 23.9115\n",
      "Epoch 71/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.8436 - val_loss: 23.1090\n",
      "Epoch 72/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.3326 - val_loss: 24.7850\n",
      "Epoch 73/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 22.5600 - val_loss: 25.4239\n",
      "Epoch 74/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 22.3320 - val_loss: 25.3900\n",
      "Epoch 75/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.8611 - val_loss: 24.4333\n",
      "Epoch 76/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.3564 - val_loss: 24.8002\n",
      "Epoch 77/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.5309 - val_loss: 23.1968\n",
      "Epoch 78/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.0792 - val_loss: 23.3768\n",
      "Epoch 79/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.8233 - val_loss: 24.0000\n",
      "Epoch 80/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.5340 - val_loss: 24.4404\n",
      "Epoch 81/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 21.1215 - val_loss: 23.3440\n",
      "Epoch 82/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 21.7688 - val_loss: 23.7067\n",
      "Epoch 83/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 22.2699 - val_loss: 24.4333\n",
      "Epoch 84/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 22.4086 - val_loss: 23.4973\n",
      "Epoch 85/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.7133 - val_loss: 24.5336\n",
      "Epoch 86/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 20.7780 - val_loss: 28.9976\n",
      "Epoch 87/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 21.7421 - val_loss: 22.8036\n",
      "Epoch 88/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.0352 - val_loss: 25.9734\n",
      "Epoch 89/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.4159 - val_loss: 23.7957\n",
      "Epoch 90/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.8537 - val_loss: 24.2761\n",
      "Epoch 91/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.1699 - val_loss: 22.9475\n",
      "Epoch 92/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.9352 - val_loss: 24.0260\n",
      "Epoch 93/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 21.2663 - val_loss: 24.3306\n",
      "Epoch 94/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 20.8228 - val_loss: 25.2533\n",
      "Epoch 95/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.8163 - val_loss: 23.7448\n",
      "Epoch 96/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 20.9657 - val_loss: 24.4350\n",
      "Epoch 97/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 21.5563 - val_loss: 26.2813\n",
      "Epoch 98/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 21.6613 - val_loss: 23.5419\n",
      "Epoch 99/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 21.0210 - val_loss: 24.4889\n",
      "Epoch 100/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.4319 - val_loss: 24.4450\n",
      "Epoch 101/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.9783 - val_loss: 23.7197\n",
      "Epoch 102/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.9579 - val_loss: 24.9622\n",
      "Epoch 103/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.0524 - val_loss: 25.7048\n",
      "Epoch 104/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.6024 - val_loss: 24.4180\n",
      "Epoch 105/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.6739 - val_loss: 25.4500\n",
      "Epoch 106/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.7575 - val_loss: 23.2194\n",
      "Epoch 107/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.8389 - val_loss: 23.7667\n",
      "Epoch 108/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 20.2084 - val_loss: 23.6180\n",
      "Epoch 109/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.6749 - val_loss: 22.9867\n",
      "Epoch 110/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 21.3743 - val_loss: 24.7132\n",
      "Epoch 111/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.8787 - val_loss: 24.0171\n",
      "Epoch 112/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.6354 - val_loss: 26.8784\n",
      "Epoch 113/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 21.8198 - val_loss: 25.2678\n",
      "Epoch 114/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.9818 - val_loss: 23.4519\n",
      "Epoch 115/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.1136 - val_loss: 24.3537\n",
      "Epoch 116/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.6044 - val_loss: 24.5172\n",
      "Epoch 117/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 21.4480 - val_loss: 22.8211\n",
      "Epoch 118/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 21.5852 - val_loss: 23.2469\n",
      "Epoch 119/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.4638 - val_loss: 24.0139\n",
      "Epoch 120/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.2459 - val_loss: 25.0758\n",
      "Epoch 121/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.6032 - val_loss: 24.4114\n",
      "Epoch 122/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.5932 - val_loss: 23.7938\n",
      "Epoch 123/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 20.5832 - val_loss: 24.2282\n",
      "Epoch 124/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.6830 - val_loss: 24.0617\n",
      "Epoch 125/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.7571 - val_loss: 23.2378\n",
      "Epoch 126/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.6050 - val_loss: 23.0988\n",
      "Epoch 127/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 21.0219 - val_loss: 22.7617\n",
      "Epoch 128/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.5458 - val_loss: 23.1109\n",
      "Epoch 129/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.5971 - val_loss: 25.5509\n",
      "Epoch 130/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 21.6404 - val_loss: 23.7955\n",
      "Epoch 131/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.9262 - val_loss: 26.3339\n",
      "Epoch 132/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.2374 - val_loss: 25.2758\n",
      "Epoch 133/2000\n",
      "1500/1500 [==============================] - 0s 31us/sample - loss: 20.9190 - val_loss: 26.5314\n",
      "Epoch 134/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 21.2022 - val_loss: 22.7005\n",
      "Epoch 135/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 21.0164 - val_loss: 23.0608\n",
      "Epoch 136/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 21.1793 - val_loss: 22.5454\n",
      "Epoch 137/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.2100 - val_loss: 24.7693\n",
      "Epoch 138/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 21.5380 - val_loss: 23.5776\n",
      "Epoch 139/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.6477 - val_loss: 23.1604\n",
      "Epoch 140/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.4749 - val_loss: 24.1914\n",
      "Epoch 141/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 20.3302 - val_loss: 24.8731\n",
      "Epoch 142/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.0166 - val_loss: 23.4228\n",
      "Epoch 143/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 20.8677 - val_loss: 23.2748\n",
      "Epoch 144/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 20.6287 - val_loss: 24.3698\n",
      "Epoch 145/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 20.1110 - val_loss: 23.1917\n",
      "Epoch 146/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.6914 - val_loss: 23.7737\n",
      "Epoch 147/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.9768 - val_loss: 22.7027\n",
      "Epoch 148/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.5693 - val_loss: 23.9642\n",
      "Epoch 149/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.4781 - val_loss: 23.6106\n",
      "Epoch 150/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.1559 - val_loss: 24.2704\n",
      "Epoch 151/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.5077 - val_loss: 24.7737\n",
      "Epoch 152/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.7175 - val_loss: 23.4233\n",
      "Epoch 153/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.8069 - val_loss: 23.7587\n",
      "Epoch 154/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.8988 - val_loss: 23.9906\n",
      "Epoch 155/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.0377 - val_loss: 24.1150\n",
      "Epoch 156/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.6277 - val_loss: 22.5925\n",
      "Epoch 157/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.5686 - val_loss: 24.0210\n",
      "Epoch 158/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.8082 - val_loss: 24.2227\n",
      "Epoch 159/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 20.7977 - val_loss: 23.9206\n",
      "Epoch 160/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.8702 - val_loss: 23.1647\n",
      "Epoch 161/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.3516 - val_loss: 23.1030\n",
      "Epoch 162/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.7702 - val_loss: 23.5894\n",
      "Epoch 163/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.1050 - val_loss: 23.4301\n",
      "Epoch 164/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.1817 - val_loss: 23.2167\n",
      "Epoch 165/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.7695 - val_loss: 25.6557\n",
      "Epoch 166/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 20.4756 - val_loss: 24.6088\n",
      "Epoch 167/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.7442 - val_loss: 24.0240\n",
      "Epoch 168/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.2681 - val_loss: 23.5569\n",
      "Epoch 169/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 21.1544 - val_loss: 24.0707\n",
      "Epoch 170/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.6114 - val_loss: 25.1740\n",
      "Epoch 171/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.2182 - val_loss: 23.3167\n",
      "Epoch 172/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.2165 - val_loss: 23.7007\n",
      "Epoch 173/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.4772 - val_loss: 23.1019\n",
      "Epoch 174/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.5008 - val_loss: 24.5017\n",
      "Epoch 175/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.4547 - val_loss: 25.1013\n",
      "Epoch 176/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.5259 - val_loss: 24.7215\n",
      "Epoch 177/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.8276 - val_loss: 24.2188\n",
      "Epoch 178/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.4932 - val_loss: 25.1367\n",
      "Epoch 179/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.2666 - val_loss: 22.4930\n",
      "Epoch 180/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.4137 - val_loss: 24.9738\n",
      "Epoch 181/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.1007 - val_loss: 23.3562\n",
      "Epoch 182/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.5772 - val_loss: 23.7721\n",
      "Epoch 183/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.7025 - val_loss: 22.5908\n",
      "Epoch 184/2000\n",
      "1500/1500 [==============================] - 0s 31us/sample - loss: 19.7189 - val_loss: 23.5817\n",
      "Epoch 185/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.6967 - val_loss: 23.2723\n",
      "Epoch 186/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.2197 - val_loss: 24.2544\n",
      "Epoch 187/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 20.3095 - val_loss: 24.0928\n",
      "Epoch 188/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.0740 - val_loss: 23.2614\n",
      "Epoch 189/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.3295 - val_loss: 23.4081\n",
      "Epoch 190/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.2075 - val_loss: 24.0586\n",
      "Epoch 191/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.8667 - val_loss: 23.8673\n",
      "Epoch 192/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.3888 - val_loss: 24.2439\n",
      "Epoch 193/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.6920 - val_loss: 24.6498\n",
      "Epoch 194/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.5913 - val_loss: 23.7207\n",
      "Epoch 195/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.7888 - val_loss: 22.9784\n",
      "Epoch 196/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.3049 - val_loss: 23.5665\n",
      "Epoch 197/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.2471 - val_loss: 22.6885\n",
      "Epoch 198/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.8188 - val_loss: 26.1517\n",
      "Epoch 199/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.9797 - val_loss: 24.2981\n",
      "Epoch 200/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.4294 - val_loss: 24.7387\n",
      "Epoch 201/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.5061 - val_loss: 23.8824\n",
      "Epoch 202/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.5966 - val_loss: 24.4079\n",
      "Epoch 203/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.1304 - val_loss: 22.9169\n",
      "Epoch 204/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.6123 - val_loss: 23.1635\n",
      "Epoch 205/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.6928 - val_loss: 25.6278\n",
      "Epoch 206/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.2456 - val_loss: 26.8917\n",
      "Epoch 207/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.1060 - val_loss: 24.1454\n",
      "Epoch 208/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.7457 - val_loss: 23.5948\n",
      "Epoch 209/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.3976 - val_loss: 23.6446\n",
      "Epoch 210/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.3376 - val_loss: 25.2984\n",
      "Epoch 211/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.4336 - val_loss: 23.4980\n",
      "Epoch 212/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.7858 - val_loss: 24.5289\n",
      "Epoch 213/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.7225 - val_loss: 23.0193\n",
      "Epoch 214/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.7563 - val_loss: 26.2844\n",
      "Epoch 215/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.7777 - val_loss: 25.0460\n",
      "Epoch 216/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.7975 - val_loss: 26.3316\n",
      "Epoch 217/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.1247 - val_loss: 23.4761\n",
      "Epoch 218/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.9557 - val_loss: 28.6159\n",
      "Epoch 219/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.6981 - val_loss: 22.5825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 220/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.5923 - val_loss: 22.3609\n",
      "Epoch 221/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.8750 - val_loss: 22.3705\n",
      "Epoch 222/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.3820 - val_loss: 24.0028\n",
      "Epoch 223/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.4204 - val_loss: 23.0709\n",
      "Epoch 224/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.6512 - val_loss: 23.1322\n",
      "Epoch 225/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.9814 - val_loss: 22.4962\n",
      "Epoch 226/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.1068 - val_loss: 24.0888\n",
      "Epoch 227/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.0178 - val_loss: 22.2625\n",
      "Epoch 228/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.2854 - val_loss: 22.7056\n",
      "Epoch 229/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.4044 - val_loss: 22.4018\n",
      "Epoch 230/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.2018 - val_loss: 22.9609\n",
      "Epoch 231/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.4046 - val_loss: 22.1101\n",
      "Epoch 232/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.4390 - val_loss: 22.1725\n",
      "Epoch 233/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.5755 - val_loss: 22.7882\n",
      "Epoch 234/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.0710 - val_loss: 24.3624\n",
      "Epoch 235/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 19.0652 - val_loss: 23.2676\n",
      "Epoch 236/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.2345 - val_loss: 23.2854\n",
      "Epoch 237/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.5282 - val_loss: 22.8138\n",
      "Epoch 238/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.9446 - val_loss: 22.7104\n",
      "Epoch 239/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.3852 - val_loss: 23.6056\n",
      "Epoch 240/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.7412 - val_loss: 24.2855\n",
      "Epoch 241/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.7135 - val_loss: 24.8388\n",
      "Epoch 242/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.4265 - val_loss: 22.9699\n",
      "Epoch 243/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.5276 - val_loss: 23.8256\n",
      "Epoch 244/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.9311 - val_loss: 23.4978\n",
      "Epoch 245/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.8539 - val_loss: 23.9298\n",
      "Epoch 246/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.9066 - val_loss: 23.8656\n",
      "Epoch 247/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.0588 - val_loss: 22.9736\n",
      "Epoch 248/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.9026 - val_loss: 22.4098\n",
      "Epoch 249/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.5027 - val_loss: 23.7653\n",
      "Epoch 250/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.5011 - val_loss: 24.5829\n",
      "Epoch 251/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.8525 - val_loss: 23.3077\n",
      "Epoch 252/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.9219 - val_loss: 22.9524\n",
      "Epoch 253/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.7809 - val_loss: 25.1647\n",
      "Epoch 254/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.5132 - val_loss: 25.4751\n",
      "Epoch 255/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.1935 - val_loss: 23.0207\n",
      "Epoch 256/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.0665 - val_loss: 23.7971\n",
      "Epoch 257/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.9873 - val_loss: 24.1599\n",
      "Epoch 258/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.4758 - val_loss: 23.4902\n",
      "Epoch 259/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.9342 - val_loss: 24.4959\n",
      "Epoch 260/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.8162 - val_loss: 25.6823\n",
      "Epoch 261/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 20.0657 - val_loss: 22.5443\n",
      "Epoch 262/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.7287 - val_loss: 25.2689\n",
      "Epoch 263/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.4674 - val_loss: 23.3274\n",
      "Epoch 264/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.5653 - val_loss: 24.5185\n",
      "Epoch 265/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.3589 - val_loss: 25.4533\n",
      "Epoch 266/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.6013 - val_loss: 25.1742\n",
      "Epoch 267/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.8980 - val_loss: 22.8381\n",
      "Epoch 268/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.0497 - val_loss: 23.5149\n",
      "Epoch 269/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.2620 - val_loss: 22.6469\n",
      "Epoch 270/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.3614 - val_loss: 24.9649\n",
      "Epoch 271/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.8534 - val_loss: 22.9463\n",
      "Epoch 272/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.0563 - val_loss: 23.0756\n",
      "Epoch 273/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.5408 - val_loss: 23.4834\n",
      "Epoch 274/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.4911 - val_loss: 24.0528\n",
      "Epoch 275/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.5247 - val_loss: 23.1498\n",
      "Epoch 276/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.6867 - val_loss: 22.5852\n",
      "Epoch 277/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.9367 - val_loss: 24.6058\n",
      "Epoch 278/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.8182 - val_loss: 24.4359\n",
      "Epoch 279/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.3889 - val_loss: 23.6267\n",
      "Epoch 280/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.6777 - val_loss: 24.8155\n",
      "Epoch 281/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.8922 - val_loss: 22.1457\n",
      "Epoch 282/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.8480 - val_loss: 23.5217\n",
      "Epoch 283/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.0461 - val_loss: 23.4510\n",
      "Epoch 284/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.1470 - val_loss: 24.1643\n",
      "Epoch 285/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.0076 - val_loss: 24.2633\n",
      "Epoch 286/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 19.6337 - val_loss: 22.7291\n",
      "Epoch 287/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.9249 - val_loss: 24.5099\n",
      "Epoch 288/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.7200 - val_loss: 23.2263\n",
      "Epoch 289/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.2934 - val_loss: 22.9192\n",
      "Epoch 290/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.6751 - val_loss: 24.9290\n",
      "Epoch 291/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.2863 - val_loss: 24.0575\n",
      "Epoch 292/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.1337 - val_loss: 24.0545\n",
      "Epoch 293/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.8901 - val_loss: 22.9329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 294/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.6880 - val_loss: 23.4643\n",
      "Epoch 295/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.9394 - val_loss: 23.7536\n",
      "Epoch 296/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.0594 - val_loss: 24.7571\n",
      "Epoch 297/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.7526 - val_loss: 23.2387\n",
      "Epoch 298/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 19.0555 - val_loss: 23.1117\n",
      "Epoch 299/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.6066 - val_loss: 23.5015\n",
      "Epoch 300/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.0751 - val_loss: 23.5434\n",
      "Epoch 301/2000\n",
      "1500/1500 [==============================] - 0s 31us/sample - loss: 18.7164 - val_loss: 22.9169\n",
      "Epoch 302/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.4888 - val_loss: 23.7993\n",
      "Epoch 303/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.8007 - val_loss: 24.7609\n",
      "Epoch 304/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.4241 - val_loss: 23.5997\n",
      "Epoch 305/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.7151 - val_loss: 23.8007\n",
      "Epoch 306/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.4036 - val_loss: 22.5763\n",
      "Epoch 307/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.6092 - val_loss: 22.8173\n",
      "Epoch 308/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.3910 - val_loss: 23.9352\n",
      "Epoch 309/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.7942 - val_loss: 24.8495\n",
      "Epoch 310/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.3949 - val_loss: 22.7282\n",
      "Epoch 311/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.2567 - val_loss: 22.6601\n",
      "Epoch 312/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.9105 - val_loss: 22.6742\n",
      "Epoch 313/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.5261 - val_loss: 22.5715\n",
      "Epoch 314/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.5874 - val_loss: 22.8370\n",
      "Epoch 315/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.0285 - val_loss: 25.1681\n",
      "Epoch 316/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.2650 - val_loss: 24.5685\n",
      "Epoch 317/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.5358 - val_loss: 22.7871\n",
      "Epoch 318/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.9939 - val_loss: 23.0183\n",
      "Epoch 319/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.1430 - val_loss: 22.4477\n",
      "Epoch 320/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.2457 - val_loss: 23.3306\n",
      "Epoch 321/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.0653 - val_loss: 22.8435\n",
      "Epoch 322/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.8804 - val_loss: 22.5029\n",
      "Epoch 323/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.6046 - val_loss: 23.8729\n",
      "Epoch 324/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.4690 - val_loss: 23.2620\n",
      "Epoch 325/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.9868 - val_loss: 23.1093\n",
      "Epoch 326/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.6397 - val_loss: 24.2575\n",
      "Epoch 327/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.6486 - val_loss: 22.9078\n",
      "Epoch 328/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.6260 - val_loss: 23.0240\n",
      "Epoch 329/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.6697 - val_loss: 22.2111\n",
      "Epoch 330/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.7736 - val_loss: 22.4729\n",
      "Epoch 331/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.7040 - val_loss: 25.3652\n",
      "Epoch 332/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.4216 - val_loss: 22.4800\n",
      "Epoch 333/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.3185 - val_loss: 22.6914\n",
      "Epoch 334/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.4868 - val_loss: 22.7960\n",
      "Epoch 335/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.4099 - val_loss: 23.3221\n",
      "Epoch 336/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.8038 - val_loss: 23.8162\n",
      "Epoch 337/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 19.5000 - val_loss: 23.9841\n",
      "Epoch 338/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.4998 - val_loss: 22.4551\n",
      "Epoch 339/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.3824 - val_loss: 23.6287\n",
      "Epoch 340/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.5462 - val_loss: 22.6142\n",
      "Epoch 341/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.8861 - val_loss: 22.4839\n",
      "Epoch 342/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.7564 - val_loss: 22.4761\n",
      "Epoch 343/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 19.0581 - val_loss: 23.7575\n",
      "Epoch 344/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.4377 - val_loss: 23.0703\n",
      "Epoch 345/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.2064 - val_loss: 22.8363\n",
      "Epoch 346/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.6653 - val_loss: 24.0700\n",
      "Epoch 347/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.1516 - val_loss: 24.8992\n",
      "Epoch 348/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.0404 - val_loss: 23.4459\n",
      "Epoch 349/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.5777 - val_loss: 23.4091\n",
      "Epoch 350/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.9094 - val_loss: 23.9276\n",
      "Epoch 351/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.4073 - val_loss: 23.4699\n",
      "Epoch 352/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.6762 - val_loss: 23.1944\n",
      "Epoch 353/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.8012 - val_loss: 23.0500\n",
      "Epoch 354/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.5036 - val_loss: 23.9456\n",
      "Epoch 355/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.6749 - val_loss: 23.5580\n",
      "Epoch 356/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.2666 - val_loss: 24.5964\n",
      "Epoch 357/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.5596 - val_loss: 23.7161\n",
      "Epoch 358/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.4419 - val_loss: 23.3331\n",
      "Epoch 359/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.6896 - val_loss: 23.5174\n",
      "Epoch 360/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.1939 - val_loss: 22.8283\n",
      "Epoch 361/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.2309 - val_loss: 24.5194\n",
      "Epoch 362/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.7818 - val_loss: 22.9592\n",
      "Epoch 363/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.1445 - val_loss: 22.3550\n",
      "Epoch 364/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.1475 - val_loss: 22.3355\n",
      "Epoch 365/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.5881 - val_loss: 23.8690\n",
      "Epoch 366/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.0718 - val_loss: 23.0375\n",
      "Epoch 367/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.3476 - val_loss: 25.1289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 368/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.7868 - val_loss: 22.5595\n",
      "Epoch 369/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.9225 - val_loss: 22.9942\n",
      "Epoch 370/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.4620 - val_loss: 23.2854\n",
      "Epoch 371/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.6751 - val_loss: 23.9797\n",
      "Epoch 372/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.5513 - val_loss: 22.6864\n",
      "Epoch 373/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.4536 - val_loss: 24.6374\n",
      "Epoch 374/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.6227 - val_loss: 22.8119\n",
      "Epoch 375/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.0998 - val_loss: 21.8399\n",
      "Epoch 376/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.4980 - val_loss: 22.8851\n",
      "Epoch 377/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.8408 - val_loss: 24.3382\n",
      "Epoch 378/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.6677 - val_loss: 24.3761\n",
      "Epoch 379/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.8541 - val_loss: 22.5061\n",
      "Epoch 380/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.0451 - val_loss: 24.1237\n",
      "Epoch 381/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.0357 - val_loss: 22.8993\n",
      "Epoch 382/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.1261 - val_loss: 24.0718\n",
      "Epoch 383/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.7531 - val_loss: 23.7909\n",
      "Epoch 384/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.3169 - val_loss: 23.5359\n",
      "Epoch 385/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.1440 - val_loss: 23.0011\n",
      "Epoch 386/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.3366 - val_loss: 25.7546\n",
      "Epoch 387/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.1107 - val_loss: 22.7577\n",
      "Epoch 388/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 18.4695 - val_loss: 23.1931\n",
      "Epoch 389/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.1440 - val_loss: 23.1057\n",
      "Epoch 390/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.4383 - val_loss: 23.9069\n",
      "Epoch 391/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.7440 - val_loss: 23.2501\n",
      "Epoch 392/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.8716 - val_loss: 22.7468\n",
      "Epoch 393/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.3894 - val_loss: 22.5215\n",
      "Epoch 394/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.7084 - val_loss: 23.6040\n",
      "Epoch 395/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.6151 - val_loss: 23.5688\n",
      "Epoch 396/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.1699 - val_loss: 23.3184\n",
      "Epoch 397/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.3418 - val_loss: 24.2169\n",
      "Epoch 398/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.6858 - val_loss: 24.7538\n",
      "Epoch 399/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.5878 - val_loss: 24.2745\n",
      "Epoch 400/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.0735 - val_loss: 22.4966\n",
      "Epoch 401/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.2211 - val_loss: 23.8596\n",
      "Epoch 402/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.6568 - val_loss: 22.8482\n",
      "Epoch 403/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.1867 - val_loss: 22.4493\n",
      "Epoch 404/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.0311 - val_loss: 23.0958\n",
      "Epoch 405/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.4508 - val_loss: 25.3482\n",
      "Epoch 406/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.2753 - val_loss: 22.9623\n",
      "Epoch 407/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.6052 - val_loss: 23.4334\n",
      "Epoch 408/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.5060 - val_loss: 24.2460\n",
      "Epoch 409/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.6760 - val_loss: 22.9848\n",
      "Epoch 410/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.9455 - val_loss: 25.9240\n",
      "Epoch 411/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.3067 - val_loss: 23.7528\n",
      "Epoch 412/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.9715 - val_loss: 23.2446\n",
      "Epoch 413/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.6457 - val_loss: 22.6361\n",
      "Epoch 414/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 17.8718 - val_loss: 23.5944\n",
      "Epoch 415/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.0443 - val_loss: 22.5386\n",
      "Epoch 416/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.4159 - val_loss: 23.4563\n",
      "Epoch 417/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.9122 - val_loss: 23.4021\n",
      "Epoch 418/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.9547 - val_loss: 22.8314\n",
      "Epoch 419/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.5468 - val_loss: 22.4765\n",
      "Epoch 420/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.9621 - val_loss: 23.1072\n",
      "Epoch 421/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.3270 - val_loss: 23.4478\n",
      "Epoch 422/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.1238 - val_loss: 23.1548\n",
      "Epoch 423/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.7494 - val_loss: 22.4731\n",
      "Epoch 424/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.8875 - val_loss: 22.2827\n",
      "Epoch 425/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.2823 - val_loss: 23.4133\n",
      "Epoch 426/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.3280 - val_loss: 23.8759\n",
      "Epoch 427/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.4408 - val_loss: 23.4248\n",
      "Epoch 428/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.0217 - val_loss: 24.4548\n",
      "Epoch 429/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.2383 - val_loss: 24.1371\n",
      "Epoch 430/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.7939 - val_loss: 22.1241\n",
      "Epoch 431/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.5231 - val_loss: 24.4398\n",
      "Epoch 432/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.2416 - val_loss: 22.5119\n",
      "Epoch 433/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.7954 - val_loss: 23.6612\n",
      "Epoch 434/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.9754 - val_loss: 23.5158\n",
      "Epoch 435/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.7044 - val_loss: 22.6587\n",
      "Epoch 436/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.1052 - val_loss: 22.4695\n",
      "Epoch 437/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.0638 - val_loss: 22.8783\n",
      "Epoch 438/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.6318 - val_loss: 23.7945\n",
      "Epoch 439/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 18.1108 - val_loss: 23.3528\n",
      "Epoch 440/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.2717 - val_loss: 23.8298\n",
      "Epoch 441/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.0653 - val_loss: 24.2328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 442/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.7776 - val_loss: 23.4651\n",
      "Epoch 443/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.4730 - val_loss: 22.3869\n",
      "Epoch 444/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.7588 - val_loss: 22.3538\n",
      "Epoch 445/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.7965 - val_loss: 22.6878\n",
      "Epoch 446/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.1183 - val_loss: 23.3805\n",
      "Epoch 447/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.5308 - val_loss: 23.2606\n",
      "Epoch 448/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.7194 - val_loss: 22.4621\n",
      "Epoch 449/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.8960 - val_loss: 24.1435\n",
      "Epoch 450/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.5069 - val_loss: 22.7933\n",
      "Epoch 451/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.3196 - val_loss: 23.5631\n",
      "Epoch 452/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.8613 - val_loss: 23.9810\n",
      "Epoch 453/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.0959 - val_loss: 22.6478\n",
      "Epoch 454/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.9823 - val_loss: 22.5404\n",
      "Epoch 455/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.7423 - val_loss: 23.2336\n",
      "Epoch 456/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.7911 - val_loss: 23.5541\n",
      "Epoch 457/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.3065 - val_loss: 23.0830\n",
      "Epoch 458/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.4082 - val_loss: 24.2137\n",
      "Epoch 459/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.7913 - val_loss: 23.0465\n",
      "Epoch 460/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.4219 - val_loss: 22.9452\n",
      "Epoch 461/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.7083 - val_loss: 22.6086\n",
      "Epoch 462/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.1911 - val_loss: 23.8945\n",
      "Epoch 463/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.0481 - val_loss: 25.1710\n",
      "Epoch 464/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.7807 - val_loss: 24.7888\n",
      "Epoch 465/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.1509 - val_loss: 22.5827\n",
      "Epoch 466/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.7896 - val_loss: 24.7933\n",
      "Epoch 467/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.9516 - val_loss: 22.5826\n",
      "Epoch 468/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.7206 - val_loss: 22.0918\n",
      "Epoch 469/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.6604 - val_loss: 22.6041\n",
      "Epoch 470/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.2571 - val_loss: 24.3122\n",
      "Epoch 471/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.9605 - val_loss: 24.0895\n",
      "Epoch 472/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.6023 - val_loss: 23.3890\n",
      "Epoch 473/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.6370 - val_loss: 22.0199\n",
      "Epoch 474/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.3780 - val_loss: 22.8759\n",
      "Epoch 475/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.4864 - val_loss: 22.5460\n",
      "Epoch 476/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.0616 - val_loss: 23.6432\n",
      "Epoch 477/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.0286 - val_loss: 22.8601\n",
      "Epoch 478/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.3564 - val_loss: 23.8587\n",
      "Epoch 479/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.3224 - val_loss: 22.7390\n",
      "Epoch 480/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.4800 - val_loss: 22.6816\n",
      "Epoch 481/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.4773 - val_loss: 23.4622\n",
      "Epoch 482/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.8103 - val_loss: 25.0186\n",
      "Epoch 483/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.8339 - val_loss: 23.5677\n",
      "Epoch 484/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.1873 - val_loss: 23.5199\n",
      "Epoch 485/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.7881 - val_loss: 23.9774\n",
      "Epoch 486/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.4632 - val_loss: 23.2205\n",
      "Epoch 487/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.5757 - val_loss: 23.0561\n",
      "Epoch 488/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.6289 - val_loss: 22.3953\n",
      "Epoch 489/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.7272 - val_loss: 22.9571\n",
      "Epoch 490/2000\n",
      "1500/1500 [==============================] - 0s 30us/sample - loss: 18.0917 - val_loss: 23.0714\n",
      "Epoch 491/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.0322 - val_loss: 23.3934\n",
      "Epoch 492/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.8190 - val_loss: 22.6170\n",
      "Epoch 493/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.3627 - val_loss: 23.1681\n",
      "Epoch 494/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.5803 - val_loss: 23.2566\n",
      "Epoch 495/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.2602 - val_loss: 22.7089\n",
      "Epoch 496/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.0536 - val_loss: 21.8980\n",
      "Epoch 497/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.6604 - val_loss: 22.3116\n",
      "Epoch 498/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.1075 - val_loss: 23.2072\n",
      "Epoch 499/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.4395 - val_loss: 22.1642\n",
      "Epoch 500/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.4911 - val_loss: 23.1298\n",
      "Epoch 501/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.5881 - val_loss: 22.4650\n",
      "Epoch 502/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.1741 - val_loss: 24.2558\n",
      "Epoch 503/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.5945 - val_loss: 22.7024\n",
      "Epoch 504/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.9552 - val_loss: 24.1119\n",
      "Epoch 505/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.8003 - val_loss: 24.0661\n",
      "Epoch 506/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.7633 - val_loss: 24.5377\n",
      "Epoch 507/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.1665 - val_loss: 22.2751\n",
      "Epoch 508/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.2769 - val_loss: 22.2209\n",
      "Epoch 509/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.5431 - val_loss: 24.0154\n",
      "Epoch 510/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.1900 - val_loss: 27.9462\n",
      "Epoch 511/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.3751 - val_loss: 23.5074\n",
      "Epoch 512/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.2748 - val_loss: 22.1208\n",
      "Epoch 513/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.7184 - val_loss: 22.7144\n",
      "Epoch 514/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.6154 - val_loss: 23.9891\n",
      "Epoch 515/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.7825 - val_loss: 23.8724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 516/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.4942 - val_loss: 23.8612\n",
      "Epoch 517/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.4251 - val_loss: 24.2056\n",
      "Epoch 518/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.2730 - val_loss: 22.3844\n",
      "Epoch 519/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.0771 - val_loss: 22.4886\n",
      "Epoch 520/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.9989 - val_loss: 23.9419\n",
      "Epoch 521/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.5150 - val_loss: 23.3781\n",
      "Epoch 522/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.8461 - val_loss: 23.1045\n",
      "Epoch 523/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.2586 - val_loss: 26.4237\n",
      "Epoch 524/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.1155 - val_loss: 22.6611\n",
      "Epoch 525/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.6092 - val_loss: 25.7246\n",
      "Epoch 526/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.9780 - val_loss: 24.2949\n",
      "Epoch 527/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.8507 - val_loss: 22.8160\n",
      "Epoch 528/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.4772 - val_loss: 22.7188\n",
      "Epoch 529/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.4853 - val_loss: 22.8236\n",
      "Epoch 530/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.6879 - val_loss: 22.1377\n",
      "Epoch 531/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.6325 - val_loss: 24.4748\n",
      "Epoch 532/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.6901 - val_loss: 23.1217\n",
      "Epoch 533/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.4126 - val_loss: 22.9591\n",
      "Epoch 534/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.0883 - val_loss: 23.9125\n",
      "Epoch 535/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.7856 - val_loss: 25.0179\n",
      "Epoch 536/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.3766 - val_loss: 24.2126\n",
      "Epoch 537/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.4445 - val_loss: 24.0187\n",
      "Epoch 538/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.4588 - val_loss: 26.8451\n",
      "Epoch 539/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.7666 - val_loss: 24.9969\n",
      "Epoch 540/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.8050 - val_loss: 24.2708\n",
      "Epoch 541/2000\n",
      "1500/1500 [==============================] - 0s 30us/sample - loss: 17.4703 - val_loss: 22.3427\n",
      "Epoch 542/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.9422 - val_loss: 22.9374\n",
      "Epoch 543/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.9312 - val_loss: 22.6580\n",
      "Epoch 544/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.1295 - val_loss: 21.6358\n",
      "Epoch 545/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.7324 - val_loss: 22.8039\n",
      "Epoch 546/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.9264 - val_loss: 23.7261\n",
      "Epoch 547/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.2956 - val_loss: 22.3626\n",
      "Epoch 548/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.4973 - val_loss: 22.6731\n",
      "Epoch 549/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.4108 - val_loss: 22.8634\n",
      "Epoch 550/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.7383 - val_loss: 22.1455\n",
      "Epoch 551/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.2572 - val_loss: 23.7139\n",
      "Epoch 552/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.4899 - val_loss: 22.1903\n",
      "Epoch 553/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.2655 - val_loss: 22.5747\n",
      "Epoch 554/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.5524 - val_loss: 24.8550\n",
      "Epoch 555/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.3059 - val_loss: 22.3490\n",
      "Epoch 556/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.6315 - val_loss: 22.7565\n",
      "Epoch 557/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.3305 - val_loss: 24.4495\n",
      "Epoch 558/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.9875 - val_loss: 22.6482\n",
      "Epoch 559/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.1361 - val_loss: 22.3975\n",
      "Epoch 560/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.0991 - val_loss: 23.4524\n",
      "Epoch 561/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.5771 - val_loss: 23.8907\n",
      "Epoch 562/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.0663 - val_loss: 25.5413\n",
      "Epoch 563/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.6719 - val_loss: 24.0555\n",
      "Epoch 564/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.1467 - val_loss: 23.1685\n",
      "Epoch 565/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.0142 - val_loss: 23.2960\n",
      "Epoch 566/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.4453 - val_loss: 23.1291\n",
      "Epoch 567/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.2972 - val_loss: 22.5649\n",
      "Epoch 568/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.9021 - val_loss: 22.9844\n",
      "Epoch 569/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.8085 - val_loss: 23.6323\n",
      "Epoch 570/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.8691 - val_loss: 23.0820\n",
      "Epoch 571/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.4887 - val_loss: 22.4636\n",
      "Epoch 572/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.8623 - val_loss: 22.7705\n",
      "Epoch 573/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.9549 - val_loss: 22.5607\n",
      "Epoch 574/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.4310 - val_loss: 23.9241\n",
      "Epoch 575/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 18.2179 - val_loss: 23.1725\n",
      "Epoch 576/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 17.1871 - val_loss: 22.1262\n",
      "Epoch 577/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.8442 - val_loss: 22.6948\n",
      "Epoch 578/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.1399 - val_loss: 22.5831\n",
      "Epoch 579/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.3937 - val_loss: 22.4832\n",
      "Epoch 580/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.1174 - val_loss: 24.3341\n",
      "Epoch 581/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.2715 - val_loss: 22.4304\n",
      "Epoch 582/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.3068 - val_loss: 23.4365\n",
      "Epoch 583/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.7136 - val_loss: 22.1713\n",
      "Epoch 584/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.8535 - val_loss: 22.6709\n",
      "Epoch 585/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.2014 - val_loss: 22.5121\n",
      "Epoch 586/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.4887 - val_loss: 22.6707\n",
      "Epoch 587/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.5528 - val_loss: 22.3645\n",
      "Epoch 588/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.9284 - val_loss: 23.6890\n",
      "Epoch 589/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.7304 - val_loss: 23.6209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 590/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.9713 - val_loss: 22.8946\n",
      "Epoch 591/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 17.6479 - val_loss: 22.5506\n",
      "Epoch 592/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 17.2852 - val_loss: 22.8513\n",
      "Epoch 593/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.9993 - val_loss: 23.9583\n",
      "Epoch 594/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.8591 - val_loss: 24.5027\n",
      "Epoch 595/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.9495 - val_loss: 22.6443\n",
      "Epoch 596/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.3528 - val_loss: 22.0576\n",
      "Epoch 597/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.2433 - val_loss: 21.8194\n",
      "Epoch 598/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.2918 - val_loss: 22.5605\n",
      "Epoch 599/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.9014 - val_loss: 23.2630\n",
      "Epoch 600/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.1043 - val_loss: 22.5744\n",
      "Epoch 601/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.2335 - val_loss: 24.1212\n",
      "Epoch 602/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.9329 - val_loss: 22.7228\n",
      "Epoch 603/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 16.9613 - val_loss: 24.2879\n",
      "Epoch 604/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.5404 - val_loss: 24.4581\n",
      "Epoch 605/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.9330 - val_loss: 22.2590\n",
      "Epoch 606/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.3798 - val_loss: 23.1326\n",
      "Epoch 607/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.7090 - val_loss: 23.1801\n",
      "Epoch 608/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.3026 - val_loss: 26.0745\n",
      "Epoch 609/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.0392 - val_loss: 23.0046\n",
      "Epoch 610/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.9935 - val_loss: 22.2872\n",
      "Epoch 611/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.4937 - val_loss: 22.3958\n",
      "Epoch 612/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.9332 - val_loss: 22.0591\n",
      "Epoch 613/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.6302 - val_loss: 22.7140\n",
      "Epoch 614/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.2986 - val_loss: 22.8885\n",
      "Epoch 615/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.0002 - val_loss: 22.8840\n",
      "Epoch 616/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.7045 - val_loss: 22.2253\n",
      "Epoch 617/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.5847 - val_loss: 23.1120\n",
      "Epoch 618/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.9924 - val_loss: 23.1971\n",
      "Epoch 619/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.5448 - val_loss: 24.3034\n",
      "Epoch 620/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.9367 - val_loss: 22.3931\n",
      "Epoch 621/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.1914 - val_loss: 23.1924\n",
      "Epoch 622/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.8095 - val_loss: 21.8424\n",
      "Epoch 623/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.5207 - val_loss: 22.9595\n",
      "Epoch 624/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.6673 - val_loss: 23.9496\n",
      "Epoch 625/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.6293 - val_loss: 22.9451\n",
      "Epoch 626/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.6109 - val_loss: 22.2521\n",
      "Epoch 627/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.0391 - val_loss: 22.9528\n",
      "Epoch 628/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.6701 - val_loss: 24.0365\n",
      "Epoch 629/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.1373 - val_loss: 24.6292\n",
      "Epoch 630/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.2826 - val_loss: 23.0290\n",
      "Epoch 631/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.5017 - val_loss: 23.0367\n",
      "Epoch 632/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.3944 - val_loss: 22.7009\n",
      "Epoch 633/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.7787 - val_loss: 24.0327\n",
      "Epoch 634/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.1004 - val_loss: 23.7460\n",
      "Epoch 635/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.0466 - val_loss: 25.2633\n",
      "Epoch 636/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.7922 - val_loss: 25.3261\n",
      "Epoch 637/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.9609 - val_loss: 23.4921\n",
      "Epoch 638/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.6328 - val_loss: 23.1124\n",
      "Epoch 639/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.7346 - val_loss: 23.4200\n",
      "Epoch 640/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.1232 - val_loss: 22.4925\n",
      "Epoch 641/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 16.7086 - val_loss: 22.0854\n",
      "Epoch 642/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.9385 - val_loss: 24.9247\n",
      "Epoch 643/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.0718 - val_loss: 22.2250\n",
      "Epoch 644/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.1313 - val_loss: 22.0089\n",
      "Epoch 645/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.5661 - val_loss: 22.6547\n",
      "Epoch 646/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.8958 - val_loss: 22.3683\n",
      "Epoch 647/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.5687 - val_loss: 22.6890\n",
      "Epoch 648/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 16.8926 - val_loss: 22.7735\n",
      "Epoch 649/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.3670 - val_loss: 22.6387\n",
      "Epoch 650/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.0662 - val_loss: 22.6578\n",
      "Epoch 651/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.3211 - val_loss: 23.4757\n",
      "Epoch 652/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.0274 - val_loss: 22.9058\n",
      "Epoch 653/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.3615 - val_loss: 22.5816\n",
      "Epoch 654/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.0718 - val_loss: 22.2411\n",
      "Epoch 655/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.9868 - val_loss: 23.5543\n",
      "Epoch 656/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.7387 - val_loss: 22.3680\n",
      "Epoch 657/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.4678 - val_loss: 23.8402\n",
      "Epoch 658/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.9318 - val_loss: 22.0182\n",
      "Epoch 659/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.1002 - val_loss: 22.2939\n",
      "Epoch 660/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.5081 - val_loss: 23.8443\n",
      "Epoch 661/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.0597 - val_loss: 23.4020\n",
      "Epoch 662/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.3918 - val_loss: 22.3677\n",
      "Epoch 663/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.3372 - val_loss: 22.6320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 664/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.8145 - val_loss: 22.4622\n",
      "Epoch 665/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.6853 - val_loss: 22.9573\n",
      "Epoch 666/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.0127 - val_loss: 23.1062\n",
      "Epoch 667/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.6585 - val_loss: 23.4147\n",
      "Epoch 668/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.1309 - val_loss: 22.2764\n",
      "Epoch 669/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.0087 - val_loss: 22.9552\n",
      "Epoch 670/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.5956 - val_loss: 22.3053\n",
      "Epoch 671/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.4677 - val_loss: 22.3712\n",
      "Epoch 672/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.6905 - val_loss: 28.4649\n",
      "Epoch 673/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.6215 - val_loss: 22.6300\n",
      "Epoch 674/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.6502 - val_loss: 22.5212\n",
      "Epoch 675/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.5685 - val_loss: 21.9762\n",
      "Epoch 676/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.6805 - val_loss: 22.5176\n",
      "Epoch 677/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.7001 - val_loss: 21.9661\n",
      "Epoch 678/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.9878 - val_loss: 22.3226\n",
      "Epoch 679/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.9448 - val_loss: 23.7695\n",
      "Epoch 680/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.7710 - val_loss: 23.3818\n",
      "Epoch 681/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.2105 - val_loss: 22.1726\n",
      "Epoch 682/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.3522 - val_loss: 22.2468\n",
      "Epoch 683/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.6772 - val_loss: 23.4872\n",
      "Epoch 684/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.2966 - val_loss: 22.0008\n",
      "Epoch 685/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.7553 - val_loss: 24.1231\n",
      "Epoch 686/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.0886 - val_loss: 26.0737\n",
      "Epoch 687/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.3683 - val_loss: 23.0060\n",
      "Epoch 688/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.1773 - val_loss: 23.4953\n",
      "Epoch 689/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.0634 - val_loss: 22.2722\n",
      "Epoch 690/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.2218 - val_loss: 22.7034\n",
      "Epoch 691/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.2429 - val_loss: 23.2783\n",
      "Epoch 692/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.2925 - val_loss: 23.5752\n",
      "Epoch 693/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.4866 - val_loss: 23.4548\n",
      "Epoch 694/2000\n",
      "1500/1500 [==============================] - 0s 31us/sample - loss: 16.9938 - val_loss: 23.8586\n",
      "Epoch 695/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.3068 - val_loss: 22.5728\n",
      "Epoch 696/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.8970 - val_loss: 22.4838\n",
      "Epoch 697/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.0782 - val_loss: 22.5705\n",
      "Epoch 698/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.8251 - val_loss: 23.6286\n",
      "Epoch 699/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.5905 - val_loss: 27.6300\n",
      "Epoch 700/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.1769 - val_loss: 22.8454\n",
      "Epoch 701/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.2020 - val_loss: 23.1833\n",
      "Epoch 702/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.6768 - val_loss: 22.5305\n",
      "Epoch 703/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.9657 - val_loss: 23.9604\n",
      "Epoch 704/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.8242 - val_loss: 23.0648\n",
      "Epoch 705/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.6682 - val_loss: 23.7037\n",
      "Epoch 706/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.5848 - val_loss: 23.3523\n",
      "Epoch 707/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.9787 - val_loss: 22.8757\n",
      "Epoch 708/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.0914 - val_loss: 25.2581\n",
      "Epoch 709/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.4529 - val_loss: 24.3486\n",
      "Epoch 710/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.6754 - val_loss: 22.5482\n",
      "Epoch 711/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.7287 - val_loss: 23.2142\n",
      "Epoch 712/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.3916 - val_loss: 21.8782\n",
      "Epoch 713/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.7650 - val_loss: 22.1581\n",
      "Epoch 714/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.2621 - val_loss: 22.7077\n",
      "Epoch 715/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.3265 - val_loss: 22.2750\n",
      "Epoch 716/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.8616 - val_loss: 22.3386\n",
      "Epoch 717/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.8787 - val_loss: 23.6844\n",
      "Epoch 718/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.2273 - val_loss: 23.6850\n",
      "Epoch 719/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.0929 - val_loss: 21.6933\n",
      "Epoch 720/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.2414 - val_loss: 24.3135\n",
      "Epoch 721/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.9705 - val_loss: 24.0290\n",
      "Epoch 722/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.2850 - val_loss: 22.7953\n",
      "Epoch 723/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.4183 - val_loss: 22.8295\n",
      "Epoch 724/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.1027 - val_loss: 23.9736\n",
      "Epoch 725/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.0900 - val_loss: 22.6414\n",
      "Epoch 726/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.6908 - val_loss: 21.9794\n",
      "Epoch 727/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.5831 - val_loss: 23.8804\n",
      "Epoch 728/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.9468 - val_loss: 23.9654\n",
      "Epoch 729/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.4928 - val_loss: 22.9738\n",
      "Epoch 730/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.3418 - val_loss: 23.2496\n",
      "Epoch 731/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.3631 - val_loss: 24.7718\n",
      "Epoch 732/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.6166 - val_loss: 23.4632\n",
      "Epoch 733/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.3577 - val_loss: 23.1599\n",
      "Epoch 734/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 17.0888 - val_loss: 22.1912\n",
      "Epoch 735/2000\n",
      "1500/1500 [==============================] - 0s 32us/sample - loss: 16.8359 - val_loss: 22.7158\n",
      "Epoch 736/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 16.3982 - val_loss: 23.8807\n",
      "Epoch 737/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.3159 - val_loss: 23.7223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 738/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.8522 - val_loss: 23.5922\n",
      "Epoch 739/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.9594 - val_loss: 22.8954\n",
      "Epoch 740/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.9575 - val_loss: 22.6297\n",
      "Epoch 741/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.2206 - val_loss: 22.3555\n",
      "Epoch 742/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.1219 - val_loss: 23.1166\n",
      "Epoch 743/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.6003 - val_loss: 22.5608\n",
      "Epoch 744/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 15.9132 - val_loss: 22.5090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|███████████████████████████████████████████████████████████████████████▊          | 14/16 [07:31<00:58, 29.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'batc_normalization': False, 'batch_size': 64, 'dropout': 0, 'epochs': 2000, 'first_neuron': 320, 'hidden_layers': 2, 'hidden_neuron': 50, 'kernel_initializer': 'normal', 'last_activation': 'linear', 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "Train on 1500 samples, validate on 375 samples\n",
      "Epoch 1/2000\n",
      "1500/1500 [==============================] - 0s 246us/sample - loss: 2185.6590 - val_loss: 2184.7921\n",
      "Epoch 2/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 2174.8456 - val_loss: 2154.3013\n",
      "Epoch 3/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 2089.2405 - val_loss: 1967.2747\n",
      "Epoch 4/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 1707.2924 - val_loss: 1266.9109\n",
      "Epoch 5/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 831.2908 - val_loss: 594.4867\n",
      "Epoch 6/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 575.7107 - val_loss: 495.9956\n",
      "Epoch 7/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 471.6913 - val_loss: 403.7868\n",
      "Epoch 8/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 382.9968 - val_loss: 326.7036\n",
      "Epoch 9/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 300.8634 - val_loss: 250.7103\n",
      "Epoch 10/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 226.3702 - val_loss: 188.2831\n",
      "Epoch 11/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 169.9855 - val_loss: 142.1735\n",
      "Epoch 12/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 128.1689 - val_loss: 115.7861\n",
      "Epoch 13/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 100.2013 - val_loss: 87.1392\n",
      "Epoch 14/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 77.5328 - val_loss: 70.5024\n",
      "Epoch 15/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 62.4473 - val_loss: 59.6230\n",
      "Epoch 16/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 51.7336 - val_loss: 50.4711\n",
      "Epoch 17/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 45.0555 - val_loss: 46.9157\n",
      "Epoch 18/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 39.9451 - val_loss: 38.6637\n",
      "Epoch 19/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 36.7414 - val_loss: 36.1953\n",
      "Epoch 20/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 36.5590 - val_loss: 38.1829\n",
      "Epoch 21/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 32.1669 - val_loss: 33.5598\n",
      "Epoch 22/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 30.9728 - val_loss: 31.2630\n",
      "Epoch 23/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 28.0904 - val_loss: 29.0833\n",
      "Epoch 24/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 28.2138 - val_loss: 26.9196\n",
      "Epoch 25/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 26.6561 - val_loss: 27.3125\n",
      "Epoch 26/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 26.9459 - val_loss: 25.5065\n",
      "Epoch 27/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 26.7890 - val_loss: 27.5191\n",
      "Epoch 28/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 25.5704 - val_loss: 26.9971\n",
      "Epoch 29/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 24.5543 - val_loss: 25.4009\n",
      "Epoch 30/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 24.3590 - val_loss: 25.0950\n",
      "Epoch 31/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 24.1684 - val_loss: 25.4838\n",
      "Epoch 32/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 24.4102 - val_loss: 26.0607\n",
      "Epoch 33/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 23.9567 - val_loss: 28.1715\n",
      "Epoch 34/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 25.0095 - val_loss: 26.2538\n",
      "Epoch 35/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 24.1983 - val_loss: 24.7076\n",
      "Epoch 36/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 23.3714 - val_loss: 25.3935\n",
      "Epoch 37/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 23.9283 - val_loss: 26.6488\n",
      "Epoch 38/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 24.4708 - val_loss: 24.4814\n",
      "Epoch 39/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 24.0889 - val_loss: 25.6968\n",
      "Epoch 40/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 22.2505 - val_loss: 23.7841\n",
      "Epoch 41/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 23.0493 - val_loss: 23.7789\n",
      "Epoch 42/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 22.7521 - val_loss: 23.5912\n",
      "Epoch 43/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 22.6500 - val_loss: 25.5980\n",
      "Epoch 44/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 23.2986 - val_loss: 26.7928\n",
      "Epoch 45/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 22.8848 - val_loss: 24.7071\n",
      "Epoch 46/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 23.1718 - val_loss: 23.5045\n",
      "Epoch 47/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 22.4126 - val_loss: 23.7645\n",
      "Epoch 48/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 22.1331 - val_loss: 24.5957\n",
      "Epoch 49/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 22.9424 - val_loss: 25.8236\n",
      "Epoch 50/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 22.0235 - val_loss: 23.6074\n",
      "Epoch 51/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 22.5913 - val_loss: 23.4743\n",
      "Epoch 52/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 22.4001 - val_loss: 25.1188\n",
      "Epoch 53/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 22.9523 - val_loss: 23.2968\n",
      "Epoch 54/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 23.0852 - val_loss: 24.9666\n",
      "Epoch 55/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 22.0096 - val_loss: 24.4511\n",
      "Epoch 56/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 22.6094 - val_loss: 25.4924\n",
      "Epoch 57/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 21.8890 - val_loss: 25.1369\n",
      "Epoch 58/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 22.6262 - val_loss: 25.7160\n",
      "Epoch 59/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 21.5766 - val_loss: 23.8821\n",
      "Epoch 60/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 21.3846 - val_loss: 25.5282\n",
      "Epoch 61/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 22.2158 - val_loss: 24.9228\n",
      "Epoch 62/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 22.4134 - val_loss: 22.9933\n",
      "Epoch 63/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 21.1587 - val_loss: 26.2220\n",
      "Epoch 64/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 23.1719 - val_loss: 28.3458\n",
      "Epoch 65/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 21.7305 - val_loss: 23.3395\n",
      "Epoch 66/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 21.0745 - val_loss: 24.4742\n",
      "Epoch 67/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.5618 - val_loss: 23.9438\n",
      "Epoch 68/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 21.7198 - val_loss: 23.3469\n",
      "Epoch 69/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 22.8024 - val_loss: 24.4692\n",
      "Epoch 70/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 22.7804 - val_loss: 26.7671\n",
      "Epoch 71/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 22.0832 - val_loss: 23.2046\n",
      "Epoch 72/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.8408 - val_loss: 24.9541\n",
      "Epoch 73/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 21.0243 - val_loss: 24.6106\n",
      "Epoch 74/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 21.9380 - val_loss: 23.9356\n",
      "Epoch 75/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 21.0661 - val_loss: 24.5694\n",
      "Epoch 76/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.1198 - val_loss: 23.6905\n",
      "Epoch 77/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 22.7545 - val_loss: 28.2322\n",
      "Epoch 78/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 22.0104 - val_loss: 28.2537\n",
      "Epoch 79/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 21.4382 - val_loss: 26.5989\n",
      "Epoch 80/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 21.1801 - val_loss: 22.4249\n",
      "Epoch 81/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.1983 - val_loss: 23.2337\n",
      "Epoch 82/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 21.3142 - val_loss: 23.7935\n",
      "Epoch 83/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.9425 - val_loss: 24.0000\n",
      "Epoch 84/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 21.0878 - val_loss: 29.0004\n",
      "Epoch 85/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 22.3062 - val_loss: 23.5188\n",
      "Epoch 86/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.1726 - val_loss: 25.4094\n",
      "Epoch 87/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 23.4726 - val_loss: 24.7828\n",
      "Epoch 88/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 20.6518 - val_loss: 25.5731\n",
      "Epoch 89/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.3297 - val_loss: 24.4977\n",
      "Epoch 90/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.9963 - val_loss: 26.0637\n",
      "Epoch 91/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.4951 - val_loss: 22.6378\n",
      "Epoch 92/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.6816 - val_loss: 24.4928\n",
      "Epoch 93/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 21.3336 - val_loss: 23.5092\n",
      "Epoch 94/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.5442 - val_loss: 22.6221\n",
      "Epoch 95/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.3419 - val_loss: 25.5551\n",
      "Epoch 96/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.9702 - val_loss: 23.1918\n",
      "Epoch 97/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.7631 - val_loss: 23.7726\n",
      "Epoch 98/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.4690 - val_loss: 25.0611\n",
      "Epoch 99/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.6980 - val_loss: 24.1296\n",
      "Epoch 100/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.3608 - val_loss: 26.1897\n",
      "Epoch 101/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 21.1098 - val_loss: 23.8997\n",
      "Epoch 102/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 21.4349 - val_loss: 24.7553\n",
      "Epoch 103/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.2404 - val_loss: 24.9077\n",
      "Epoch 104/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.6226 - val_loss: 23.4957\n",
      "Epoch 105/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.6566 - val_loss: 24.0599\n",
      "Epoch 106/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.9852 - val_loss: 24.5210\n",
      "Epoch 107/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 22.0358 - val_loss: 24.7833\n",
      "Epoch 108/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.4163 - val_loss: 23.7599\n",
      "Epoch 109/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.3177 - val_loss: 24.2218\n",
      "Epoch 110/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.9832 - val_loss: 23.6443\n",
      "Epoch 111/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.3231 - val_loss: 23.9868\n",
      "Epoch 112/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.5779 - val_loss: 24.5377\n",
      "Epoch 113/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 20.9478 - val_loss: 22.1294\n",
      "Epoch 114/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 20.4227 - val_loss: 23.7669\n",
      "Epoch 115/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.3266 - val_loss: 23.3788\n",
      "Epoch 116/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.8239 - val_loss: 23.9726\n",
      "Epoch 117/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.6190 - val_loss: 22.7651\n",
      "Epoch 118/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.1192 - val_loss: 22.9714\n",
      "Epoch 119/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.7416 - val_loss: 25.2661\n",
      "Epoch 120/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.7561 - val_loss: 27.6465\n",
      "Epoch 121/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.1565 - val_loss: 24.2495\n",
      "Epoch 122/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.0180 - val_loss: 23.1924\n",
      "Epoch 123/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.8494 - val_loss: 23.2179\n",
      "Epoch 124/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.8602 - val_loss: 23.2010\n",
      "Epoch 125/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.2166 - val_loss: 22.4150\n",
      "Epoch 126/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 22.6699 - val_loss: 25.1587\n",
      "Epoch 127/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 22.2558 - val_loss: 24.2832\n",
      "Epoch 128/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.1637 - val_loss: 23.3454\n",
      "Epoch 129/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.5376 - val_loss: 24.2224\n",
      "Epoch 130/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 20.4223 - val_loss: 22.6824\n",
      "Epoch 131/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.7333 - val_loss: 25.5461\n",
      "Epoch 132/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 22.1870 - val_loss: 23.6640\n",
      "Epoch 133/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.8914 - val_loss: 24.9201\n",
      "Epoch 134/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.8077 - val_loss: 24.0647\n",
      "Epoch 135/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.4783 - val_loss: 23.0639\n",
      "Epoch 136/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.0103 - val_loss: 23.6328\n",
      "Epoch 137/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.6465 - val_loss: 23.4141\n",
      "Epoch 138/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.7135 - val_loss: 25.6050\n",
      "Epoch 139/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 21.0224 - val_loss: 25.2445\n",
      "Epoch 140/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 22.1323 - val_loss: 25.0995\n",
      "Epoch 141/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.2709 - val_loss: 24.8465\n",
      "Epoch 142/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.4656 - val_loss: 23.8410\n",
      "Epoch 143/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.2995 - val_loss: 24.4081\n",
      "Epoch 144/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.3850 - val_loss: 23.6386\n",
      "Epoch 145/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.5808 - val_loss: 22.7359\n",
      "Epoch 146/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.6358 - val_loss: 22.7380\n",
      "Epoch 147/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.2057 - val_loss: 24.0010\n",
      "Epoch 148/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.1709 - val_loss: 24.8631\n",
      "Epoch 149/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.1004 - val_loss: 25.0301\n",
      "Epoch 150/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.3489 - val_loss: 23.5806\n",
      "Epoch 151/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.3605 - val_loss: 22.7580\n",
      "Epoch 152/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.2296 - val_loss: 24.2043\n",
      "Epoch 153/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.9855 - val_loss: 24.4496\n",
      "Epoch 154/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 20.9116 - val_loss: 23.6379\n",
      "Epoch 155/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 21.2729 - val_loss: 27.4988\n",
      "Epoch 156/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.6364 - val_loss: 25.2379\n",
      "Epoch 157/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.2876 - val_loss: 28.0414\n",
      "Epoch 158/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 21.0272 - val_loss: 22.0805\n",
      "Epoch 159/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.4101 - val_loss: 22.8349\n",
      "Epoch 160/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.3849 - val_loss: 24.9457\n",
      "Epoch 161/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.6707 - val_loss: 23.0177\n",
      "Epoch 162/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.0272 - val_loss: 24.2342\n",
      "Epoch 163/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.3173 - val_loss: 23.9703\n",
      "Epoch 164/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.6509 - val_loss: 26.4677\n",
      "Epoch 165/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.5641 - val_loss: 27.9220\n",
      "Epoch 166/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 21.3606 - val_loss: 24.1121\n",
      "Epoch 167/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.6039 - val_loss: 22.7388\n",
      "Epoch 168/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.1761 - val_loss: 22.5679\n",
      "Epoch 169/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.2079 - val_loss: 22.5701\n",
      "Epoch 170/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.4485 - val_loss: 23.0536\n",
      "Epoch 171/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.7029 - val_loss: 24.4096\n",
      "Epoch 172/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.1363 - val_loss: 23.6315\n",
      "Epoch 173/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.7276 - val_loss: 24.7900\n",
      "Epoch 174/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.7765 - val_loss: 24.0838\n",
      "Epoch 175/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.2047 - val_loss: 21.8647\n",
      "Epoch 176/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.6419 - val_loss: 22.9210\n",
      "Epoch 177/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.3437 - val_loss: 23.4360\n",
      "Epoch 178/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 22.0963 - val_loss: 27.0160\n",
      "Epoch 179/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.8242 - val_loss: 26.8005\n",
      "Epoch 180/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.8166 - val_loss: 22.4816\n",
      "Epoch 181/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.5721 - val_loss: 25.9018\n",
      "Epoch 182/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.2054 - val_loss: 24.9406\n",
      "Epoch 183/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.6269 - val_loss: 22.9551\n",
      "Epoch 184/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.8885 - val_loss: 23.6207\n",
      "Epoch 185/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.9859 - val_loss: 23.3015\n",
      "Epoch 186/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.5468 - val_loss: 25.1840\n",
      "Epoch 187/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.2318 - val_loss: 24.7385\n",
      "Epoch 188/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.4525 - val_loss: 22.6769\n",
      "Epoch 189/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.4971 - val_loss: 22.6079\n",
      "Epoch 190/2000\n",
      "1500/1500 [==============================] - 0s 30us/sample - loss: 20.5203 - val_loss: 22.8700\n",
      "Epoch 191/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.0538 - val_loss: 24.4873\n",
      "Epoch 192/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.5290 - val_loss: 22.4804\n",
      "Epoch 193/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.7875 - val_loss: 22.6330\n",
      "Epoch 194/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.0585 - val_loss: 25.8885\n",
      "Epoch 195/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.1190 - val_loss: 24.4921\n",
      "Epoch 196/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.9251 - val_loss: 23.3946\n",
      "Epoch 197/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.5358 - val_loss: 22.7240\n",
      "Epoch 198/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.8595 - val_loss: 23.3814\n",
      "Epoch 199/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.2576 - val_loss: 24.0873\n",
      "Epoch 200/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.8026 - val_loss: 23.2034\n",
      "Epoch 201/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.0371 - val_loss: 24.5726\n",
      "Epoch 202/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.1323 - val_loss: 22.2925\n",
      "Epoch 203/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.7246 - val_loss: 24.4367\n",
      "Epoch 204/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 20.1179 - val_loss: 24.1390\n",
      "Epoch 205/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.0550 - val_loss: 25.0339\n",
      "Epoch 206/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.4212 - val_loss: 22.9929\n",
      "Epoch 207/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.1747 - val_loss: 25.9203\n",
      "Epoch 208/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.0906 - val_loss: 22.8371\n",
      "Epoch 209/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.9814 - val_loss: 22.4224\n",
      "Epoch 210/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.3689 - val_loss: 22.3724\n",
      "Epoch 211/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.5617 - val_loss: 24.1679\n",
      "Epoch 212/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.5027 - val_loss: 22.9975\n",
      "Epoch 213/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.8356 - val_loss: 23.4741\n",
      "Epoch 214/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.0260 - val_loss: 24.6946\n",
      "Epoch 215/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.1673 - val_loss: 22.8518\n",
      "Epoch 216/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.5697 - val_loss: 23.4071\n",
      "Epoch 217/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.1436 - val_loss: 23.5236\n",
      "Epoch 218/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.1186 - val_loss: 22.6659\n",
      "Epoch 219/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.4067 - val_loss: 26.1157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 220/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.5299 - val_loss: 29.4108\n",
      "Epoch 221/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.3794 - val_loss: 22.4743\n",
      "Epoch 222/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.8507 - val_loss: 22.8359\n",
      "Epoch 223/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.8916 - val_loss: 23.3775\n",
      "Epoch 224/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.8531 - val_loss: 24.2860\n",
      "Epoch 225/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.7611 - val_loss: 22.8758\n",
      "Epoch 226/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.1757 - val_loss: 24.9424\n",
      "Epoch 227/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.3603 - val_loss: 23.7974\n",
      "Epoch 228/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.2544 - val_loss: 23.4944\n",
      "Epoch 229/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.5150 - val_loss: 24.4478\n",
      "Epoch 230/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.8774 - val_loss: 22.7279\n",
      "Epoch 231/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.8278 - val_loss: 24.1622\n",
      "Epoch 232/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.5600 - val_loss: 22.0396\n",
      "Epoch 233/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.7026 - val_loss: 22.6176\n",
      "Epoch 234/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.0315 - val_loss: 22.5298\n",
      "Epoch 235/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.9454 - val_loss: 25.8107\n",
      "Epoch 236/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.4753 - val_loss: 22.5792\n",
      "Epoch 237/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.1973 - val_loss: 23.1334\n",
      "Epoch 238/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.3876 - val_loss: 22.9914\n",
      "Epoch 239/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.2529 - val_loss: 23.3421\n",
      "Epoch 240/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 18.6517 - val_loss: 23.4398\n",
      "Epoch 241/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 19.2578 - val_loss: 24.5311\n",
      "Epoch 242/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.1504 - val_loss: 26.1048\n",
      "Epoch 243/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.3112 - val_loss: 23.9883\n",
      "Epoch 244/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.6731 - val_loss: 25.6192\n",
      "Epoch 245/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.8883 - val_loss: 25.5738\n",
      "Epoch 246/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.1171 - val_loss: 23.5217\n",
      "Epoch 247/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.3208 - val_loss: 23.3607\n",
      "Epoch 248/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.4395 - val_loss: 23.9684\n",
      "Epoch 249/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.4722 - val_loss: 23.6741\n",
      "Epoch 250/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.7484 - val_loss: 25.7442\n",
      "Epoch 251/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.5680 - val_loss: 21.7999\n",
      "Epoch 252/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 18.8029 - val_loss: 25.9160\n",
      "Epoch 253/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.8475 - val_loss: 22.7714\n",
      "Epoch 254/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.4110 - val_loss: 22.7659\n",
      "Epoch 255/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.6659 - val_loss: 24.0011\n",
      "Epoch 256/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.9277 - val_loss: 22.3639\n",
      "Epoch 257/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.7316 - val_loss: 23.5946\n",
      "Epoch 258/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.9049 - val_loss: 21.9152\n",
      "Epoch 259/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.3480 - val_loss: 24.1161\n",
      "Epoch 260/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.8259 - val_loss: 23.6048\n",
      "Epoch 261/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.3908 - val_loss: 23.1818\n",
      "Epoch 262/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.5419 - val_loss: 29.9504\n",
      "Epoch 263/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.7865 - val_loss: 23.2854\n",
      "Epoch 264/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.4562 - val_loss: 23.2986\n",
      "Epoch 265/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.6104 - val_loss: 24.0340\n",
      "Epoch 266/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.5439 - val_loss: 22.9930\n",
      "Epoch 267/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.0858 - val_loss: 25.8522\n",
      "Epoch 268/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.1646 - val_loss: 23.3269\n",
      "Epoch 269/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.7235 - val_loss: 23.5162\n",
      "Epoch 270/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.7767 - val_loss: 22.6157\n",
      "Epoch 271/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.7630 - val_loss: 23.8502\n",
      "Epoch 272/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 20.2928 - val_loss: 23.9333\n",
      "Epoch 273/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.6589 - val_loss: 22.4526\n",
      "Epoch 274/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.7506 - val_loss: 26.8281\n",
      "Epoch 275/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.0333 - val_loss: 23.6037\n",
      "Epoch 276/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.5332 - val_loss: 23.7351\n",
      "Epoch 277/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.4028 - val_loss: 22.2323\n",
      "Epoch 278/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.5391 - val_loss: 23.9463\n",
      "Epoch 279/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.7602 - val_loss: 24.2649\n",
      "Epoch 280/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.5113 - val_loss: 23.8768\n",
      "Epoch 281/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.6972 - val_loss: 25.2674\n",
      "Epoch 282/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.9305 - val_loss: 23.1987\n",
      "Epoch 283/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.5154 - val_loss: 23.1520\n",
      "Epoch 284/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.4198 - val_loss: 22.4382\n",
      "Epoch 285/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.7642 - val_loss: 21.7717\n",
      "Epoch 286/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.5570 - val_loss: 23.0087\n",
      "Epoch 287/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.9898 - val_loss: 23.4078\n",
      "Epoch 288/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.9500 - val_loss: 22.8469\n",
      "Epoch 289/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.0775 - val_loss: 24.5982\n",
      "Epoch 290/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.2363 - val_loss: 24.8883\n",
      "Epoch 291/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.5648 - val_loss: 23.1510\n",
      "Epoch 292/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 18.2052 - val_loss: 21.9061\n",
      "Epoch 293/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.7047 - val_loss: 22.8162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 294/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.8961 - val_loss: 22.3708\n",
      "Epoch 295/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.4610 - val_loss: 22.9709\n",
      "Epoch 296/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.2313 - val_loss: 22.4319\n",
      "Epoch 297/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.0406 - val_loss: 22.1621\n",
      "Epoch 298/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.7364 - val_loss: 24.6193\n",
      "Epoch 299/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.9726 - val_loss: 22.4523\n",
      "Epoch 300/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.3273 - val_loss: 24.2813\n",
      "Epoch 301/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.8532 - val_loss: 24.5397\n",
      "Epoch 302/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.4233 - val_loss: 23.2215\n",
      "Epoch 303/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.7272 - val_loss: 22.2466\n",
      "Epoch 304/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.1892 - val_loss: 23.2141\n",
      "Epoch 305/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.4179 - val_loss: 24.1777\n",
      "Epoch 306/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.5080 - val_loss: 21.6395\n",
      "Epoch 307/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.2882 - val_loss: 25.4946\n",
      "Epoch 308/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.0969 - val_loss: 26.6788\n",
      "Epoch 309/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.8048 - val_loss: 25.4015\n",
      "Epoch 310/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.9918 - val_loss: 22.3805\n",
      "Epoch 311/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.0894 - val_loss: 24.3739\n",
      "Epoch 312/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.9165 - val_loss: 24.6638\n",
      "Epoch 313/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.1649 - val_loss: 23.9087\n",
      "Epoch 314/2000\n",
      "1500/1500 [==============================] - 0s 23us/sample - loss: 19.1325 - val_loss: 23.6079\n",
      "Epoch 315/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.8927 - val_loss: 22.6383\n",
      "Epoch 316/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 20.1208 - val_loss: 25.7990\n",
      "Epoch 317/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.5196 - val_loss: 22.2685\n",
      "Epoch 318/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.5046 - val_loss: 22.9417\n",
      "Epoch 319/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.7089 - val_loss: 25.6939\n",
      "Epoch 320/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.5659 - val_loss: 24.1728\n",
      "Epoch 321/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.3089 - val_loss: 23.2948\n",
      "Epoch 322/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.0130 - val_loss: 25.0347\n",
      "Epoch 323/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.0156 - val_loss: 24.1604\n",
      "Epoch 324/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.9317 - val_loss: 21.3988\n",
      "Epoch 325/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.7408 - val_loss: 23.5825\n",
      "Epoch 326/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.4876 - val_loss: 24.6311\n",
      "Epoch 327/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.2872 - val_loss: 21.7261\n",
      "Epoch 328/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.8595 - val_loss: 26.5253\n",
      "Epoch 329/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 20.3584 - val_loss: 23.0619\n",
      "Epoch 330/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.7063 - val_loss: 22.9222\n",
      "Epoch 331/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.2755 - val_loss: 22.7112\n",
      "Epoch 332/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.2789 - val_loss: 23.4641\n",
      "Epoch 333/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.3594 - val_loss: 22.9690\n",
      "Epoch 334/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.4739 - val_loss: 22.3059\n",
      "Epoch 335/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.6352 - val_loss: 23.4048\n",
      "Epoch 336/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.4204 - val_loss: 23.8858\n",
      "Epoch 337/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.4601 - val_loss: 22.7766\n",
      "Epoch 338/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.2563 - val_loss: 22.5125\n",
      "Epoch 339/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.2623 - val_loss: 22.0402\n",
      "Epoch 340/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.5679 - val_loss: 22.7251\n",
      "Epoch 341/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.3470 - val_loss: 24.2576\n",
      "Epoch 342/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.6869 - val_loss: 21.6497\n",
      "Epoch 343/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 17.1114 - val_loss: 23.4717\n",
      "Epoch 344/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.6656 - val_loss: 24.4369\n",
      "Epoch 345/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.6988 - val_loss: 28.1427\n",
      "Epoch 346/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.6614 - val_loss: 23.7452\n",
      "Epoch 347/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.0928 - val_loss: 21.9091\n",
      "Epoch 348/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.0969 - val_loss: 22.5154\n",
      "Epoch 349/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.4443 - val_loss: 25.0204\n",
      "Epoch 350/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.6120 - val_loss: 23.0407\n",
      "Epoch 351/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.5909 - val_loss: 22.8066\n",
      "Epoch 352/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.4693 - val_loss: 26.7676\n",
      "Epoch 353/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.1649 - val_loss: 22.3352\n",
      "Epoch 354/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.8876 - val_loss: 21.6256\n",
      "Epoch 355/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.0286 - val_loss: 22.7546\n",
      "Epoch 356/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.2431 - val_loss: 22.5812\n",
      "Epoch 357/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 17.7929 - val_loss: 21.8419\n",
      "Epoch 358/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.7021 - val_loss: 23.1968\n",
      "Epoch 359/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.6189 - val_loss: 22.1609\n",
      "Epoch 360/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.4428 - val_loss: 22.6849\n",
      "Epoch 361/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.1941 - val_loss: 22.4746\n",
      "Epoch 362/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 16.8917 - val_loss: 23.6187\n",
      "Epoch 363/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.4871 - val_loss: 24.4978\n",
      "Epoch 364/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.7699 - val_loss: 24.2993\n",
      "Epoch 365/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.7888 - val_loss: 22.4173\n",
      "Epoch 366/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.9780 - val_loss: 23.7790\n",
      "Epoch 367/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.7473 - val_loss: 23.7169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 368/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.4262 - val_loss: 21.7749\n",
      "Epoch 369/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.1496 - val_loss: 23.0342\n",
      "Epoch 370/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.8990 - val_loss: 23.1040\n",
      "Epoch 371/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.5941 - val_loss: 22.1731\n",
      "Epoch 372/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.9198 - val_loss: 22.3787\n",
      "Epoch 373/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.3149 - val_loss: 23.4437\n",
      "Epoch 374/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.6636 - val_loss: 21.6751\n",
      "Epoch 375/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.6639 - val_loss: 24.3829\n",
      "Epoch 376/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.4525 - val_loss: 24.8966\n",
      "Epoch 377/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.9831 - val_loss: 22.8988\n",
      "Epoch 378/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.4417 - val_loss: 21.9831\n",
      "Epoch 379/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.1772 - val_loss: 25.0281\n",
      "Epoch 380/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.6894 - val_loss: 23.4913\n",
      "Epoch 381/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.2464 - val_loss: 21.7859\n",
      "Epoch 382/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.9131 - val_loss: 24.3904\n",
      "Epoch 383/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.4196 - val_loss: 22.6674\n",
      "Epoch 384/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.8691 - val_loss: 25.2371\n",
      "Epoch 385/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.5614 - val_loss: 21.9649\n",
      "Epoch 386/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.4195 - val_loss: 21.4645\n",
      "Epoch 387/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.1742 - val_loss: 23.5262\n",
      "Epoch 388/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 19.3195 - val_loss: 24.6413\n",
      "Epoch 389/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.5459 - val_loss: 22.3206\n",
      "Epoch 390/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.6818 - val_loss: 22.3028\n",
      "Epoch 391/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.0335 - val_loss: 23.5468\n",
      "Epoch 392/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.9075 - val_loss: 22.1528\n",
      "Epoch 393/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.4160 - val_loss: 21.7782\n",
      "Epoch 394/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 17.9805 - val_loss: 22.2614\n",
      "Epoch 395/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.8057 - val_loss: 23.8136\n",
      "Epoch 396/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.6962 - val_loss: 21.8905\n",
      "Epoch 397/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.9612 - val_loss: 22.2184\n",
      "Epoch 398/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.7303 - val_loss: 21.6570\n",
      "Epoch 399/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.8917 - val_loss: 23.0778\n",
      "Epoch 400/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.4706 - val_loss: 22.2089\n",
      "Epoch 401/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.6753 - val_loss: 22.3020\n",
      "Epoch 402/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.2933 - val_loss: 24.8582\n",
      "Epoch 403/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.2828 - val_loss: 23.2531\n",
      "Epoch 404/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.2068 - val_loss: 22.7821\n",
      "Epoch 405/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.8293 - val_loss: 24.4016\n",
      "Epoch 406/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.4644 - val_loss: 21.6068\n",
      "Epoch 407/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.1263 - val_loss: 23.2943\n",
      "Epoch 408/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.9720 - val_loss: 23.7077\n",
      "Epoch 409/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.6465 - val_loss: 22.4444\n",
      "Epoch 410/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.0266 - val_loss: 22.3564\n",
      "Epoch 411/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.3891 - val_loss: 22.9334\n",
      "Epoch 412/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.8912 - val_loss: 25.1343\n",
      "Epoch 413/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.3673 - val_loss: 25.0218\n",
      "Epoch 414/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.2629 - val_loss: 22.6929\n",
      "Epoch 415/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.3600 - val_loss: 23.4892\n",
      "Epoch 416/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.9600 - val_loss: 22.3068\n",
      "Epoch 417/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.0859 - val_loss: 23.7634\n",
      "Epoch 418/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.9699 - val_loss: 22.1341\n",
      "Epoch 419/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.0075 - val_loss: 23.5961\n",
      "Epoch 420/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.9235 - val_loss: 24.9351\n",
      "Epoch 421/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.0227 - val_loss: 22.2495\n",
      "Epoch 422/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.8251 - val_loss: 25.8900\n",
      "Epoch 423/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.8481 - val_loss: 23.3028\n",
      "Epoch 424/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.2299 - val_loss: 21.6662\n",
      "Epoch 425/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.3471 - val_loss: 21.2904\n",
      "Epoch 426/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.9226 - val_loss: 22.7813\n",
      "Epoch 427/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.1411 - val_loss: 22.3848\n",
      "Epoch 428/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.0551 - val_loss: 21.6681\n",
      "Epoch 429/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.9220 - val_loss: 23.4384\n",
      "Epoch 430/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.6576 - val_loss: 21.6013\n",
      "Epoch 431/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.5494 - val_loss: 22.7885\n",
      "Epoch 432/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.9087 - val_loss: 25.6707\n",
      "Epoch 433/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.9583 - val_loss: 23.6088\n",
      "Epoch 434/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.9350 - val_loss: 22.2304\n",
      "Epoch 435/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.9687 - val_loss: 23.4843\n",
      "Epoch 436/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.5926 - val_loss: 23.9476\n",
      "Epoch 437/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.9304 - val_loss: 24.8242\n",
      "Epoch 438/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.4856 - val_loss: 22.1217\n",
      "Epoch 439/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.1557 - val_loss: 22.4728\n",
      "Epoch 440/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.0602 - val_loss: 26.3089\n",
      "Epoch 441/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.7344 - val_loss: 25.1793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 442/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.7158 - val_loss: 21.5382\n",
      "Epoch 443/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.0702 - val_loss: 23.0680\n",
      "Epoch 444/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.7159 - val_loss: 22.7241\n",
      "Epoch 445/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 17.2744 - val_loss: 22.9303\n",
      "Epoch 446/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.7386 - val_loss: 21.5344\n",
      "Epoch 447/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.4018 - val_loss: 22.5550\n",
      "Epoch 448/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.6803 - val_loss: 22.7156\n",
      "Epoch 449/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.2034 - val_loss: 22.2263\n",
      "Epoch 450/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.2984 - val_loss: 22.2289\n",
      "Epoch 451/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.8830 - val_loss: 24.6083\n",
      "Epoch 452/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.1653 - val_loss: 22.8676\n",
      "Epoch 453/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.4716 - val_loss: 21.7754\n",
      "Epoch 454/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.1165 - val_loss: 21.7636\n",
      "Epoch 455/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.7761 - val_loss: 24.1215\n",
      "Epoch 456/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.3781 - val_loss: 21.5567\n",
      "Epoch 457/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.2867 - val_loss: 23.7952\n",
      "Epoch 458/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.5907 - val_loss: 26.1178\n",
      "Epoch 459/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.2502 - val_loss: 24.4788\n",
      "Epoch 460/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.9019 - val_loss: 23.8462\n",
      "Epoch 461/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.6709 - val_loss: 21.6354\n",
      "Epoch 462/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.2622 - val_loss: 24.4038\n",
      "Epoch 463/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.8196 - val_loss: 24.5111\n",
      "Epoch 464/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.7618 - val_loss: 23.3068\n",
      "Epoch 465/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.9540 - val_loss: 22.5402\n",
      "Epoch 466/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.7857 - val_loss: 21.4752\n",
      "Epoch 467/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.4133 - val_loss: 23.3784\n",
      "Epoch 468/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.1084 - val_loss: 22.3716\n",
      "Epoch 469/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.6321 - val_loss: 26.2754\n",
      "Epoch 470/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.6539 - val_loss: 21.4230\n",
      "Epoch 471/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.8318 - val_loss: 21.6060\n",
      "Epoch 472/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.4724 - val_loss: 21.3054\n",
      "Epoch 473/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.3990 - val_loss: 22.9226\n",
      "Epoch 474/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.3581 - val_loss: 22.6192\n",
      "Epoch 475/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.8318 - val_loss: 23.9927\n",
      "Epoch 476/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.5508 - val_loss: 24.3353\n",
      "Epoch 477/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 18.0043 - val_loss: 21.9243\n",
      "Epoch 478/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.6314 - val_loss: 22.0330\n",
      "Epoch 479/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.1626 - val_loss: 22.2241\n",
      "Epoch 480/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.4283 - val_loss: 24.7035\n",
      "Epoch 481/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.5299 - val_loss: 21.8356\n",
      "Epoch 482/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.6807 - val_loss: 23.4363\n",
      "Epoch 483/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.6373 - val_loss: 23.2570\n",
      "Epoch 484/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.6564 - val_loss: 22.9143\n",
      "Epoch 485/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.8885 - val_loss: 24.1003\n",
      "Epoch 486/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.5517 - val_loss: 22.9248\n",
      "Epoch 487/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.9798 - val_loss: 25.3876\n",
      "Epoch 488/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.4505 - val_loss: 23.4873\n",
      "Epoch 489/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.0067 - val_loss: 26.8519\n",
      "Epoch 490/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.8918 - val_loss: 23.2372\n",
      "Epoch 491/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.0797 - val_loss: 26.2725\n",
      "Epoch 492/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.5476 - val_loss: 23.8746\n",
      "Epoch 493/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.2720 - val_loss: 22.6325\n",
      "Epoch 494/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.3336 - val_loss: 23.9426\n",
      "Epoch 495/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.4721 - val_loss: 23.5528\n",
      "Epoch 496/2000\n",
      "1500/1500 [==============================] - 0s 30us/sample - loss: 16.9955 - val_loss: 22.0910\n",
      "Epoch 497/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.2917 - val_loss: 22.6034\n",
      "Epoch 498/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.2202 - val_loss: 21.7122\n",
      "Epoch 499/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.0482 - val_loss: 24.1083\n",
      "Epoch 500/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.0798 - val_loss: 27.7375\n",
      "Epoch 501/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.9324 - val_loss: 22.6318\n",
      "Epoch 502/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.2287 - val_loss: 23.1573\n",
      "Epoch 503/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.6553 - val_loss: 22.1997\n",
      "Epoch 504/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.0836 - val_loss: 22.1726\n",
      "Epoch 505/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.9931 - val_loss: 21.7802\n",
      "Epoch 506/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.0137 - val_loss: 22.9053\n",
      "Epoch 507/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.8323 - val_loss: 24.3131\n",
      "Epoch 508/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.2272 - val_loss: 23.5905\n",
      "Epoch 509/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.1100 - val_loss: 21.2147\n",
      "Epoch 510/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.2699 - val_loss: 23.2022\n",
      "Epoch 511/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.8875 - val_loss: 24.8077\n",
      "Epoch 512/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.7936 - val_loss: 25.5307\n",
      "Epoch 513/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.1539 - val_loss: 23.2033\n",
      "Epoch 514/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.4705 - val_loss: 22.9877\n",
      "Epoch 515/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 15.8648 - val_loss: 23.2188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 516/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.5048 - val_loss: 22.1612\n",
      "Epoch 517/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.0622 - val_loss: 23.0484\n",
      "Epoch 518/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.4207 - val_loss: 22.6149\n",
      "Epoch 519/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.0245 - val_loss: 24.4415\n",
      "Epoch 520/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.6549 - val_loss: 22.0725\n",
      "Epoch 521/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.7285 - val_loss: 23.0069\n",
      "Epoch 522/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 15.9549 - val_loss: 21.8670\n",
      "Epoch 523/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.0525 - val_loss: 24.6097\n",
      "Epoch 524/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.9480 - val_loss: 21.8928\n",
      "Epoch 525/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.7578 - val_loss: 22.2179\n",
      "Epoch 526/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.4289 - val_loss: 23.2778\n",
      "Epoch 527/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.6820 - val_loss: 22.3189\n",
      "Epoch 528/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.6183 - val_loss: 22.8938\n",
      "Epoch 529/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.0732 - val_loss: 22.7494\n",
      "Epoch 530/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.0252 - val_loss: 22.0033\n",
      "Epoch 531/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.9654 - val_loss: 22.4773\n",
      "Epoch 532/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.0169 - val_loss: 23.9206\n",
      "Epoch 533/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.9258 - val_loss: 22.4641\n",
      "Epoch 534/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.3798 - val_loss: 22.6183\n",
      "Epoch 535/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 16.5011 - val_loss: 23.4309\n",
      "Epoch 536/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.1505 - val_loss: 22.5808\n",
      "Epoch 537/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 15.9283 - val_loss: 21.8894\n",
      "Epoch 538/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.1290 - val_loss: 25.5835\n",
      "Epoch 539/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 19.5158 - val_loss: 25.8802\n",
      "Epoch 540/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.5177 - val_loss: 21.8742\n",
      "Epoch 541/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.4634 - val_loss: 22.4049\n",
      "Epoch 542/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.0898 - val_loss: 23.4971\n",
      "Epoch 543/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.7597 - val_loss: 24.1041\n",
      "Epoch 544/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.5031 - val_loss: 22.0816\n",
      "Epoch 545/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.6345 - val_loss: 24.1330\n",
      "Epoch 546/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.4646 - val_loss: 22.1505\n",
      "Epoch 547/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 16.3731 - val_loss: 22.8447\n",
      "Epoch 548/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.2835 - val_loss: 23.4393\n",
      "Epoch 549/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.2303 - val_loss: 23.1735\n",
      "Epoch 550/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.5765 - val_loss: 22.0893\n",
      "Epoch 551/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.7875 - val_loss: 22.4995\n",
      "Epoch 552/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.4560 - val_loss: 22.0341\n",
      "Epoch 553/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 15.2610 - val_loss: 22.3894\n",
      "Epoch 554/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.9328 - val_loss: 22.0510\n",
      "Epoch 555/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.8872 - val_loss: 22.4244\n",
      "Epoch 556/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.2672 - val_loss: 23.1229\n",
      "Epoch 557/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.1929 - val_loss: 22.6087\n",
      "Epoch 558/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.7399 - val_loss: 21.5500\n",
      "Epoch 559/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.5138 - val_loss: 22.4454\n",
      "Epoch 560/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.0026 - val_loss: 21.8670\n",
      "Epoch 561/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.4829 - val_loss: 23.2809\n",
      "Epoch 562/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.3776 - val_loss: 21.6803\n",
      "Epoch 563/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.1392 - val_loss: 22.6143\n",
      "Epoch 564/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 15.9725 - val_loss: 22.0923\n",
      "Epoch 565/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.5565 - val_loss: 22.0228\n",
      "Epoch 566/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.8288 - val_loss: 23.0631\n",
      "Epoch 567/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.2802 - val_loss: 23.4758\n",
      "Epoch 568/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.2442 - val_loss: 21.8046\n",
      "Epoch 569/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.7805 - val_loss: 22.6067\n",
      "Epoch 570/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.8910 - val_loss: 22.4737\n",
      "Epoch 571/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.0465 - val_loss: 24.4963\n",
      "Epoch 572/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.9315 - val_loss: 22.3706\n",
      "Epoch 573/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.2468 - val_loss: 22.0266\n",
      "Epoch 574/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.0708 - val_loss: 24.3566\n",
      "Epoch 575/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.2277 - val_loss: 26.0465\n",
      "Epoch 576/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.6473 - val_loss: 23.3378\n",
      "Epoch 577/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.4911 - val_loss: 21.5468\n",
      "Epoch 578/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.2323 - val_loss: 21.2641\n",
      "Epoch 579/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 15.3797 - val_loss: 22.4441\n",
      "Epoch 580/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 16.0595 - val_loss: 24.0983\n",
      "Epoch 581/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.7449 - val_loss: 24.7494\n",
      "Epoch 582/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.7944 - val_loss: 22.3878\n",
      "Epoch 583/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.7289 - val_loss: 23.3609\n",
      "Epoch 584/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.8052 - val_loss: 21.8891\n",
      "Epoch 585/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.8012 - val_loss: 22.0404\n",
      "Epoch 586/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.3012 - val_loss: 24.2088\n",
      "Epoch 587/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.8392 - val_loss: 23.7940\n",
      "Epoch 588/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 15.3920 - val_loss: 23.0804\n",
      "Epoch 589/2000\n",
      "1500/1500 [==============================] - 0s 30us/sample - loss: 15.5525 - val_loss: 24.4279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 590/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 15.2729 - val_loss: 21.6356\n",
      "Epoch 591/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 15.8034 - val_loss: 23.4100\n",
      "Epoch 592/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.8860 - val_loss: 21.4987\n",
      "Epoch 593/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.5524 - val_loss: 21.6957\n",
      "Epoch 594/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.4669 - val_loss: 23.1313\n",
      "Epoch 595/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.0241 - val_loss: 22.6733\n",
      "Epoch 596/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.5117 - val_loss: 21.6631\n",
      "Epoch 597/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.0810 - val_loss: 22.9193\n",
      "Epoch 598/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 16.3726 - val_loss: 22.2696\n",
      "Epoch 599/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.2338 - val_loss: 23.2768\n",
      "Epoch 600/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.0852 - val_loss: 22.0648\n",
      "Epoch 601/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.7420 - val_loss: 23.5358\n",
      "Epoch 602/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.1137 - val_loss: 22.8675\n",
      "Epoch 603/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.6094 - val_loss: 21.9814\n",
      "Epoch 604/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.8608 - val_loss: 21.7913\n",
      "Epoch 605/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.7428 - val_loss: 22.8867\n",
      "Epoch 606/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.8021 - val_loss: 23.5643\n",
      "Epoch 607/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.4248 - val_loss: 23.4379\n",
      "Epoch 608/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.2990 - val_loss: 23.2808\n",
      "Epoch 609/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.0028 - val_loss: 21.7344\n",
      "Epoch 610/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 17.9102 - val_loss: 24.4862\n",
      "Epoch 611/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 18.2692 - val_loss: 22.3029\n",
      "Epoch 612/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.5843 - val_loss: 23.0365\n",
      "Epoch 613/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.0473 - val_loss: 24.0449\n",
      "Epoch 614/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.0936 - val_loss: 22.2270\n",
      "Epoch 615/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.6597 - val_loss: 21.7133\n",
      "Epoch 616/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 15.5979 - val_loss: 21.5726\n",
      "Epoch 617/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 15.6305 - val_loss: 23.6799\n",
      "Epoch 618/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.0001 - val_loss: 22.4210\n",
      "Epoch 619/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.7803 - val_loss: 21.8622\n",
      "Epoch 620/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.1756 - val_loss: 21.7818\n",
      "Epoch 621/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 15.5620 - val_loss: 23.6425\n",
      "Epoch 622/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.1032 - val_loss: 22.7694\n",
      "Epoch 623/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.8784 - val_loss: 24.8458\n",
      "Epoch 624/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.1207 - val_loss: 23.2599\n",
      "Epoch 625/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.4210 - val_loss: 22.6340\n",
      "Epoch 626/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.1265 - val_loss: 22.4312\n",
      "Epoch 627/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 15.7606 - val_loss: 24.2721\n",
      "Epoch 628/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 15.8599 - val_loss: 22.1569\n",
      "Epoch 629/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.3479 - val_loss: 21.5604\n",
      "Epoch 630/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.5386 - val_loss: 24.1442\n",
      "Epoch 631/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.6527 - val_loss: 24.4365\n",
      "Epoch 632/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.1291 - val_loss: 23.4085\n",
      "Epoch 633/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.4103 - val_loss: 22.7666\n",
      "Epoch 634/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.9276 - val_loss: 21.0660\n",
      "Epoch 635/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.5845 - val_loss: 25.6590\n",
      "Epoch 636/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.2865 - val_loss: 25.9334\n",
      "Epoch 637/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.1936 - val_loss: 23.1867\n",
      "Epoch 638/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.6093 - val_loss: 22.3911\n",
      "Epoch 639/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.7871 - val_loss: 22.5093\n",
      "Epoch 640/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.3325 - val_loss: 22.7031\n",
      "Epoch 641/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.9624 - val_loss: 21.6642\n",
      "Epoch 642/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 15.6840 - val_loss: 23.4600\n",
      "Epoch 643/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 15.6582 - val_loss: 23.1969\n",
      "Epoch 644/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.8589 - val_loss: 21.9282\n",
      "Epoch 645/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.5486 - val_loss: 22.5310\n",
      "Epoch 646/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.1654 - val_loss: 22.4113\n",
      "Epoch 647/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 15.6433 - val_loss: 21.2241\n",
      "Epoch 648/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 15.6057 - val_loss: 21.8344\n",
      "Epoch 649/2000\n",
      "1500/1500 [==============================] - 0s 30us/sample - loss: 16.1623 - val_loss: 23.5433\n",
      "Epoch 650/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.7567 - val_loss: 23.8801\n",
      "Epoch 651/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.5842 - val_loss: 23.0249\n",
      "Epoch 652/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.5833 - val_loss: 21.7842\n",
      "Epoch 653/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.8422 - val_loss: 22.1503\n",
      "Epoch 654/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.3117 - val_loss: 23.0501\n",
      "Epoch 655/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.6874 - val_loss: 22.0269\n",
      "Epoch 656/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.2507 - val_loss: 25.2608\n",
      "Epoch 657/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.5405 - val_loss: 24.1054\n",
      "Epoch 658/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.6637 - val_loss: 25.4728\n",
      "Epoch 659/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.7553 - val_loss: 21.9041\n",
      "Epoch 660/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 15.6429 - val_loss: 21.5275\n",
      "Epoch 661/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.5134 - val_loss: 21.3756\n",
      "Epoch 662/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 15.3842 - val_loss: 21.7364\n",
      "Epoch 663/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.1208 - val_loss: 21.9911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 664/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 16.2583 - val_loss: 22.0855\n",
      "Epoch 665/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.7568 - val_loss: 22.4965\n",
      "Epoch 666/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 15.5548 - val_loss: 21.7792\n",
      "Epoch 667/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 15.1628 - val_loss: 22.6399\n",
      "Epoch 668/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.1935 - val_loss: 21.9254\n",
      "Epoch 669/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 15.4065 - val_loss: 22.9134\n",
      "Epoch 670/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.2035 - val_loss: 22.3856\n",
      "Epoch 671/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.2321 - val_loss: 22.1836\n",
      "Epoch 672/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.0628 - val_loss: 22.0850\n",
      "Epoch 673/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.0143 - val_loss: 22.9407\n",
      "Epoch 674/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.2054 - val_loss: 22.7991\n",
      "Epoch 675/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.7596 - val_loss: 24.1552\n",
      "Epoch 676/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.3799 - val_loss: 22.5661\n",
      "Epoch 677/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.9554 - val_loss: 23.4218\n",
      "Epoch 678/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 15.6977 - val_loss: 21.5166\n",
      "Epoch 679/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.3623 - val_loss: 25.0108\n",
      "Epoch 680/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.1058 - val_loss: 22.8090\n",
      "Epoch 681/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.6504 - val_loss: 24.3816\n",
      "Epoch 682/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.6518 - val_loss: 22.1770\n",
      "Epoch 683/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.1682 - val_loss: 22.3703\n",
      "Epoch 684/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.1902 - val_loss: 23.8678\n",
      "Epoch 685/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.7877 - val_loss: 21.4029\n",
      "Epoch 686/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.0225 - val_loss: 22.9737\n",
      "Epoch 687/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.1355 - val_loss: 23.2063\n",
      "Epoch 688/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.8293 - val_loss: 22.2238\n",
      "Epoch 689/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.8293 - val_loss: 21.8814\n",
      "Epoch 690/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.7076 - val_loss: 22.0844\n",
      "Epoch 691/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 15.0635 - val_loss: 22.7077\n",
      "Epoch 692/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.1544 - val_loss: 22.1280\n",
      "Epoch 693/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.3099 - val_loss: 21.4288\n",
      "Epoch 694/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.3111 - val_loss: 23.1022\n",
      "Epoch 695/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.5261 - val_loss: 22.4455\n",
      "Epoch 696/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.1007 - val_loss: 21.0960\n",
      "Epoch 697/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 15.3642 - val_loss: 22.1870\n",
      "Epoch 698/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 15.1621 - val_loss: 23.0890\n",
      "Epoch 699/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 15.7914 - val_loss: 25.3959\n",
      "Epoch 700/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 15.1010 - val_loss: 21.5442\n",
      "Epoch 701/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 15.0329 - val_loss: 20.8038\n",
      "Epoch 702/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 14.8889 - val_loss: 22.2001\n",
      "Epoch 703/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 14.8446 - val_loss: 22.8597\n",
      "Epoch 704/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 14.9577 - val_loss: 23.1097\n",
      "Epoch 705/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.5705 - val_loss: 23.0688\n",
      "Epoch 706/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.2070 - val_loss: 21.7528\n",
      "Epoch 707/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.8181 - val_loss: 23.1953\n",
      "Epoch 708/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.3303 - val_loss: 22.3774\n",
      "Epoch 709/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.4936 - val_loss: 22.8560\n",
      "Epoch 710/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.1282 - val_loss: 22.8642\n",
      "Epoch 711/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 15.2134 - val_loss: 22.0788\n",
      "Epoch 712/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.9723 - val_loss: 22.4098\n",
      "Epoch 713/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.2332 - val_loss: 24.5474\n",
      "Epoch 714/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 15.0387 - val_loss: 21.8860\n",
      "Epoch 715/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 15.1639 - val_loss: 24.0898\n",
      "Epoch 716/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.8423 - val_loss: 21.6852\n",
      "Epoch 717/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.5256 - val_loss: 21.3322\n",
      "Epoch 718/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.1176 - val_loss: 25.0698\n",
      "Epoch 719/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.3588 - val_loss: 23.4394\n",
      "Epoch 720/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.5254 - val_loss: 22.2727\n",
      "Epoch 721/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.8291 - val_loss: 22.2409\n",
      "Epoch 722/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.2498 - val_loss: 21.3448\n",
      "Epoch 723/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.1083 - val_loss: 24.4942\n",
      "Epoch 724/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.7234 - val_loss: 23.2478\n",
      "Epoch 725/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.0688 - val_loss: 21.4233\n",
      "Epoch 726/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 15.3862 - val_loss: 23.6132\n",
      "Epoch 727/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 15.2134 - val_loss: 23.5142\n",
      "Epoch 728/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.0317 - val_loss: 21.4034\n",
      "Epoch 729/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.7090 - val_loss: 25.9795\n",
      "Epoch 730/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.9702 - val_loss: 22.8697\n",
      "Epoch 731/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.0712 - val_loss: 22.0481\n",
      "Epoch 732/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.1902 - val_loss: 23.7684\n",
      "Epoch 733/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 16.6475 - val_loss: 24.7488\n",
      "Epoch 734/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.0034 - val_loss: 22.1208\n",
      "Epoch 735/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.0480 - val_loss: 21.3997\n",
      "Epoch 736/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.4833 - val_loss: 24.1446\n",
      "Epoch 737/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 14.7971 - val_loss: 24.1882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 738/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 14.9777 - val_loss: 22.7994\n",
      "Epoch 739/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 15.4416 - val_loss: 21.6872\n",
      "Epoch 740/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 14.8564 - val_loss: 22.8107\n",
      "Epoch 741/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 14.4667 - val_loss: 21.1456\n",
      "Epoch 742/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.0593 - val_loss: 24.3991\n",
      "Epoch 743/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.0705 - val_loss: 21.3344\n",
      "Epoch 744/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 15.3718 - val_loss: 23.7666\n",
      "Epoch 745/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 18.9745 - val_loss: 23.1271\n",
      "Epoch 746/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.3513 - val_loss: 21.8327\n",
      "Epoch 747/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.5717 - val_loss: 24.0159\n",
      "Epoch 748/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 15.5476 - val_loss: 22.2860\n",
      "Epoch 749/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 14.9691 - val_loss: 21.1670\n",
      "Epoch 750/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 14.4841 - val_loss: 21.1259\n",
      "Epoch 751/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 15.5559 - val_loss: 22.4414\n",
      "Epoch 752/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 14.6013 - val_loss: 23.7140\n",
      "Epoch 753/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 15.1410 - val_loss: 24.1971\n",
      "Epoch 754/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.0894 - val_loss: 22.7221\n",
      "Epoch 755/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.6323 - val_loss: 22.2154\n",
      "Epoch 756/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 14.7623 - val_loss: 22.7205\n",
      "Epoch 757/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.4447 - val_loss: 24.5118\n",
      "Epoch 758/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.8797 - val_loss: 23.6675\n",
      "Epoch 759/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 17.1647 - val_loss: 22.5875\n",
      "Epoch 760/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 14.9853 - val_loss: 21.9949\n",
      "Epoch 761/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 15.3428 - val_loss: 21.0235\n",
      "Epoch 762/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 14.4792 - val_loss: 22.6268\n",
      "Epoch 763/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 14.3985 - val_loss: 22.2066\n",
      "Epoch 764/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.0895 - val_loss: 23.1763\n",
      "Epoch 765/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.1679 - val_loss: 22.2273\n",
      "Epoch 766/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 15.2399 - val_loss: 22.7305\n",
      "Epoch 767/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.2946 - val_loss: 24.6613\n",
      "Epoch 768/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.0666 - val_loss: 21.4110\n",
      "Epoch 769/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.2215 - val_loss: 23.6919\n",
      "Epoch 770/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.0441 - val_loss: 22.1057\n",
      "Epoch 771/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.2821 - val_loss: 21.6556\n",
      "Epoch 772/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 16.4456 - val_loss: 23.9293\n",
      "Epoch 773/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.8561 - val_loss: 22.6155\n",
      "Epoch 774/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.9413 - val_loss: 21.6626\n",
      "Epoch 775/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 15.2367 - val_loss: 24.1424\n",
      "Epoch 776/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.7562 - val_loss: 22.0425\n",
      "Epoch 777/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 15.3368 - val_loss: 21.3028\n",
      "Epoch 778/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 14.6735 - val_loss: 22.3328\n",
      "Epoch 779/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.3093 - val_loss: 22.2370\n",
      "Epoch 780/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 14.9454 - val_loss: 21.1785\n",
      "Epoch 781/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 14.3533 - val_loss: 22.4709\n",
      "Epoch 782/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 14.6566 - val_loss: 25.5638\n",
      "Epoch 783/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.4694 - val_loss: 22.3937\n",
      "Epoch 784/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 14.6107 - val_loss: 21.7872\n",
      "Epoch 785/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 15.4739 - val_loss: 22.5522\n",
      "Epoch 786/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 14.4956 - val_loss: 21.8274\n",
      "Epoch 787/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 14.6947 - val_loss: 24.9375\n",
      "Epoch 788/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 15.5136 - val_loss: 21.7006\n",
      "Epoch 789/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.6227 - val_loss: 22.9525\n",
      "Epoch 790/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 15.0725 - val_loss: 22.3296\n",
      "Epoch 791/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.9073 - val_loss: 24.4359\n",
      "Epoch 792/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.2910 - val_loss: 22.9676\n",
      "Epoch 793/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 15.1569 - val_loss: 20.7928\n",
      "Epoch 794/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 14.3250 - val_loss: 23.1607\n",
      "Epoch 795/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.1054 - val_loss: 21.8537\n",
      "Epoch 796/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 14.7838 - val_loss: 22.0914\n",
      "Epoch 797/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.1899 - val_loss: 21.3469\n",
      "Epoch 798/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 15.7471 - val_loss: 23.7138\n",
      "Epoch 799/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 15.2958 - val_loss: 21.8729\n",
      "Epoch 800/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 15.0021 - val_loss: 22.7069\n",
      "Epoch 801/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 14.4572 - val_loss: 23.3702\n",
      "Epoch 802/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 15.8210 - val_loss: 21.7198\n",
      "Epoch 803/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.4946 - val_loss: 21.8984\n",
      "Epoch 804/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.8063 - val_loss: 21.2356\n",
      "Epoch 805/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.0261 - val_loss: 22.6035\n",
      "Epoch 806/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 14.4552 - val_loss: 22.3991\n",
      "Epoch 807/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 14.4528 - val_loss: 22.2145\n",
      "Epoch 808/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 14.6087 - val_loss: 23.3645\n",
      "Epoch 809/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.1136 - val_loss: 22.2568\n",
      "Epoch 810/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 15.1485 - val_loss: 22.2330\n",
      "Epoch 811/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 14.7660 - val_loss: 22.7303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 812/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.2160 - val_loss: 21.6535\n",
      "Epoch 813/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 14.7620 - val_loss: 21.7871\n",
      "Epoch 814/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 14.4038 - val_loss: 22.3391\n",
      "Epoch 815/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 14.1183 - val_loss: 21.9895\n",
      "Epoch 816/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 14.6084 - val_loss: 22.1337\n",
      "Epoch 817/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.7238 - val_loss: 22.0942\n",
      "Epoch 818/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 14.7300 - val_loss: 23.1798\n",
      "Epoch 819/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.2891 - val_loss: 23.6710\n",
      "Epoch 820/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.5607 - val_loss: 25.1774\n",
      "Epoch 821/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.8277 - val_loss: 22.4733\n",
      "Epoch 822/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 15.2567 - val_loss: 23.1450\n",
      "Epoch 823/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 15.1880 - val_loss: 21.9374\n",
      "Epoch 824/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 14.7044 - val_loss: 22.0107\n",
      "Epoch 825/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.0691 - val_loss: 21.9127\n",
      "Epoch 826/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 14.8648 - val_loss: 21.2509\n",
      "Epoch 827/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 15.1336 - val_loss: 25.1441\n",
      "Epoch 828/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 14.8177 - val_loss: 22.7252\n",
      "Epoch 829/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 14.8888 - val_loss: 23.0110\n",
      "Epoch 830/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 14.7365 - val_loss: 21.2618\n",
      "Epoch 831/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 14.1567 - val_loss: 24.5489\n",
      "Epoch 832/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.3495 - val_loss: 22.2465\n",
      "Epoch 833/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.5989 - val_loss: 22.7844\n",
      "Epoch 834/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.2284 - val_loss: 22.6883\n",
      "Epoch 835/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.9848 - val_loss: 23.4487\n",
      "Epoch 836/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.2937 - val_loss: 21.8854\n",
      "Epoch 837/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.5284 - val_loss: 21.2294\n",
      "Epoch 838/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.2374 - val_loss: 21.2124\n",
      "Epoch 839/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.3622 - val_loss: 21.6313\n",
      "Epoch 840/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.4568 - val_loss: 22.7146\n",
      "Epoch 841/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 15.6530 - val_loss: 21.1713\n",
      "Epoch 842/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 13.8956 - val_loss: 22.5973\n",
      "Epoch 843/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 14.5228 - val_loss: 21.8880\n",
      "Epoch 844/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 14.6468 - val_loss: 24.3092\n",
      "Epoch 845/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 15.3975 - val_loss: 22.4867\n",
      "Epoch 846/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 15.3395 - val_loss: 23.2663\n",
      "Epoch 847/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 14.3785 - val_loss: 23.5048\n",
      "Epoch 848/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 15.2489 - val_loss: 22.0116\n",
      "Epoch 849/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 15.4082 - val_loss: 22.9468\n",
      "Epoch 850/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.8824 - val_loss: 25.5588\n",
      "Epoch 851/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.7229 - val_loss: 23.2311\n",
      "Epoch 852/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 15.6681 - val_loss: 21.9010\n",
      "Epoch 853/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 14.1759 - val_loss: 21.6234\n",
      "Epoch 854/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 14.0926 - val_loss: 23.5276\n",
      "Epoch 855/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.4578 - val_loss: 22.9953\n",
      "Epoch 856/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.0599 - val_loss: 22.7153\n",
      "Epoch 857/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 14.7402 - val_loss: 21.9751\n",
      "Epoch 858/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 14.3510 - val_loss: 21.6113\n",
      "Epoch 859/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.7135 - val_loss: 28.0236\n",
      "Epoch 860/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.3134 - val_loss: 25.1035\n",
      "Epoch 861/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 14.8606 - val_loss: 22.7351\n",
      "Epoch 862/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 14.5015 - val_loss: 22.0133\n",
      "Epoch 863/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 14.7353 - val_loss: 22.6749\n",
      "Epoch 864/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 14.2489 - val_loss: 23.0188\n",
      "Epoch 865/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 15.5092 - val_loss: 23.1614\n",
      "Epoch 866/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 15.4270 - val_loss: 24.9854\n",
      "Epoch 867/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.1307 - val_loss: 25.2036\n",
      "Epoch 868/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.1097 - val_loss: 24.0930\n",
      "Epoch 869/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.1265 - val_loss: 22.2647\n",
      "Epoch 870/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 14.3032 - val_loss: 24.0538\n",
      "Epoch 871/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 13.9680 - val_loss: 22.8669\n",
      "Epoch 872/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 14.9547 - val_loss: 27.9631\n",
      "Epoch 873/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 17.6656 - val_loss: 23.9676\n",
      "Epoch 874/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.3074 - val_loss: 23.5192\n",
      "Epoch 875/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 15.8729 - val_loss: 23.0059\n",
      "Epoch 876/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.0458 - val_loss: 21.1667\n",
      "Epoch 877/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 14.0705 - val_loss: 22.1813\n",
      "Epoch 878/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 15.1421 - val_loss: 23.6514\n",
      "Epoch 879/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 14.5820 - val_loss: 21.6783\n",
      "Epoch 880/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 13.7433 - val_loss: 21.4168\n",
      "Epoch 881/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 14.3373 - val_loss: 21.7796\n",
      "Epoch 882/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 14.9679 - val_loss: 26.3628\n",
      "Epoch 883/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 15.9970 - val_loss: 23.0551\n",
      "Epoch 884/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 14.9841 - val_loss: 22.3267\n",
      "Epoch 885/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 14.8498 - val_loss: 21.9465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 886/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 14.5924 - val_loss: 23.0473\n",
      "Epoch 887/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 14.2995 - val_loss: 22.1759\n",
      "Epoch 888/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 15.0277 - val_loss: 21.9027\n",
      "Epoch 889/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 14.4872 - val_loss: 22.1430\n",
      "Epoch 890/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 14.6503 - val_loss: 21.9262\n",
      "Epoch 891/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 14.9694 - val_loss: 22.4136\n",
      "Epoch 892/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.0457 - val_loss: 22.2269\n",
      "Epoch 893/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 14.4430 - val_loss: 22.4445\n",
      "Epoch 894/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 14.5704 - val_loss: 23.1809\n",
      "Epoch 895/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 15.6944 - val_loss: 22.6062\n",
      "Epoch 896/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 15.4306 - val_loss: 25.4095\n",
      "Epoch 897/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 15.1874 - val_loss: 22.1593\n",
      "Epoch 898/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.8226 - val_loss: 21.8220\n",
      "Epoch 899/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 14.1922 - val_loss: 22.4127\n",
      "Epoch 900/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 14.7822 - val_loss: 22.4909\n",
      "Epoch 901/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.6998 - val_loss: 22.8716\n",
      "Epoch 902/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 15.4852 - val_loss: 22.2964\n",
      "Epoch 903/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 14.1586 - val_loss: 23.0993\n",
      "Epoch 904/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 14.9914 - val_loss: 21.5094\n",
      "Epoch 905/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 14.5552 - val_loss: 22.5834\n",
      "Epoch 906/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 13.8485 - val_loss: 23.2510\n",
      "Epoch 907/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 14.7702 - val_loss: 22.3418\n",
      "Epoch 908/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 14.5127 - val_loss: 23.4944\n",
      "Epoch 909/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 14.6904 - val_loss: 22.5676\n",
      "Epoch 910/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 14.4631 - val_loss: 24.9338\n",
      "Epoch 911/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 14.7814 - val_loss: 22.4050\n",
      "Epoch 912/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.5543 - val_loss: 24.7142\n",
      "Epoch 913/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.6949 - val_loss: 22.2833\n",
      "Epoch 914/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 15.4967 - val_loss: 22.2767\n",
      "Epoch 915/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 15.3588 - val_loss: 23.4557\n",
      "Epoch 916/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 14.4942 - val_loss: 21.6695\n",
      "Epoch 917/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 14.1129 - val_loss: 21.9596\n",
      "Epoch 918/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 14.3092 - val_loss: 22.5298\n",
      "Epoch 919/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 14.1638 - val_loss: 23.0542\n",
      "Epoch 920/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 14.4402 - val_loss: 22.3401\n",
      "Epoch 921/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 14.6095 - val_loss: 22.3568\n",
      "Epoch 922/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 14.0490 - val_loss: 22.3698\n",
      "Epoch 923/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 14.3760 - val_loss: 22.0231\n",
      "Epoch 924/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 14.1387 - val_loss: 22.0277\n",
      "Epoch 925/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 13.9756 - val_loss: 25.2214\n",
      "Epoch 926/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 15.2163 - val_loss: 23.1517\n",
      "Epoch 927/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.5246 - val_loss: 24.7120\n",
      "Epoch 928/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 15.5576 - val_loss: 21.8806\n",
      "Epoch 929/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.6323 - val_loss: 22.8730\n",
      "Epoch 930/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 14.7077 - val_loss: 22.1507\n",
      "Epoch 931/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 14.2500 - val_loss: 22.2798\n",
      "Epoch 932/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 13.6821 - val_loss: 21.9860\n",
      "Epoch 933/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 13.3867 - val_loss: 24.1774\n",
      "Epoch 934/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 15.0857 - val_loss: 22.0975\n",
      "Epoch 935/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 14.4268 - val_loss: 23.2717\n",
      "Epoch 936/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 13.7689 - val_loss: 22.8703\n",
      "Epoch 937/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 14.7016 - val_loss: 23.8182\n",
      "Epoch 938/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 14.5278 - val_loss: 22.2261\n",
      "Epoch 939/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 14.6943 - val_loss: 23.9681\n",
      "Epoch 940/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.0881 - val_loss: 24.4959\n",
      "Epoch 941/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 14.7382 - val_loss: 22.4601\n",
      "Epoch 942/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.0116 - val_loss: 22.9812\n",
      "Epoch 943/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 14.6389 - val_loss: 23.8843\n",
      "Epoch 944/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 14.9718 - val_loss: 22.5204\n",
      "Epoch 945/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 14.5329 - val_loss: 22.5084\n",
      "Epoch 946/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 14.2704 - val_loss: 21.6698\n",
      "Epoch 947/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.3457 - val_loss: 22.7235\n",
      "Epoch 948/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 14.2106 - val_loss: 22.1389\n",
      "Epoch 949/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 14.1415 - val_loss: 21.8107\n",
      "Epoch 950/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 15.0489 - val_loss: 25.8230\n",
      "Epoch 951/2000\n",
      "1500/1500 [==============================] - 0s 24us/sample - loss: 14.8626 - val_loss: 24.0983\n",
      "Epoch 952/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 14.5422 - val_loss: 21.8979\n",
      "Epoch 953/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 15.0500 - val_loss: 24.8900\n",
      "Epoch 954/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 15.2073 - val_loss: 21.8744\n",
      "Epoch 955/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 15.1216 - val_loss: 23.6385\n",
      "Epoch 956/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.7956 - val_loss: 22.9353\n",
      "Epoch 957/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 14.9279 - val_loss: 21.9611\n",
      "Epoch 958/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 13.7725 - val_loss: 22.8692\n",
      "Epoch 959/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 14.6121 - val_loss: 23.0112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 960/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.5433 - val_loss: 22.4463\n",
      "Epoch 961/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 14.9343 - val_loss: 22.7987\n",
      "Epoch 962/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 14.3579 - val_loss: 22.5348\n",
      "Epoch 963/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 14.3079 - val_loss: 22.1899\n",
      "Epoch 964/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.5163 - val_loss: 23.4249\n",
      "Epoch 965/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 15.8476 - val_loss: 25.4321\n",
      "Epoch 966/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.3292 - val_loss: 24.4482\n",
      "Epoch 967/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 16.8148 - val_loss: 22.6917\n",
      "Epoch 968/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.9790 - val_loss: 22.8885\n",
      "Epoch 969/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.3200 - val_loss: 22.3038\n",
      "Epoch 970/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.0564 - val_loss: 21.9054\n",
      "Epoch 971/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 14.6812 - val_loss: 23.3988\n",
      "Epoch 972/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 13.9511 - val_loss: 22.3328\n",
      "Epoch 973/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 14.6011 - val_loss: 22.3495\n",
      "Epoch 974/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 14.3662 - val_loss: 22.9240\n",
      "Epoch 975/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 14.6039 - val_loss: 22.4042\n",
      "Epoch 976/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 14.3351 - val_loss: 24.1564\n",
      "Epoch 977/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 14.3256 - val_loss: 21.2954\n",
      "Epoch 978/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 14.3047 - val_loss: 22.0349\n",
      "Epoch 979/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 13.5575 - val_loss: 22.1412\n",
      "Epoch 980/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 14.1611 - val_loss: 22.0081\n",
      "Epoch 981/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 14.8074 - val_loss: 23.0999\n",
      "Epoch 982/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 14.5304 - val_loss: 22.3733\n",
      "Epoch 983/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 15.4156 - val_loss: 23.6721\n",
      "Epoch 984/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.0160 - val_loss: 22.4082\n",
      "Epoch 985/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 13.8629 - val_loss: 21.6614\n",
      "Epoch 986/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 14.3011 - val_loss: 24.5651\n",
      "Epoch 987/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 14.4735 - val_loss: 22.0650\n",
      "Epoch 988/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 13.4855 - val_loss: 21.6650\n",
      "Epoch 989/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 14.0931 - val_loss: 23.4634\n",
      "Epoch 990/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 13.9852 - val_loss: 21.4201\n",
      "Epoch 991/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 15.5306 - val_loss: 23.0273\n",
      "Epoch 992/2000\n",
      "1500/1500 [==============================] - 0s 25us/sample - loss: 14.0237 - val_loss: 22.1673\n",
      "Epoch 993/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 14.9145 - val_loss: 23.4369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|████████████████████████████████████████████████████████████████████████████▉     | 15/16 [08:11<00:32, 32.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_layer': 'relu', 'batc_normalization': False, 'batch_size': 64, 'dropout': 0, 'epochs': 2000, 'first_neuron': 320, 'hidden_layers': 2, 'hidden_neuron': 100, 'kernel_initializer': 'normal', 'last_activation': 'linear', 'optimizer': 'adam'}\n",
      "adding layer 1\n",
      "adding layer 2\n",
      "Train on 1500 samples, validate on 375 samples\n",
      "Epoch 1/2000\n",
      "1500/1500 [==============================] - 0s 249us/sample - loss: 2185.0466 - val_loss: 2181.8338\n",
      "Epoch 2/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 2156.8171 - val_loss: 2097.0280\n",
      "Epoch 3/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 1903.9121 - val_loss: 1530.4841\n",
      "Epoch 4/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 950.1674 - val_loss: 579.7852\n",
      "Epoch 5/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 553.5173 - val_loss: 461.0860\n",
      "Epoch 6/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 425.4420 - val_loss: 340.5225\n",
      "Epoch 7/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 317.2555 - val_loss: 252.0051\n",
      "Epoch 8/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 227.6426 - val_loss: 189.2215\n",
      "Epoch 9/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 165.0267 - val_loss: 139.3716\n",
      "Epoch 10/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 123.8921 - val_loss: 110.6995\n",
      "Epoch 11/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 97.1504 - val_loss: 83.2609\n",
      "Epoch 12/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 73.9301 - val_loss: 67.5445\n",
      "Epoch 13/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 60.2854 - val_loss: 60.8118\n",
      "Epoch 14/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 49.8674 - val_loss: 47.1005\n",
      "Epoch 15/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 42.7192 - val_loss: 42.2983\n",
      "Epoch 16/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 40.5329 - val_loss: 37.6617\n",
      "Epoch 17/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 34.4560 - val_loss: 34.1465\n",
      "Epoch 18/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 31.8431 - val_loss: 34.0700\n",
      "Epoch 19/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 30.9532 - val_loss: 29.5420\n",
      "Epoch 20/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 29.1838 - val_loss: 28.8752\n",
      "Epoch 21/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 28.2356 - val_loss: 25.5992\n",
      "Epoch 22/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 25.6765 - val_loss: 27.4307\n",
      "Epoch 23/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 26.8014 - val_loss: 26.6665\n",
      "Epoch 24/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 27.1074 - val_loss: 30.8565\n",
      "Epoch 25/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 28.6198 - val_loss: 25.9138\n",
      "Epoch 26/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 25.1214 - val_loss: 25.0168\n",
      "Epoch 27/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 25.0087 - val_loss: 28.2474\n",
      "Epoch 28/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 27.1516 - val_loss: 25.5018\n",
      "Epoch 29/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 24.4386 - val_loss: 26.2756\n",
      "Epoch 30/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 25.0064 - val_loss: 27.5474\n",
      "Epoch 31/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 24.8010 - val_loss: 28.3656\n",
      "Epoch 32/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 24.3564 - val_loss: 25.6254\n",
      "Epoch 33/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 26.5953 - val_loss: 25.2953\n",
      "Epoch 34/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 24.3567 - val_loss: 24.4626\n",
      "Epoch 35/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 23.6941 - val_loss: 24.1598\n",
      "Epoch 36/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 23.6393 - val_loss: 33.0100\n",
      "Epoch 37/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 25.2361 - val_loss: 26.7514\n",
      "Epoch 38/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 24.2456 - val_loss: 26.7234\n",
      "Epoch 39/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 22.7566 - val_loss: 24.8408\n",
      "Epoch 40/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 22.2881 - val_loss: 23.2696\n",
      "Epoch 41/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 22.1729 - val_loss: 25.7601\n",
      "Epoch 42/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 23.3848 - val_loss: 25.4814\n",
      "Epoch 43/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 24.5485 - val_loss: 25.0073\n",
      "Epoch 44/2000\n",
      "1500/1500 [==============================] - 0s 33us/sample - loss: 24.2735 - val_loss: 23.8082\n",
      "Epoch 45/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 23.3661 - val_loss: 25.6667\n",
      "Epoch 46/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 22.9306 - val_loss: 24.4449\n",
      "Epoch 47/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 22.2388 - val_loss: 24.5781\n",
      "Epoch 48/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 23.2445 - val_loss: 29.2360\n",
      "Epoch 49/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 23.8052 - val_loss: 25.5575\n",
      "Epoch 50/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 24.7071 - val_loss: 24.9127\n",
      "Epoch 51/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 23.7120 - val_loss: 25.0630\n",
      "Epoch 52/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 22.2936 - val_loss: 23.2917\n",
      "Epoch 53/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 23.5816 - val_loss: 27.7561\n",
      "Epoch 54/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 24.2963 - val_loss: 23.8930\n",
      "Epoch 55/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 24.2030 - val_loss: 23.7741\n",
      "Epoch 56/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 23.0067 - val_loss: 29.4979\n",
      "Epoch 57/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 24.7618 - val_loss: 25.9808\n",
      "Epoch 58/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 24.6973 - val_loss: 27.0558\n",
      "Epoch 59/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 25.0132 - val_loss: 27.9712\n",
      "Epoch 60/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 24.3424 - val_loss: 28.3032\n",
      "Epoch 61/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 23.4017 - val_loss: 26.6769\n",
      "Epoch 62/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 22.8984 - val_loss: 25.5306\n",
      "Epoch 63/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 22.0261 - val_loss: 24.1399\n",
      "Epoch 64/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 21.5834 - val_loss: 23.7477\n",
      "Epoch 65/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 21.9479 - val_loss: 24.8310\n",
      "Epoch 66/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 22.1194 - val_loss: 23.5549\n",
      "Epoch 67/2000\n",
      "1500/1500 [==============================] - 0s 31us/sample - loss: 21.6689 - val_loss: 23.7741\n",
      "Epoch 68/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 22.9688 - val_loss: 26.1013\n",
      "Epoch 69/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 22.3929 - val_loss: 25.5999\n",
      "Epoch 70/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 22.3887 - val_loss: 24.2998\n",
      "Epoch 71/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 21.5878 - val_loss: 24.7477\n",
      "Epoch 72/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 22.9171 - val_loss: 27.5893\n",
      "Epoch 73/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 24.1649 - val_loss: 26.6772\n",
      "Epoch 74/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 21.8947 - val_loss: 24.2954\n",
      "Epoch 75/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 20.9072 - val_loss: 24.5514\n",
      "Epoch 76/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 21.8732 - val_loss: 26.3097\n",
      "Epoch 77/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 21.3446 - val_loss: 25.8740\n",
      "Epoch 78/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 21.2715 - val_loss: 25.5512\n",
      "Epoch 79/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 22.5439 - val_loss: 24.2205\n",
      "Epoch 80/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 21.5863 - val_loss: 23.7177\n",
      "Epoch 81/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 22.7481 - val_loss: 25.5533\n",
      "Epoch 82/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 20.9200 - val_loss: 23.8717\n",
      "Epoch 83/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 20.6168 - val_loss: 23.3164\n",
      "Epoch 84/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 20.6234 - val_loss: 24.5334\n",
      "Epoch 85/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 21.3005 - val_loss: 25.1715\n",
      "Epoch 86/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 22.0968 - val_loss: 23.7265\n",
      "Epoch 87/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 21.6331 - val_loss: 26.3995\n",
      "Epoch 88/2000\n",
      "1500/1500 [==============================] - 0s 31us/sample - loss: 21.3925 - val_loss: 23.3686\n",
      "Epoch 89/2000\n",
      "1500/1500 [==============================] - 0s 30us/sample - loss: 20.5277 - val_loss: 23.1098\n",
      "Epoch 90/2000\n",
      "1500/1500 [==============================] - 0s 31us/sample - loss: 20.5143 - val_loss: 24.1692\n",
      "Epoch 91/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 20.5706 - val_loss: 23.9696\n",
      "Epoch 92/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 21.1362 - val_loss: 28.2763\n",
      "Epoch 93/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 22.3871 - val_loss: 32.3255\n",
      "Epoch 94/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 21.8629 - val_loss: 25.9202\n",
      "Epoch 95/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 21.6264 - val_loss: 25.2865\n",
      "Epoch 96/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 23.7283 - val_loss: 29.1310\n",
      "Epoch 97/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 22.0456 - val_loss: 24.8276\n",
      "Epoch 98/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 20.6029 - val_loss: 25.3314\n",
      "Epoch 99/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 20.4976 - val_loss: 23.9367\n",
      "Epoch 100/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 21.2286 - val_loss: 24.4322\n",
      "Epoch 101/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 21.2810 - val_loss: 24.5298\n",
      "Epoch 102/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 21.6571 - val_loss: 24.6359\n",
      "Epoch 103/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 21.3302 - val_loss: 24.5969\n",
      "Epoch 104/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 21.4912 - val_loss: 25.2376\n",
      "Epoch 105/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 22.2273 - val_loss: 24.3081\n",
      "Epoch 106/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 20.3962 - val_loss: 24.1294\n",
      "Epoch 107/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 21.2697 - val_loss: 24.1819\n",
      "Epoch 108/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 20.8840 - val_loss: 26.5552\n",
      "Epoch 109/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 20.9028 - val_loss: 25.7737\n",
      "Epoch 110/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 21.6588 - val_loss: 25.3448\n",
      "Epoch 111/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 20.6215 - val_loss: 24.4487\n",
      "Epoch 112/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 21.4287 - val_loss: 24.4831\n",
      "Epoch 113/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 23.2233 - val_loss: 24.7516\n",
      "Epoch 114/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 20.6696 - val_loss: 25.5632\n",
      "Epoch 115/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 22.2950 - val_loss: 23.7662\n",
      "Epoch 116/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 21.1407 - val_loss: 28.4376\n",
      "Epoch 117/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 21.1874 - val_loss: 22.9265\n",
      "Epoch 118/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 22.4553 - val_loss: 26.2956\n",
      "Epoch 119/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 20.5074 - val_loss: 23.8584\n",
      "Epoch 120/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 20.3172 - val_loss: 23.5492\n",
      "Epoch 121/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 20.8939 - val_loss: 24.8806\n",
      "Epoch 122/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 20.2650 - val_loss: 23.0445\n",
      "Epoch 123/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 20.5326 - val_loss: 22.8541\n",
      "Epoch 124/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 23.4607 - val_loss: 24.7590\n",
      "Epoch 125/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 22.1510 - val_loss: 27.1404\n",
      "Epoch 126/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 22.2427 - val_loss: 27.8358\n",
      "Epoch 127/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 21.9856 - val_loss: 23.5615\n",
      "Epoch 128/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 20.5595 - val_loss: 22.8509\n",
      "Epoch 129/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 20.3178 - val_loss: 23.8932\n",
      "Epoch 130/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 20.3172 - val_loss: 23.9282\n",
      "Epoch 131/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 21.3954 - val_loss: 23.2288\n",
      "Epoch 132/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 20.6292 - val_loss: 22.9659\n",
      "Epoch 133/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 22.5886 - val_loss: 25.2287\n",
      "Epoch 134/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 22.6727 - val_loss: 25.5034\n",
      "Epoch 135/2000\n",
      "1500/1500 [==============================] - 0s 30us/sample - loss: 21.5948 - val_loss: 23.7309\n",
      "Epoch 136/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 22.1830 - val_loss: 24.3603\n",
      "Epoch 137/2000\n",
      "1500/1500 [==============================] - 0s 33us/sample - loss: 19.7816 - val_loss: 22.1554\n",
      "Epoch 138/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 19.6688 - val_loss: 24.4486\n",
      "Epoch 139/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 21.6581 - val_loss: 34.6483\n",
      "Epoch 140/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 23.9759 - val_loss: 27.8360\n",
      "Epoch 141/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 21.2527 - val_loss: 23.6191\n",
      "Epoch 142/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 20.7323 - val_loss: 23.3403\n",
      "Epoch 143/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 20.0042 - val_loss: 23.4757\n",
      "Epoch 144/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 20.4081 - val_loss: 23.9731\n",
      "Epoch 145/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 20.7230 - val_loss: 23.8427\n",
      "Epoch 146/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 28us/sample - loss: 19.5999 - val_loss: 26.5955\n",
      "Epoch 147/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 20.7816 - val_loss: 25.1778\n",
      "Epoch 148/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 23.1768 - val_loss: 24.2169\n",
      "Epoch 149/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.3874 - val_loss: 25.1470\n",
      "Epoch 150/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 20.1704 - val_loss: 24.9156\n",
      "Epoch 151/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 20.9403 - val_loss: 22.6555\n",
      "Epoch 152/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 20.3970 - val_loss: 23.5977\n",
      "Epoch 153/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 19.0223 - val_loss: 24.0031\n",
      "Epoch 154/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 20.4077 - val_loss: 23.6827\n",
      "Epoch 155/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 20.3209 - val_loss: 26.3237\n",
      "Epoch 156/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 22.0862 - val_loss: 32.6755\n",
      "Epoch 157/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 22.3607 - val_loss: 24.7275\n",
      "Epoch 158/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 21.3408 - val_loss: 25.0400\n",
      "Epoch 159/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 20.5664 - val_loss: 24.3910\n",
      "Epoch 160/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 21.7581 - val_loss: 23.5275\n",
      "Epoch 161/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 22.7308 - val_loss: 23.5486\n",
      "Epoch 162/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 19.8450 - val_loss: 22.7937\n",
      "Epoch 163/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 19.2651 - val_loss: 25.3841\n",
      "Epoch 164/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 19.8119 - val_loss: 22.9503\n",
      "Epoch 165/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 19.4584 - val_loss: 24.6996\n",
      "Epoch 166/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 21.1376 - val_loss: 28.4966\n",
      "Epoch 167/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 23.0749 - val_loss: 28.8801\n",
      "Epoch 168/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 21.3667 - val_loss: 25.3207\n",
      "Epoch 169/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.8595 - val_loss: 23.6357\n",
      "Epoch 170/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 19.6747 - val_loss: 24.1475\n",
      "Epoch 171/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 21.3115 - val_loss: 26.5573\n",
      "Epoch 172/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 21.8286 - val_loss: 25.7737\n",
      "Epoch 173/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.4383 - val_loss: 25.2875\n",
      "Epoch 174/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 19.8017 - val_loss: 23.9244\n",
      "Epoch 175/2000\n",
      "1500/1500 [==============================] - 0s 32us/sample - loss: 18.9741 - val_loss: 23.4239\n",
      "Epoch 176/2000\n",
      "1500/1500 [==============================] - 0s 30us/sample - loss: 18.0095 - val_loss: 24.2231\n",
      "Epoch 177/2000\n",
      "1500/1500 [==============================] - 0s 31us/sample - loss: 20.0372 - val_loss: 25.9998\n",
      "Epoch 178/2000\n",
      "1500/1500 [==============================] - 0s 34us/sample - loss: 19.7955 - val_loss: 25.2899\n",
      "Epoch 179/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 18.3856 - val_loss: 23.2780\n",
      "Epoch 180/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 19.3156 - val_loss: 24.4505\n",
      "Epoch 181/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 20.7895 - val_loss: 25.4216\n",
      "Epoch 182/2000\n",
      "1500/1500 [==============================] - 0s 31us/sample - loss: 19.4873 - val_loss: 22.4404\n",
      "Epoch 183/2000\n",
      "1500/1500 [==============================] - 0s 32us/sample - loss: 19.4624 - val_loss: 23.9339\n",
      "Epoch 184/2000\n",
      "1500/1500 [==============================] - 0s 30us/sample - loss: 19.7642 - val_loss: 22.2247\n",
      "Epoch 185/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 19.2603 - val_loss: 22.9660\n",
      "Epoch 186/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 20.8934 - val_loss: 23.5623\n",
      "Epoch 187/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 19.6612 - val_loss: 22.8458\n",
      "Epoch 188/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 19.6131 - val_loss: 23.8950\n",
      "Epoch 189/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 21.0601 - val_loss: 26.0574\n",
      "Epoch 190/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 20.5107 - val_loss: 25.7168\n",
      "Epoch 191/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.5650 - val_loss: 23.8350\n",
      "Epoch 192/2000\n",
      "1500/1500 [==============================] - 0s 30us/sample - loss: 19.5709 - val_loss: 23.7086\n",
      "Epoch 193/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 19.4428 - val_loss: 23.6566\n",
      "Epoch 194/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 21.7978 - val_loss: 24.2786\n",
      "Epoch 195/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 20.0722 - val_loss: 21.9956\n",
      "Epoch 196/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 18.8331 - val_loss: 24.2707\n",
      "Epoch 197/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 19.2693 - val_loss: 24.4626\n",
      "Epoch 198/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 19.0759 - val_loss: 24.3557\n",
      "Epoch 199/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.7945 - val_loss: 23.4110\n",
      "Epoch 200/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 19.2256 - val_loss: 22.7830\n",
      "Epoch 201/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 19.1337 - val_loss: 22.7192\n",
      "Epoch 202/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 18.9674 - val_loss: 25.5257\n",
      "Epoch 203/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 18.6610 - val_loss: 23.3945\n",
      "Epoch 204/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 19.9983 - val_loss: 24.0604\n",
      "Epoch 205/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 20.2173 - val_loss: 24.3375\n",
      "Epoch 206/2000\n",
      "1500/1500 [==============================] - 0s 31us/sample - loss: 20.0996 - val_loss: 24.7076\n",
      "Epoch 207/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 20.5430 - val_loss: 30.0989\n",
      "Epoch 208/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 19.6780 - val_loss: 23.4104\n",
      "Epoch 209/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 18.3089 - val_loss: 24.4555\n",
      "Epoch 210/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 19.0338 - val_loss: 23.8895\n",
      "Epoch 211/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 18.6379 - val_loss: 24.1591\n",
      "Epoch 212/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 19.2527 - val_loss: 24.4303\n",
      "Epoch 213/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.6871 - val_loss: 23.8685\n",
      "Epoch 214/2000\n",
      "1500/1500 [==============================] - 0s 30us/sample - loss: 19.7546 - val_loss: 24.1913\n",
      "Epoch 215/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.0497 - val_loss: 23.5100\n",
      "Epoch 216/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 20.0786 - val_loss: 25.1733\n",
      "Epoch 217/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 19.8170 - val_loss: 23.4099\n",
      "Epoch 218/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 19.1940 - val_loss: 23.1715\n",
      "Epoch 219/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 19.1438 - val_loss: 23.5346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 220/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.1680 - val_loss: 22.3423\n",
      "Epoch 221/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 19.3025 - val_loss: 23.0699\n",
      "Epoch 222/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 18.8972 - val_loss: 23.0628\n",
      "Epoch 223/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 18.6736 - val_loss: 24.4813\n",
      "Epoch 224/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.9030 - val_loss: 23.1906\n",
      "Epoch 225/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.1765 - val_loss: 21.7102\n",
      "Epoch 226/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.3440 - val_loss: 26.6689\n",
      "Epoch 227/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.8987 - val_loss: 22.1366\n",
      "Epoch 228/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 17.9640 - val_loss: 22.2880\n",
      "Epoch 229/2000\n",
      "1500/1500 [==============================] - 0s 31us/sample - loss: 18.1471 - val_loss: 23.5721\n",
      "Epoch 230/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 18.7506 - val_loss: 24.2191\n",
      "Epoch 231/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.7081 - val_loss: 22.8874\n",
      "Epoch 232/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 17.6412 - val_loss: 22.6386\n",
      "Epoch 233/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 17.7982 - val_loss: 26.5206\n",
      "Epoch 234/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.2963 - val_loss: 22.6078\n",
      "Epoch 235/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 18.4141 - val_loss: 23.3025\n",
      "Epoch 236/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.0857 - val_loss: 24.9957\n",
      "Epoch 237/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 20.4857 - val_loss: 23.0277\n",
      "Epoch 238/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 18.7691 - val_loss: 25.3313\n",
      "Epoch 239/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.8747 - val_loss: 23.9319\n",
      "Epoch 240/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 18.9927 - val_loss: 22.8175\n",
      "Epoch 241/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 19.3397 - val_loss: 26.8302\n",
      "Epoch 242/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.9907 - val_loss: 22.8060\n",
      "Epoch 243/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 19.6384 - val_loss: 23.8396\n",
      "Epoch 244/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.3485 - val_loss: 24.3348\n",
      "Epoch 245/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 17.8065 - val_loss: 22.8400\n",
      "Epoch 246/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 17.6265 - val_loss: 21.9640\n",
      "Epoch 247/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 18.0529 - val_loss: 25.0143\n",
      "Epoch 248/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 19.5389 - val_loss: 25.9620\n",
      "Epoch 249/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.5676 - val_loss: 24.4103\n",
      "Epoch 250/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.4167 - val_loss: 24.1861\n",
      "Epoch 251/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 19.3079 - val_loss: 23.1670\n",
      "Epoch 252/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 18.5419 - val_loss: 22.7389\n",
      "Epoch 253/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 20.0627 - val_loss: 23.1400\n",
      "Epoch 254/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 19.1219 - val_loss: 22.7091\n",
      "Epoch 255/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.6298 - val_loss: 29.1661\n",
      "Epoch 256/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 20.2693 - val_loss: 22.0878\n",
      "Epoch 257/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 19.0053 - val_loss: 22.8266\n",
      "Epoch 258/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 17.9794 - val_loss: 22.9254\n",
      "Epoch 259/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 18.8455 - val_loss: 21.5117\n",
      "Epoch 260/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 18.1533 - val_loss: 22.7662\n",
      "Epoch 261/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.2874 - val_loss: 23.8113\n",
      "Epoch 262/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.2683 - val_loss: 27.5361\n",
      "Epoch 263/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 20.1484 - val_loss: 22.2368\n",
      "Epoch 264/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 20.4480 - val_loss: 22.8795\n",
      "Epoch 265/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.7201 - val_loss: 23.3369\n",
      "Epoch 266/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 19.3536 - val_loss: 21.4806\n",
      "Epoch 267/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 19.8448 - val_loss: 27.5692\n",
      "Epoch 268/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 20.1801 - val_loss: 22.8097\n",
      "Epoch 269/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 18.4728 - val_loss: 23.7154\n",
      "Epoch 270/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 17.9768 - val_loss: 24.5755\n",
      "Epoch 271/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 21.0645 - val_loss: 22.4035\n",
      "Epoch 272/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.5983 - val_loss: 22.9444\n",
      "Epoch 273/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.9823 - val_loss: 22.3272\n",
      "Epoch 274/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.9823 - val_loss: 24.0075\n",
      "Epoch 275/2000\n",
      "1500/1500 [==============================] - 0s 31us/sample - loss: 18.5438 - val_loss: 22.4628\n",
      "Epoch 276/2000\n",
      "1500/1500 [==============================] - 0s 31us/sample - loss: 19.0304 - val_loss: 22.2307\n",
      "Epoch 277/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 17.1893 - val_loss: 23.3698\n",
      "Epoch 278/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 18.0827 - val_loss: 25.9722\n",
      "Epoch 279/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 18.2218 - val_loss: 23.5841\n",
      "Epoch 280/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 19.8993 - val_loss: 23.1457\n",
      "Epoch 281/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 18.5778 - val_loss: 21.8643\n",
      "Epoch 282/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.3470 - val_loss: 23.3403\n",
      "Epoch 283/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.0189 - val_loss: 21.8679\n",
      "Epoch 284/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.6036 - val_loss: 22.5619\n",
      "Epoch 285/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 19.8382 - val_loss: 21.9875\n",
      "Epoch 286/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 20.3141 - val_loss: 22.6627\n",
      "Epoch 287/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 19.5637 - val_loss: 22.8683\n",
      "Epoch 288/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 19.0626 - val_loss: 27.1460\n",
      "Epoch 289/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.6806 - val_loss: 22.0628\n",
      "Epoch 290/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 17.9628 - val_loss: 22.5576\n",
      "Epoch 291/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.3504 - val_loss: 23.9982\n",
      "Epoch 292/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.5227 - val_loss: 23.0584\n",
      "Epoch 293/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 18.6312 - val_loss: 23.2965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 294/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 19.6033 - val_loss: 24.6945\n",
      "Epoch 295/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 17.9274 - val_loss: 22.6127\n",
      "Epoch 296/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 18.2634 - val_loss: 23.1536\n",
      "Epoch 297/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.9404 - val_loss: 28.4974\n",
      "Epoch 298/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 20.6494 - val_loss: 23.1300\n",
      "Epoch 299/2000\n",
      "1500/1500 [==============================] - 0s 31us/sample - loss: 17.1636 - val_loss: 23.3176\n",
      "Epoch 300/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 17.7082 - val_loss: 21.8642\n",
      "Epoch 301/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.6703 - val_loss: 22.8248\n",
      "Epoch 302/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 19.9481 - val_loss: 25.1727\n",
      "Epoch 303/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 18.1051 - val_loss: 21.3943\n",
      "Epoch 304/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.4633 - val_loss: 23.7104\n",
      "Epoch 305/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.6451 - val_loss: 24.2391\n",
      "Epoch 306/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.4980 - val_loss: 22.5787\n",
      "Epoch 307/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 19.8006 - val_loss: 23.0099\n",
      "Epoch 308/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.1941 - val_loss: 29.9695\n",
      "Epoch 309/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 20.3705 - val_loss: 22.0894\n",
      "Epoch 310/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.2978 - val_loss: 21.2223\n",
      "Epoch 311/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.3949 - val_loss: 22.9055\n",
      "Epoch 312/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.2990 - val_loss: 24.7420\n",
      "Epoch 313/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 17.4984 - val_loss: 22.6854\n",
      "Epoch 314/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.9727 - val_loss: 23.9080\n",
      "Epoch 315/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 17.6083 - val_loss: 22.7545\n",
      "Epoch 316/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.4762 - val_loss: 23.8039\n",
      "Epoch 317/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.7035 - val_loss: 21.7640\n",
      "Epoch 318/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 17.7517 - val_loss: 23.4769\n",
      "Epoch 319/2000\n",
      "1500/1500 [==============================] - 0s 31us/sample - loss: 18.2683 - val_loss: 23.8165\n",
      "Epoch 320/2000\n",
      "1500/1500 [==============================] - 0s 33us/sample - loss: 17.9403 - val_loss: 25.6055\n",
      "Epoch 321/2000\n",
      "1500/1500 [==============================] - 0s 31us/sample - loss: 18.3677 - val_loss: 23.9680\n",
      "Epoch 322/2000\n",
      "1500/1500 [==============================] - 0s 37us/sample - loss: 17.9042 - val_loss: 24.3001\n",
      "Epoch 323/2000\n",
      "1500/1500 [==============================] - 0s 31us/sample - loss: 18.0182 - val_loss: 21.5686\n",
      "Epoch 324/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 17.0690 - val_loss: 23.2590\n",
      "Epoch 325/2000\n",
      "1500/1500 [==============================] - 0s 30us/sample - loss: 16.8237 - val_loss: 24.6716\n",
      "Epoch 326/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.3315 - val_loss: 23.6454\n",
      "Epoch 327/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 17.7101 - val_loss: 25.7913\n",
      "Epoch 328/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 18.5726 - val_loss: 22.3649\n",
      "Epoch 329/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 17.2057 - val_loss: 21.8976\n",
      "Epoch 330/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 20.7662 - val_loss: 23.3271\n",
      "Epoch 331/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 19.4615 - val_loss: 28.4894\n",
      "Epoch 332/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 19.1957 - val_loss: 24.8670\n",
      "Epoch 333/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 17.4933 - val_loss: 24.6213\n",
      "Epoch 334/2000\n",
      "1500/1500 [==============================] - 0s 30us/sample - loss: 19.7639 - val_loss: 26.7153\n",
      "Epoch 335/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 19.2327 - val_loss: 22.3798\n",
      "Epoch 336/2000\n",
      "1500/1500 [==============================] - 0s 30us/sample - loss: 19.6567 - val_loss: 24.8385\n",
      "Epoch 337/2000\n",
      "1500/1500 [==============================] - 0s 31us/sample - loss: 19.7939 - val_loss: 23.6505\n",
      "Epoch 338/2000\n",
      "1500/1500 [==============================] - 0s 30us/sample - loss: 18.4312 - val_loss: 23.4952\n",
      "Epoch 339/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 19.4525 - val_loss: 26.3498\n",
      "Epoch 340/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 19.0898 - val_loss: 23.4242\n",
      "Epoch 341/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.6446 - val_loss: 22.8056\n",
      "Epoch 342/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 20.2858 - val_loss: 23.8368\n",
      "Epoch 343/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.5512 - val_loss: 23.3356\n",
      "Epoch 344/2000\n",
      "1500/1500 [==============================] - 0s 32us/sample - loss: 17.5392 - val_loss: 22.8590\n",
      "Epoch 345/2000\n",
      "1500/1500 [==============================] - 0s 30us/sample - loss: 16.7331 - val_loss: 24.3974\n",
      "Epoch 346/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 17.6447 - val_loss: 23.1083\n",
      "Epoch 347/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 20.6664 - val_loss: 23.5927\n",
      "Epoch 348/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 18.0263 - val_loss: 25.2621\n",
      "Epoch 349/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 20.1273 - val_loss: 21.7351\n",
      "Epoch 350/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 18.0214 - val_loss: 26.0323\n",
      "Epoch 351/2000\n",
      "1500/1500 [==============================] - 0s 30us/sample - loss: 18.6681 - val_loss: 22.2248\n",
      "Epoch 352/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 19.3897 - val_loss: 26.2684\n",
      "Epoch 353/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 19.6250 - val_loss: 26.2079\n",
      "Epoch 354/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 18.1817 - val_loss: 27.7940\n",
      "Epoch 355/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 18.2685 - val_loss: 21.6209\n",
      "Epoch 356/2000\n",
      "1500/1500 [==============================] - 0s 33us/sample - loss: 18.2486 - val_loss: 23.2236\n",
      "Epoch 357/2000\n",
      "1500/1500 [==============================] - 0s 30us/sample - loss: 18.1065 - val_loss: 23.5611\n",
      "Epoch 358/2000\n",
      "1500/1500 [==============================] - 0s 33us/sample - loss: 18.2089 - val_loss: 24.3948\n",
      "Epoch 359/2000\n",
      "1500/1500 [==============================] - 0s 33us/sample - loss: 17.6900 - val_loss: 22.4929\n",
      "Epoch 360/2000\n",
      "1500/1500 [==============================] - 0s 31us/sample - loss: 17.3396 - val_loss: 23.2753\n",
      "Epoch 361/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 17.0010 - val_loss: 24.0055\n",
      "Epoch 362/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.0544 - val_loss: 23.1920\n",
      "Epoch 363/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 17.7897 - val_loss: 24.2935\n",
      "Epoch 364/2000\n",
      "1500/1500 [==============================] - 0s 30us/sample - loss: 19.2251 - val_loss: 23.3679\n",
      "Epoch 365/2000\n",
      "1500/1500 [==============================] - 0s 41us/sample - loss: 17.7810 - val_loss: 23.4300\n",
      "Epoch 366/2000\n",
      "1500/1500 [==============================] - 0s 48us/sample - loss: 18.5289 - val_loss: 25.8490\n",
      "Epoch 367/2000\n",
      "1500/1500 [==============================] - 0s 33us/sample - loss: 17.8477 - val_loss: 25.2785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 368/2000\n",
      "1500/1500 [==============================] - 0s 30us/sample - loss: 18.9218 - val_loss: 25.6012\n",
      "Epoch 369/2000\n",
      "1500/1500 [==============================] - 0s 30us/sample - loss: 17.9383 - val_loss: 21.1792\n",
      "Epoch 370/2000\n",
      "1500/1500 [==============================] - 0s 31us/sample - loss: 16.8329 - val_loss: 21.9382\n",
      "Epoch 371/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 18.7292 - val_loss: 25.0622\n",
      "Epoch 372/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 18.7918 - val_loss: 22.3485\n",
      "Epoch 373/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 17.1058 - val_loss: 23.2300\n",
      "Epoch 374/2000\n",
      "1500/1500 [==============================] - 0s 30us/sample - loss: 17.3599 - val_loss: 25.3365\n",
      "Epoch 375/2000\n",
      "1500/1500 [==============================] - 0s 31us/sample - loss: 17.7905 - val_loss: 22.4630\n",
      "Epoch 376/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 16.4094 - val_loss: 22.9609\n",
      "Epoch 377/2000\n",
      "1500/1500 [==============================] - 0s 39us/sample - loss: 17.7999 - val_loss: 21.5470\n",
      "Epoch 378/2000\n",
      "1500/1500 [==============================] - 0s 35us/sample - loss: 17.3723 - val_loss: 25.2860\n",
      "Epoch 379/2000\n",
      "1500/1500 [==============================] - 0s 33us/sample - loss: 18.4547 - val_loss: 23.3188\n",
      "Epoch 380/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 19.0811 - val_loss: 27.8888\n",
      "Epoch 381/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 17.3186 - val_loss: 23.9455\n",
      "Epoch 382/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 17.3775 - val_loss: 23.8608\n",
      "Epoch 383/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 17.7017 - val_loss: 23.5804\n",
      "Epoch 384/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 17.5129 - val_loss: 25.4988\n",
      "Epoch 385/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 17.6484 - val_loss: 22.7911\n",
      "Epoch 386/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 17.4657 - val_loss: 24.1348\n",
      "Epoch 387/2000\n",
      "1500/1500 [==============================] - 0s 33us/sample - loss: 16.8381 - val_loss: 23.4943\n",
      "Epoch 388/2000\n",
      "1500/1500 [==============================] - 0s 31us/sample - loss: 16.6775 - val_loss: 22.5478\n",
      "Epoch 389/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 17.5337 - val_loss: 25.1807\n",
      "Epoch 390/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 17.3505 - val_loss: 23.6000\n",
      "Epoch 391/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 17.0671 - val_loss: 23.9457\n",
      "Epoch 392/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 17.5870 - val_loss: 25.8070\n",
      "Epoch 393/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 18.1532 - val_loss: 22.9958\n",
      "Epoch 394/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 17.1087 - val_loss: 22.4899\n",
      "Epoch 395/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 17.9390 - val_loss: 24.4069\n",
      "Epoch 396/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 17.2687 - val_loss: 21.6079\n",
      "Epoch 397/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 16.7563 - val_loss: 24.5722\n",
      "Epoch 398/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 16.6073 - val_loss: 21.4502\n",
      "Epoch 399/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.0879 - val_loss: 23.2865\n",
      "Epoch 400/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 16.9809 - val_loss: 22.8694\n",
      "Epoch 401/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 17.9040 - val_loss: 26.1725\n",
      "Epoch 402/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 17.6221 - val_loss: 23.1922\n",
      "Epoch 403/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.8635 - val_loss: 25.3493\n",
      "Epoch 404/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 17.5363 - val_loss: 22.4524\n",
      "Epoch 405/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.2207 - val_loss: 24.1857\n",
      "Epoch 406/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.2835 - val_loss: 22.6809\n",
      "Epoch 407/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 17.1001 - val_loss: 25.0942\n",
      "Epoch 408/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 18.0557 - val_loss: 26.9151\n",
      "Epoch 409/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 18.9648 - val_loss: 23.0761\n",
      "Epoch 410/2000\n",
      "1500/1500 [==============================] - 0s 34us/sample - loss: 18.0306 - val_loss: 24.9454\n",
      "Epoch 411/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 17.6545 - val_loss: 23.7370\n",
      "Epoch 412/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 17.9012 - val_loss: 22.9703\n",
      "Epoch 413/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 20.4111 - val_loss: 33.1412\n",
      "Epoch 414/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 22.5231 - val_loss: 26.8123\n",
      "Epoch 415/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 17.7889 - val_loss: 26.7269\n",
      "Epoch 416/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.7289 - val_loss: 25.4506\n",
      "Epoch 417/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 17.0767 - val_loss: 22.5730\n",
      "Epoch 418/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 17.1537 - val_loss: 22.3454\n",
      "Epoch 419/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 16.2572 - val_loss: 22.4981\n",
      "Epoch 420/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 17.1391 - val_loss: 25.8886\n",
      "Epoch 421/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 17.9672 - val_loss: 25.6942\n",
      "Epoch 422/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 18.4233 - val_loss: 22.2069\n",
      "Epoch 423/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 17.3246 - val_loss: 21.6601\n",
      "Epoch 424/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.8913 - val_loss: 23.4445\n",
      "Epoch 425/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 17.1576 - val_loss: 22.9436\n",
      "Epoch 426/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 17.7894 - val_loss: 23.1825\n",
      "Epoch 427/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.7056 - val_loss: 23.3118\n",
      "Epoch 428/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 18.4553 - val_loss: 22.8967\n",
      "Epoch 429/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.4466 - val_loss: 22.7207\n",
      "Epoch 430/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 16.5053 - val_loss: 24.3892\n",
      "Epoch 431/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.0097 - val_loss: 25.5240\n",
      "Epoch 432/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.0433 - val_loss: 24.3621\n",
      "Epoch 433/2000\n",
      "1500/1500 [==============================] - 0s 30us/sample - loss: 16.2713 - val_loss: 23.5636\n",
      "Epoch 434/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 17.8714 - val_loss: 24.4048\n",
      "Epoch 435/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 18.2520 - val_loss: 23.3758\n",
      "Epoch 436/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.3001 - val_loss: 24.2858\n",
      "Epoch 437/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 16.9373 - val_loss: 22.7091\n",
      "Epoch 438/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 16.6843 - val_loss: 22.2214\n",
      "Epoch 439/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.1919 - val_loss: 23.8015\n",
      "Epoch 440/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.8295 - val_loss: 22.5726\n",
      "Epoch 441/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.6481 - val_loss: 21.9927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 442/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 17.4577 - val_loss: 21.0718\n",
      "Epoch 443/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 17.2586 - val_loss: 22.8824\n",
      "Epoch 444/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 16.9294 - val_loss: 23.3871\n",
      "Epoch 445/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 17.2720 - val_loss: 22.3604\n",
      "Epoch 446/2000\n",
      "1500/1500 [==============================] - 0s 30us/sample - loss: 17.4605 - val_loss: 23.6381\n",
      "Epoch 447/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 17.3750 - val_loss: 22.9503\n",
      "Epoch 448/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 17.4077 - val_loss: 23.8343\n",
      "Epoch 449/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 17.6151 - val_loss: 21.8992\n",
      "Epoch 450/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.7866 - val_loss: 26.1899\n",
      "Epoch 451/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 16.8128 - val_loss: 23.5084\n",
      "Epoch 452/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 17.4002 - val_loss: 22.8721\n",
      "Epoch 453/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.8904 - val_loss: 24.8152\n",
      "Epoch 454/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 16.9189 - val_loss: 21.8454\n",
      "Epoch 455/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.3623 - val_loss: 28.1938\n",
      "Epoch 456/2000\n",
      "1500/1500 [==============================] - 0s 31us/sample - loss: 16.9828 - val_loss: 22.8192\n",
      "Epoch 457/2000\n",
      "1500/1500 [==============================] - 0s 30us/sample - loss: 15.9510 - val_loss: 22.7908\n",
      "Epoch 458/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 17.0732 - val_loss: 25.1027\n",
      "Epoch 459/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 17.2776 - val_loss: 22.5185\n",
      "Epoch 460/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.8150 - val_loss: 22.6230\n",
      "Epoch 461/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 17.2327 - val_loss: 22.9032\n",
      "Epoch 462/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.3706 - val_loss: 22.1683\n",
      "Epoch 463/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 16.8222 - val_loss: 21.8537\n",
      "Epoch 464/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.8135 - val_loss: 23.4329\n",
      "Epoch 465/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 16.7842 - val_loss: 23.0329\n",
      "Epoch 466/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 15.8177 - val_loss: 22.1719\n",
      "Epoch 467/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.0109 - val_loss: 24.2105\n",
      "Epoch 468/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 17.4330 - val_loss: 23.1747\n",
      "Epoch 469/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.6292 - val_loss: 24.0511\n",
      "Epoch 470/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.7038 - val_loss: 21.5494\n",
      "Epoch 471/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 15.8561 - val_loss: 22.6010\n",
      "Epoch 472/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.0360 - val_loss: 25.5904\n",
      "Epoch 473/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 17.9011 - val_loss: 25.6424\n",
      "Epoch 474/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.1007 - val_loss: 23.8530\n",
      "Epoch 475/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 15.9830 - val_loss: 22.5076\n",
      "Epoch 476/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.3281 - val_loss: 22.8611\n",
      "Epoch 477/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.3873 - val_loss: 24.5442\n",
      "Epoch 478/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 17.6135 - val_loss: 21.4532\n",
      "Epoch 479/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 18.0959 - val_loss: 23.0128\n",
      "Epoch 480/2000\n",
      "1500/1500 [==============================] - 0s 30us/sample - loss: 17.3240 - val_loss: 21.7972\n",
      "Epoch 481/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 16.6574 - val_loss: 21.4148\n",
      "Epoch 482/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.1943 - val_loss: 21.9037\n",
      "Epoch 483/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.3018 - val_loss: 22.5632\n",
      "Epoch 484/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 19.3366 - val_loss: 22.8926\n",
      "Epoch 485/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 17.6978 - val_loss: 25.7601\n",
      "Epoch 486/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 16.2919 - val_loss: 22.6936\n",
      "Epoch 487/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 17.5239 - val_loss: 24.0418\n",
      "Epoch 488/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 16.4045 - val_loss: 22.6104\n",
      "Epoch 489/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 16.5890 - val_loss: 22.8874\n",
      "Epoch 490/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 17.8502 - val_loss: 25.2221\n",
      "Epoch 491/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 17.7657 - val_loss: 24.0542\n",
      "Epoch 492/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 16.7199 - val_loss: 22.1625\n",
      "Epoch 493/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.7126 - val_loss: 23.4988\n",
      "Epoch 494/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 18.5354 - val_loss: 24.2636\n",
      "Epoch 495/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 17.6845 - val_loss: 23.0438\n",
      "Epoch 496/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 16.4753 - val_loss: 26.0888\n",
      "Epoch 497/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 16.0354 - val_loss: 21.6375\n",
      "Epoch 498/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 15.7318 - val_loss: 23.8628\n",
      "Epoch 499/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 16.4614 - val_loss: 21.5416\n",
      "Epoch 500/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 17.0712 - val_loss: 24.4794\n",
      "Epoch 501/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 16.6414 - val_loss: 22.0899\n",
      "Epoch 502/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 16.9073 - val_loss: 22.1045\n",
      "Epoch 503/2000\n",
      "1500/1500 [==============================] - 0s 33us/sample - loss: 17.4708 - val_loss: 21.5346\n",
      "Epoch 504/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 16.6753 - val_loss: 24.2299\n",
      "Epoch 505/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 16.4490 - val_loss: 25.1280\n",
      "Epoch 506/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 17.5223 - val_loss: 25.4621\n",
      "Epoch 507/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.1404 - val_loss: 21.1488\n",
      "Epoch 508/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 16.3992 - val_loss: 22.7326\n",
      "Epoch 509/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 16.9669 - val_loss: 22.5517\n",
      "Epoch 510/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 16.1465 - val_loss: 23.8059\n",
      "Epoch 511/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.2245 - val_loss: 21.6453\n",
      "Epoch 512/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 16.3821 - val_loss: 23.9304\n",
      "Epoch 513/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 16.4797 - val_loss: 22.4484\n",
      "Epoch 514/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 16.2210 - val_loss: 22.1596\n",
      "Epoch 515/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 15.9867 - val_loss: 23.2512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 516/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 16.4382 - val_loss: 26.5986\n",
      "Epoch 517/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 16.3329 - val_loss: 22.0203\n",
      "Epoch 518/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 15.8130 - val_loss: 21.7443\n",
      "Epoch 519/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 17.0304 - val_loss: 26.3984\n",
      "Epoch 520/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.8840 - val_loss: 21.8929\n",
      "Epoch 521/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.6985 - val_loss: 21.9401\n",
      "Epoch 522/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.2438 - val_loss: 24.0327\n",
      "Epoch 523/2000\n",
      "1500/1500 [==============================] - 0s 26us/sample - loss: 16.3592 - val_loss: 22.2839\n",
      "Epoch 524/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 15.6498 - val_loss: 23.5608\n",
      "Epoch 525/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.6767 - val_loss: 23.4399\n",
      "Epoch 526/2000\n",
      "1500/1500 [==============================] - 0s 31us/sample - loss: 15.3788 - val_loss: 23.1163\n",
      "Epoch 527/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 18.6070 - val_loss: 25.6592\n",
      "Epoch 528/2000\n",
      "1500/1500 [==============================] - 0s 31us/sample - loss: 16.8384 - val_loss: 21.7627\n",
      "Epoch 529/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.1183 - val_loss: 23.0494\n",
      "Epoch 530/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 16.0202 - val_loss: 23.0530\n",
      "Epoch 531/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 15.6955 - val_loss: 23.3438\n",
      "Epoch 532/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 15.1606 - val_loss: 23.5805\n",
      "Epoch 533/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 17.6401 - val_loss: 25.5820\n",
      "Epoch 534/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 17.8746 - val_loss: 24.0557\n",
      "Epoch 535/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 16.4545 - val_loss: 22.8733\n",
      "Epoch 536/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 16.8823 - val_loss: 21.9633\n",
      "Epoch 537/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 15.9709 - val_loss: 28.3295\n",
      "Epoch 538/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 19.1431 - val_loss: 22.7630\n",
      "Epoch 539/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.2795 - val_loss: 24.8551\n",
      "Epoch 540/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 16.3435 - val_loss: 23.9223\n",
      "Epoch 541/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 16.5812 - val_loss: 24.9706\n",
      "Epoch 542/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 19.2021 - val_loss: 23.1150\n",
      "Epoch 543/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 16.7916 - val_loss: 23.1163\n",
      "Epoch 544/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.4757 - val_loss: 21.9887\n",
      "Epoch 545/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 15.6168 - val_loss: 24.2363\n",
      "Epoch 546/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 16.5894 - val_loss: 24.2095\n",
      "Epoch 547/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 17.1143 - val_loss: 24.8404\n",
      "Epoch 548/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 17.8509 - val_loss: 24.0301\n",
      "Epoch 549/2000\n",
      "1500/1500 [==============================] - 0s 31us/sample - loss: 16.8611 - val_loss: 22.1144\n",
      "Epoch 550/2000\n",
      "1500/1500 [==============================] - 0s 31us/sample - loss: 16.8481 - val_loss: 25.9898\n",
      "Epoch 551/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 16.6012 - val_loss: 22.9254\n",
      "Epoch 552/2000\n",
      "1500/1500 [==============================] - 0s 30us/sample - loss: 16.5374 - val_loss: 23.6272\n",
      "Epoch 553/2000\n",
      "1500/1500 [==============================] - 0s 31us/sample - loss: 16.6844 - val_loss: 24.4582\n",
      "Epoch 554/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 17.6286 - val_loss: 26.4383\n",
      "Epoch 555/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 16.4549 - val_loss: 22.6282\n",
      "Epoch 556/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 14.9552 - val_loss: 22.1532\n",
      "Epoch 557/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 15.9599 - val_loss: 25.0866\n",
      "Epoch 558/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 16.7788 - val_loss: 22.2281\n",
      "Epoch 559/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 15.3991 - val_loss: 21.9432\n",
      "Epoch 560/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 15.7797 - val_loss: 22.6759\n",
      "Epoch 561/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 16.6919 - val_loss: 22.0633\n",
      "Epoch 562/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 16.6709 - val_loss: 25.3180\n",
      "Epoch 563/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 18.0895 - val_loss: 24.2261\n",
      "Epoch 564/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 16.7860 - val_loss: 25.3978\n",
      "Epoch 565/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 19.1734 - val_loss: 26.2536\n",
      "Epoch 566/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.1470 - val_loss: 25.8510\n",
      "Epoch 567/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 16.1264 - val_loss: 21.6099\n",
      "Epoch 568/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 15.1296 - val_loss: 22.9200\n",
      "Epoch 569/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 14.8748 - val_loss: 23.2198\n",
      "Epoch 570/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 16.3361 - val_loss: 24.3794\n",
      "Epoch 571/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.1600 - val_loss: 22.6851\n",
      "Epoch 572/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 15.1949 - val_loss: 21.6062\n",
      "Epoch 573/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 14.9076 - val_loss: 21.7884\n",
      "Epoch 574/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 15.8114 - val_loss: 21.7804\n",
      "Epoch 575/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 16.6683 - val_loss: 26.2846\n",
      "Epoch 576/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 16.0848 - val_loss: 21.9530\n",
      "Epoch 577/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 16.1573 - val_loss: 23.4523\n",
      "Epoch 578/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 15.8729 - val_loss: 21.2957\n",
      "Epoch 579/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 16.5431 - val_loss: 23.0059\n",
      "Epoch 580/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 15.9924 - val_loss: 21.5930\n",
      "Epoch 581/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 15.1064 - val_loss: 21.7118\n",
      "Epoch 582/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 15.2633 - val_loss: 22.6513\n",
      "Epoch 583/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 16.2434 - val_loss: 24.3714\n",
      "Epoch 584/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 15.9233 - val_loss: 22.9014\n",
      "Epoch 585/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 15.6648 - val_loss: 21.6352\n",
      "Epoch 586/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 15.3359 - val_loss: 21.8682\n",
      "Epoch 587/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 15.3009 - val_loss: 22.6679\n",
      "Epoch 588/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 16.1165 - val_loss: 24.3328\n",
      "Epoch 589/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 16.2712 - val_loss: 22.1389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 590/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 15.7366 - val_loss: 23.5165\n",
      "Epoch 591/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 16.5947 - val_loss: 25.2680\n",
      "Epoch 592/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 17.7280 - val_loss: 23.6744\n",
      "Epoch 593/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 16.0790 - val_loss: 22.0626\n",
      "Epoch 594/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 15.3886 - val_loss: 22.0208\n",
      "Epoch 595/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 16.6614 - val_loss: 24.7730\n",
      "Epoch 596/2000\n",
      "1500/1500 [==============================] - 0s 32us/sample - loss: 16.4082 - val_loss: 22.8698\n",
      "Epoch 597/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 15.7246 - val_loss: 26.9486\n",
      "Epoch 598/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 17.1672 - val_loss: 22.4675\n",
      "Epoch 599/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 15.6090 - val_loss: 23.2738\n",
      "Epoch 600/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 15.5155 - val_loss: 22.5556\n",
      "Epoch 601/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 16.0235 - val_loss: 24.2263\n",
      "Epoch 602/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 16.8060 - val_loss: 23.6741\n",
      "Epoch 603/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.3197 - val_loss: 21.3715\n",
      "Epoch 604/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 14.9894 - val_loss: 24.5793\n",
      "Epoch 605/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 18.3773 - val_loss: 24.2374\n",
      "Epoch 606/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 15.6871 - val_loss: 22.5220\n",
      "Epoch 607/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 16.0597 - val_loss: 25.0117\n",
      "Epoch 608/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 15.9008 - val_loss: 22.7617\n",
      "Epoch 609/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 15.4204 - val_loss: 22.2785\n",
      "Epoch 610/2000\n",
      "1500/1500 [==============================] - 0s 30us/sample - loss: 15.4365 - val_loss: 22.0267\n",
      "Epoch 611/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 15.4321 - val_loss: 22.0503\n",
      "Epoch 612/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 15.4173 - val_loss: 22.6573\n",
      "Epoch 613/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 15.8694 - val_loss: 23.7182\n",
      "Epoch 614/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 15.8579 - val_loss: 21.4011\n",
      "Epoch 615/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 15.9027 - val_loss: 21.4576\n",
      "Epoch 616/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 14.7160 - val_loss: 22.4249\n",
      "Epoch 617/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 14.7824 - val_loss: 21.7249\n",
      "Epoch 618/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 15.9784 - val_loss: 22.5480\n",
      "Epoch 619/2000\n",
      "1500/1500 [==============================] - 0s 31us/sample - loss: 16.9524 - val_loss: 24.5455\n",
      "Epoch 620/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 16.0710 - val_loss: 22.3332\n",
      "Epoch 621/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 15.3338 - val_loss: 24.1738\n",
      "Epoch 622/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 16.7484 - val_loss: 25.8360\n",
      "Epoch 623/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 17.1050 - val_loss: 25.4094\n",
      "Epoch 624/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 15.5763 - val_loss: 22.8913\n",
      "Epoch 625/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 15.2488 - val_loss: 22.1095\n",
      "Epoch 626/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 17.4508 - val_loss: 24.6148\n",
      "Epoch 627/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 15.9285 - val_loss: 25.2630\n",
      "Epoch 628/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 15.8084 - val_loss: 22.0839\n",
      "Epoch 629/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 15.5177 - val_loss: 23.4484\n",
      "Epoch 630/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 16.6116 - val_loss: 25.7285\n",
      "Epoch 631/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 17.6373 - val_loss: 22.0038\n",
      "Epoch 632/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 14.7611 - val_loss: 24.1195\n",
      "Epoch 633/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 14.8085 - val_loss: 23.9081\n",
      "Epoch 634/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 15.7527 - val_loss: 22.1260\n",
      "Epoch 635/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 15.6505 - val_loss: 23.6149\n",
      "Epoch 636/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 16.3028 - val_loss: 25.5531\n",
      "Epoch 637/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 16.1739 - val_loss: 23.0923\n",
      "Epoch 638/2000\n",
      "1500/1500 [==============================] - 0s 29us/sample - loss: 15.5785 - val_loss: 23.4489\n",
      "Epoch 639/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 16.8073 - val_loss: 23.5823\n",
      "Epoch 640/2000\n",
      "1500/1500 [==============================] - 0s 27us/sample - loss: 18.0459 - val_loss: 21.8912\n",
      "Epoch 641/2000\n",
      "1500/1500 [==============================] - 0s 28us/sample - loss: 15.4864 - val_loss: 21.7857\n",
      "Epoch 642/2000\n",
      "1500/1500 [==============================] - 0s 31us/sample - loss: 14.4629 - val_loss: 21.4781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 16/16 [08:39<00:00, 32.49s/it]\n"
     ]
    }
   ],
   "source": [
    "t = ta.Scan(x=train_df, y=train_label,\n",
    "            x_val=test_df, y_val=test_label,\n",
    "            model=numerai_model,\n",
    "            params=p,\n",
    "            experiment_name='Predykcja IE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-material",
   "metadata": {},
   "source": [
    "# 3. Wczytuje wyniki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "buried-korean",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('D:/STUDIA/ROK_II/Projekt/Indeks Ekonomiczny/Sieci neuro/Zestaw jalowka + krowa/Predykcja IE/051121141704.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alien-ultimate",
   "metadata": {},
   "source": [
    "## 3.1 Wyliczam ilość neuronów i połączeń"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "advance-producer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    round_epochs       loss   val_loss activation_layer  batc_normalization  \\\n",
      "15           642  14.462912  21.478142             relu               False   \n",
      "10           809  16.901900  22.135884             relu               False   \n",
      "12           732  16.560197  22.279187             relu               False   \n",
      "13           744  15.913212  22.508964             relu               False   \n",
      "7            695  13.628500  22.956164             relu               False   \n",
      "\n",
      "    batch_size  dropout  epochs  first_neuron  hidden_layers  hidden_neuron  \\\n",
      "15          64        0    2000           320              2            100   \n",
      "10          64        0    2000           160              2             50   \n",
      "12          64        0    2000           320              1             50   \n",
      "13          64        0    2000           320              1            100   \n",
      "7           32        0    2000           320              2            100   \n",
      "\n",
      "   kernel_initializer last_activation optimizer  nodes     links  \\\n",
      "15             normal          linear      adam    520  134940.0   \n",
      "10             normal          linear      adam    260   33670.0   \n",
      "12             normal          linear      adam    370   68265.0   \n",
      "13             normal          linear      adam    420   87990.0   \n",
      "7              normal          linear      adam    520  134940.0   \n",
      "\n",
      "    val_loss_improvement  \n",
      "15             -7.015229  \n",
      "10             -5.233984  \n",
      "12             -5.718990  \n",
      "13             -6.595752  \n",
      "7              -9.327664  \n"
     ]
    }
   ],
   "source": [
    "df['nodes'] = df.first_neuron + df.hidden_neuron*df.hidden_layers\n",
    "df['links'] =  df.nodes * (df.nodes-1) / 2\n",
    "df['val_loss_improvement'] = df.loss - df.val_loss \n",
    "#compare to baseline log-loss (higher is better)\n",
    "print(df.sort_values('val_loss').head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "086e68cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>activation_layer</th>\n",
       "      <th>batc_normalization</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>dropout</th>\n",
       "      <th>epochs</th>\n",
       "      <th>first_neuron</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>hidden_neuron</th>\n",
       "      <th>kernel_initializer</th>\n",
       "      <th>last_activation</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>nodes</th>\n",
       "      <th>links</th>\n",
       "      <th>val_loss_improvement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>642</td>\n",
       "      <td>14.462912</td>\n",
       "      <td>21.478142</td>\n",
       "      <td>relu</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>320</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>normal</td>\n",
       "      <td>linear</td>\n",
       "      <td>adam</td>\n",
       "      <td>520</td>\n",
       "      <td>134940.0</td>\n",
       "      <td>-7.015229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>809</td>\n",
       "      <td>16.901900</td>\n",
       "      <td>22.135884</td>\n",
       "      <td>relu</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>160</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>normal</td>\n",
       "      <td>linear</td>\n",
       "      <td>adam</td>\n",
       "      <td>260</td>\n",
       "      <td>33670.0</td>\n",
       "      <td>-5.233984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>732</td>\n",
       "      <td>16.560197</td>\n",
       "      <td>22.279187</td>\n",
       "      <td>relu</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>320</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>normal</td>\n",
       "      <td>linear</td>\n",
       "      <td>adam</td>\n",
       "      <td>370</td>\n",
       "      <td>68265.0</td>\n",
       "      <td>-5.718990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>744</td>\n",
       "      <td>15.913212</td>\n",
       "      <td>22.508964</td>\n",
       "      <td>relu</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>320</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>normal</td>\n",
       "      <td>linear</td>\n",
       "      <td>adam</td>\n",
       "      <td>420</td>\n",
       "      <td>87990.0</td>\n",
       "      <td>-6.595752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>695</td>\n",
       "      <td>13.628500</td>\n",
       "      <td>22.956164</td>\n",
       "      <td>relu</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>320</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>normal</td>\n",
       "      <td>linear</td>\n",
       "      <td>adam</td>\n",
       "      <td>520</td>\n",
       "      <td>134940.0</td>\n",
       "      <td>-9.327664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>596</td>\n",
       "      <td>17.379171</td>\n",
       "      <td>22.997549</td>\n",
       "      <td>relu</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>320</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>normal</td>\n",
       "      <td>linear</td>\n",
       "      <td>adam</td>\n",
       "      <td>370</td>\n",
       "      <td>68265.0</td>\n",
       "      <td>-5.618378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1015</td>\n",
       "      <td>17.379168</td>\n",
       "      <td>23.154316</td>\n",
       "      <td>relu</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>160</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>normal</td>\n",
       "      <td>linear</td>\n",
       "      <td>adam</td>\n",
       "      <td>210</td>\n",
       "      <td>21945.0</td>\n",
       "      <td>-5.775148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>872</td>\n",
       "      <td>16.072105</td>\n",
       "      <td>23.268880</td>\n",
       "      <td>relu</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>160</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>normal</td>\n",
       "      <td>linear</td>\n",
       "      <td>adam</td>\n",
       "      <td>260</td>\n",
       "      <td>33670.0</td>\n",
       "      <td>-7.196774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>633</td>\n",
       "      <td>16.717407</td>\n",
       "      <td>23.305383</td>\n",
       "      <td>relu</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>160</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>normal</td>\n",
       "      <td>linear</td>\n",
       "      <td>adam</td>\n",
       "      <td>360</td>\n",
       "      <td>64620.0</td>\n",
       "      <td>-6.587977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>639</td>\n",
       "      <td>17.941844</td>\n",
       "      <td>23.329955</td>\n",
       "      <td>relu</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>160</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>normal</td>\n",
       "      <td>linear</td>\n",
       "      <td>adam</td>\n",
       "      <td>260</td>\n",
       "      <td>33670.0</td>\n",
       "      <td>-5.388111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>443</td>\n",
       "      <td>18.927746</td>\n",
       "      <td>23.412201</td>\n",
       "      <td>relu</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>160</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>normal</td>\n",
       "      <td>linear</td>\n",
       "      <td>adam</td>\n",
       "      <td>210</td>\n",
       "      <td>21945.0</td>\n",
       "      <td>-4.484455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>993</td>\n",
       "      <td>14.914460</td>\n",
       "      <td>23.436875</td>\n",
       "      <td>relu</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>320</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>normal</td>\n",
       "      <td>linear</td>\n",
       "      <td>adam</td>\n",
       "      <td>420</td>\n",
       "      <td>87990.0</td>\n",
       "      <td>-8.522415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>514</td>\n",
       "      <td>18.334770</td>\n",
       "      <td>24.144108</td>\n",
       "      <td>relu</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>320</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>normal</td>\n",
       "      <td>linear</td>\n",
       "      <td>adam</td>\n",
       "      <td>420</td>\n",
       "      <td>87990.0</td>\n",
       "      <td>-5.809338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>558</td>\n",
       "      <td>15.815021</td>\n",
       "      <td>24.162733</td>\n",
       "      <td>relu</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>320</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "      <td>normal</td>\n",
       "      <td>linear</td>\n",
       "      <td>adam</td>\n",
       "      <td>420</td>\n",
       "      <td>87990.0</td>\n",
       "      <td>-8.347712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>429</td>\n",
       "      <td>17.333990</td>\n",
       "      <td>24.711018</td>\n",
       "      <td>relu</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>160</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>normal</td>\n",
       "      <td>linear</td>\n",
       "      <td>adam</td>\n",
       "      <td>360</td>\n",
       "      <td>64620.0</td>\n",
       "      <td>-7.377028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>376</td>\n",
       "      <td>19.988674</td>\n",
       "      <td>25.100906</td>\n",
       "      <td>relu</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>160</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>normal</td>\n",
       "      <td>linear</td>\n",
       "      <td>adam</td>\n",
       "      <td>260</td>\n",
       "      <td>33670.0</td>\n",
       "      <td>-5.112233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    round_epochs       loss   val_loss activation_layer  batc_normalization  \\\n",
       "15           642  14.462912  21.478142             relu               False   \n",
       "10           809  16.901900  22.135884             relu               False   \n",
       "12           732  16.560197  22.279187             relu               False   \n",
       "13           744  15.913212  22.508964             relu               False   \n",
       "7            695  13.628500  22.956164             relu               False   \n",
       "4            596  17.379171  22.997549             relu               False   \n",
       "8           1015  17.379168  23.154316             relu               False   \n",
       "2            872  16.072105  23.268880             relu               False   \n",
       "11           633  16.717407  23.305383             relu               False   \n",
       "9            639  17.941844  23.329955             relu               False   \n",
       "0            443  18.927746  23.412201             relu               False   \n",
       "14           993  14.914460  23.436875             relu               False   \n",
       "5            514  18.334770  24.144108             relu               False   \n",
       "6            558  15.815021  24.162733             relu               False   \n",
       "3            429  17.333990  24.711018             relu               False   \n",
       "1            376  19.988674  25.100906             relu               False   \n",
       "\n",
       "    batch_size  dropout  epochs  first_neuron  hidden_layers  hidden_neuron  \\\n",
       "15          64        0    2000           320              2            100   \n",
       "10          64        0    2000           160              2             50   \n",
       "12          64        0    2000           320              1             50   \n",
       "13          64        0    2000           320              1            100   \n",
       "7           32        0    2000           320              2            100   \n",
       "4           32        0    2000           320              1             50   \n",
       "8           64        0    2000           160              1             50   \n",
       "2           32        0    2000           160              2             50   \n",
       "11          64        0    2000           160              2            100   \n",
       "9           64        0    2000           160              1            100   \n",
       "0           32        0    2000           160              1             50   \n",
       "14          64        0    2000           320              2             50   \n",
       "5           32        0    2000           320              1            100   \n",
       "6           32        0    2000           320              2             50   \n",
       "3           32        0    2000           160              2            100   \n",
       "1           32        0    2000           160              1            100   \n",
       "\n",
       "   kernel_initializer last_activation optimizer  nodes     links  \\\n",
       "15             normal          linear      adam    520  134940.0   \n",
       "10             normal          linear      adam    260   33670.0   \n",
       "12             normal          linear      adam    370   68265.0   \n",
       "13             normal          linear      adam    420   87990.0   \n",
       "7              normal          linear      adam    520  134940.0   \n",
       "4              normal          linear      adam    370   68265.0   \n",
       "8              normal          linear      adam    210   21945.0   \n",
       "2              normal          linear      adam    260   33670.0   \n",
       "11             normal          linear      adam    360   64620.0   \n",
       "9              normal          linear      adam    260   33670.0   \n",
       "0              normal          linear      adam    210   21945.0   \n",
       "14             normal          linear      adam    420   87990.0   \n",
       "5              normal          linear      adam    420   87990.0   \n",
       "6              normal          linear      adam    420   87990.0   \n",
       "3              normal          linear      adam    360   64620.0   \n",
       "1              normal          linear      adam    260   33670.0   \n",
       "\n",
       "    val_loss_improvement  \n",
       "15             -7.015229  \n",
       "10             -5.233984  \n",
       "12             -5.718990  \n",
       "13             -6.595752  \n",
       "7              -9.327664  \n",
       "4              -5.618378  \n",
       "8              -5.775148  \n",
       "2              -7.196774  \n",
       "11             -6.587977  \n",
       "9              -5.388111  \n",
       "0              -4.484455  \n",
       "14             -8.522415  \n",
       "5              -5.809338  \n",
       "6              -8.347712  \n",
       "3              -7.377028  \n",
       "1              -5.112233  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values('val_loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-plenty",
   "metadata": {},
   "source": [
    "## 3.2 Najlepszy wynik walidacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "superb-membership",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.478141860961912"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(df[\"val_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "sorted-death",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 1., 2., 0., 4., 4., 0., 2., 1., 1.]),\n",
       " array([21.47814186, 21.8404183 , 22.20269475, 22.56497119, 22.92724763,\n",
       "        23.28952408, 23.65180052, 24.01407697, 24.37635341, 24.73862985,\n",
       "        25.1009063 ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAQsElEQVR4nO3df6jdd33H8edrMaLMjgxyoTE/GpUwZoXacoktHaOIG21aLGMdVJwd3R+hpd3qUKSzUJUxUMactJGGbBbt1llk1ZJpihZmsWWkmmZJahodUSKNDTZWTA0tSvS9P+63cnd6zj3fm5x7z+3H5wMO+f74fM959ZPb1z35nu85J1WFJOnV77emHUCSNBkWuiQ1wkKXpEZY6JLUCAtdkhrxmmk98Nq1a2vz5s3TenhJelV68sknf1xVM8P2Ta3QN2/ezL59+6b18JL0qpTkB6P2ecpFkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNaJ3oSdZleR/knx5yL4kuSvJ0SSHklwy2ZiSpHEW8wz9NuDIiH1XAVu623bgnnPMJUlapF6FnmQDcDXwLyOGXAvcV3P2AmuSrJtQRklSD33fKfop4EPAeSP2rweembd+vNt2Yv6gJNuZewbPpk2bFpNTv4E23/6VaUdYdsc+fvW0I+hVbOwz9CTXAM9V1ZMLDRuy7RVfhVRVu6pqtqpmZ2aGfhSBJOks9Tnlcjnw7iTHgAeAdyb5t4Exx4GN89Y3AM9OJKEkqZexhV5Vf1tVG6pqM3A98F9V9ecDw3YDN3RXu1wKnKqqE4P3JUlaOmf9aYtJbgKoqp3AHmAbcBR4EbhxIukkSb0tqtCr6lHg0W5557ztBdwyyWCSpMXxnaKS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEb0+ZLo1yX5ZpKDSQ4n+diQMVckOZXkQHe7c2niSpJG6fONRT8H3llVp5OsBh5P8nBV7R0Y91hVXTP5iJKkPsYWevf1cqe71dXdrZYylCRp8XqdQ0+yKskB4Dngkap6Ysiwy7rTMg8nuXCSISVJ4/Uq9Kr6ZVW9HdgAbE3ytoEh+4ELquoi4G7goWH3k2R7kn1J9p08efLsU0uSXmFRV7lU1U+BR4ErB7a/UFWnu+U9wOoka4ccv6uqZqtqdmZm5qxDS5Jeqc9VLjNJ1nTLrwfeBXxnYMz5SdItb+3u9/mJp5UkjdTnKpd1wOeSrGKuqL9QVV9OchNAVe0ErgNuTnIGeAm4vnsxVZK0TPpc5XIIuHjI9p3zlncAOyYbTZK0GL5TVJIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhrR5ztFX5fkm0kOJjmc5GNDxiTJXUmOJjmU5JKliStJGqXPd4r+HHhnVZ1Oshp4PMnDVbV33pirgC3d7R3APd2fkqRlMvYZes053a2u7m6DXwB9LXBfN3YvsCbJuslGlSQtpNc59CSrkhwAngMeqaonBoasB56Zt3682zZ4P9uT7Euy7+TJk2cZWZI0TK9Cr6pfVtXbgQ3A1iRvGxiSYYcNuZ9dVTVbVbMzMzOLDitJGm1RV7lU1U+BR4ErB3YdBzbOW98APHsuwSRJi9PnKpeZJGu65dcD7wK+MzBsN3BDd7XLpcCpqjox6bCSpNH6XOWyDvhcklXM/QL4QlV9OclNAFW1E9gDbAOOAi8CNy5RXknSCGMLvaoOARcP2b5z3nIBt0w2miRpMXynqCQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDWiz3eKbkzy9SRHkhxOctuQMVckOZXkQHe7c2niSpJG6fOdomeAD1TV/iTnAU8meaSqnh4Y91hVXTP5iJKkPsY+Q6+qE1W1v1v+GXAEWL/UwSRJi7Ooc+hJNjP3hdFPDNl9WZKDSR5OcuGI47cn2Zdk38mTJxefVpI0Uu9CT/IG4EHg/VX1wsDu/cAFVXURcDfw0LD7qKpdVTVbVbMzMzNnGVmSNEyvQk+ymrkyv7+qvji4v6peqKrT3fIeYHWStRNNKklaUJ+rXAJ8BjhSVZ8cMeb8bhxJtnb3+/wkg0qSFtbnKpfLgfcBTyU50G37MLAJoKp2AtcBNyc5A7wEXF9VNfm4kqRRxhZ6VT0OZMyYHcCOSYWSJC2e7xSVpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRvT5TtGNSb6e5EiSw0luGzImSe5KcjTJoSSXLE1cSdIofb5T9Azwgaran+Q84Mkkj1TV0/PGXAVs6W7vAO7p/pQkLZOxz9Cr6kRV7e+WfwYcAdYPDLsWuK/m7AXWJFk38bSSpJH6PEP/tSSbgYuBJwZ2rQeembd+vNt2YuD47cB2gE2bNi0y6m+2zbd/ZWqPfezjV0/tsbU8/PlqQ+8XRZO8AXgQeH9VvTC4e8gh9YoNVbuqaraqZmdmZhaXVJK0oF6FnmQ1c2V+f1V9cciQ48DGeesbgGfPPZ4kqa8+V7kE+AxwpKo+OWLYbuCG7mqXS4FTVXVixFhJ0hLocw79cuB9wFNJDnTbPgxsAqiqncAeYBtwFHgRuHHiSSVJCxpb6FX1OMPPkc8fU8AtkwolSVo83ykqSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjejznaL3JnkuybdH7L8iyakkB7rbnZOPKUkap893in4W2AHct8CYx6rqmokkkiSdlbHP0KvqG8BPliGLJOkcTOoc+mVJDiZ5OMmFowYl2Z5kX5J9J0+enNBDS5JgMoW+H7igqi4C7gYeGjWwqnZV1WxVzc7MzEzgoSVJLzvnQq+qF6rqdLe8B1idZO05J5MkLco5F3qS85OkW97a3efz53q/kqTFGXuVS5LPA1cAa5McBz4CrAaoqp3AdcDNSc4ALwHXV1UtWWJJ0lBjC72q3jNm/w7mLmuUJE2R7xSVpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRowt9CT3JnkuybdH7E+Su5IcTXIoySWTjylJGqfPM/TPAlcusP8qYEt32w7cc+6xJEmLNbbQq+obwE8WGHItcF/N2QusSbJuUgElSf2M/ZLoHtYDz8xbP95tOzE4MMl25p7Fs2nTprN+wM23f+Wsj5W0svwm/v987ONXL8n9TuJF0QzZVsMGVtWuqpqtqtmZmZkJPLQk6WWTKPTjwMZ56xuAZydwv5KkRZhEoe8GbuiudrkUOFVVrzjdIklaWmPPoSf5PHAFsDbJceAjwGqAqtoJ7AG2AUeBF4EblyqsJGm0sYVeVe8Zs7+AWyaWSJJ0VnynqCQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDWiV6EnuTLJd5McTXL7kP1XJDmV5EB3u3PyUSVJC+nznaKrgE8DfwQcB76VZHdVPT0w9LGqumYJMkqSeujzDH0rcLSqvl9VvwAeAK5d2liSpMXqU+jrgWfmrR/vtg26LMnBJA8nuXDYHSXZnmRfkn0nT548i7iSpFH6FHqGbKuB9f3ABVV1EXA38NCwO6qqXVU1W1WzMzMziwoqSVpYn0I/Dmyct74BeHb+gKp6oapOd8t7gNVJ1k4spSRprD6F/i1gS5I3JXktcD2we/6AJOcnSbe8tbvf5ycdVpI02tirXKrqTJJbga8Cq4B7q+pwkpu6/TuB64Cbk5wBXgKur6rB0zKSpCU0ttDh16dR9gxs2zlveQewY7LRJEmL4TtFJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqRG9Cj3JlUm+m+RoktuH7E+Su7r9h5JcMvmokqSFjC30JKuATwNXAW8F3pPkrQPDrgK2dLftwD0TzilJGqPPM/StwNGq+n5V/QJ4ALh2YMy1wH01Zy+wJsm6CWeVJC2gz5dErweembd+HHhHjzHrgRPzByXZztwzeIDTSb67qLSvtBb48Tnex3J4VefMJ6aQZGGv6vlcyJTmutn5nJKxOc/x7/mCUTv6FHqGbKuzGENV7QJ29XjMXpLsq6rZSd3fUjHnZJlzssw5WdPM2eeUy3Fg47z1DcCzZzFGkrSE+hT6t4AtSd6U5LXA9cDugTG7gRu6q10uBU5V1YnBO5IkLZ2xp1yq6kySW4GvAquAe6vqcJKbuv07gT3ANuAo8CJw49JF/n8mdvpmiZlzssw5WeacrKnlTNUrTnVLkl6FfKeoJDXCQpekRqzIQk+yMcnXkxxJcjjJbd32P+vWf5Vk5GVBSY4leSrJgST7ppDzH5J8p/sYhC8lWTPi+AU/UmEF5Zz2fP5dl/FAkq8leeOI46c9n31zTnU+5+3/YJJKsnbE8VOdz0XknPbP50eT/LB7/ANJto04funns6pW3A1YB1zSLZ8H/C9zHzvw+8DvAY8CswscfwxYO8Wcfwy8ptv+CeATQ45dBXwPeDPwWuAg8NaVlnOFzOfvzBvz18DOFTqfY3OuhPns1jcyd6HDD4ZlWQnz2SfnSphP4KPAB8ccuyzzuSKfoVfViara3y3/DDgCrK+qI1V1ru8unZgFcn6tqs50w/Yyd13+oD4fqbASci6bBXK+MG/YbzPkTWusjPnsk3PZjMrZ7f4n4EOMzjj1+eyZc9mMyTnOsszniiz0+ZJsBi4GnljEYQV8LcmTmfu4gSW3QM6/BB4ecsioj0tYUmeRE1bAfCb5+yTPAO8F7hxyyIqYzx45YcrzmeTdwA+r6uACh0x9PnvmhBXw8wnc2p1uuzfJ7w45ZFnmc0UXepI3AA8C7x949jPO5VV1CXOfAnlLkj9ckoCdUTmT3AGcAe4fdtiQbUv6LOQsc8IKmM+quqOqNnYZbx122JBtyz6fPXLCFOeTub/nOxj9y+bXhw3ZtmzzSf+cMP2fz3uAtwBvZ+7zq/5x2GFDtk18PldsoSdZzdyk3V9VX1zMsVX1bPfnc8CXmPvnzpIYlTPJXwDXAO+t7iTagGX9uIRzyLki5nOefwf+dMj2FTGf84zKOe35fAvwJuBgkmPMzdP+JOcPHDrt+eybc9rzSVX9qKp+WVW/Av55xOMvz3wuxYsH53pj7rfZfcCnRux/lBEvijJ37vK8ecv/DVy5nDmBK4GngZkFjn0N8H3mfmhffpHkwhWYcyXM55Z5y38F/McKnc8+Oac+nwNjjjH8RdGpz2fPnFOfT2DdvOW/AR6Y1nxO/D96QhP3B8z9c+QQcKC7bQP+hLnfdD8HfgR8tRv/RmBPt/zmbrIOAoeBO6aQ8yhz58te3rZzMGe3vo25V8q/t1JzrpD5fBD4drf9P5l7AXIlzufYnCthPgfGHKMrypU2n31yroT5BP4VeKrbvpuu4Kcxn771X5IasWLPoUuSFsdCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY34P/VzOb9pA9GvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df[\"val_loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reported-mumbai",
   "metadata": {},
   "source": [
    "## 3.3 Jednowymairowe zależności\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intensive-football",
   "metadata": {},
   "source": [
    "### 3.3.1 Pierwsza warstwa neurony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "intimate-clearance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Log-Loss  as function of first_neuron')"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'first_neuron'\n",
    "ax = sns.boxplot(x=metric, y=\"val_loss\", data=df.reset_index(),color='salmon')\n",
    "ax.set_title(f'Log-Loss  as function of {metric}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worst-engineering",
   "metadata": {},
   "source": [
    "### 3.3.2 Liczba ukrytych neuronów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "about-address",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Log-Loss as function of hidden_neuron')"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'hidden_neuron'\n",
    "ax = sns.boxplot(x=metric, y=\"val_loss\", data=df.reset_index(),color='salmon')\n",
    "ax.set_title(f'Log-Loss as function of {metric}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-thing",
   "metadata": {},
   "source": [
    "### 3.3.3 Liczba ukrytych warstw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "prerequisite-cruise",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Validation Loss as function of hidden_layers')"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'hidden_layers'\n",
    "ax = sns.boxplot(x=metric, y=\"val_loss\", data=df.reset_index(),color='salmon')\n",
    "ax.set_title(f'Validation Loss as function of {metric}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42e3bf4",
   "metadata": {},
   "source": [
    "### 3.3.4 Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "033d0ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Validation Loss as function of batch_size')"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = 'batch_size'\n",
    "ax = sns.boxplot(x=metric, y=\"val_loss\", data=df.reset_index(),color='salmon')\n",
    "ax.set_title(f'Validation Loss as function of {metric}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46dab39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
